{"id": "01f161fe-dd40-45dd-89bd-fb1562771d73", "displayed_text": "Title: Learning Dynamic Context Graphs for Predicting Social Events\n\nAbstract: Event forecasting with an aim at modeling contextual information is an important task for applications such as automated analysis generation and resource allocation. Captured contextual information for an event of interest can aid human analysts in understanding the factors associated with that event. However, capturing contextual information within event forecasting is challenging due to several factors: (i) uncertainty of context structure and formulation, (ii) high dimensional features, and (iii) adaptation of features over time. Recently, graph representations have demonstrated success in applications such as traffic forecasting, social influence prediction, and visual question answering systems. In this paper, we study graph representations in modeling social events to identify dynamic properties of event contexts as social indicators. Inspired by graph neural networks, we propose a novel graph convolutional network for predicting future events (e.g., civil unrest movements). We extract and learn graph representations from historical/prior event documents. By employing the hidden word graph features, our proposed model predicts the occurrence of future events and identifies sequences of dynamic graphs as event context. Experimental results on multiple real-world data sets show that the proposed method is competitive against various state-of-the-art methods for social event prediction.", "label_annotations": {"Multi-aspect Summary": {"Context": "Capturing contextual information for an event can aid analysts in understanding the factors associated with the event, but it is challenging due to uncertainty of context structure, high dimensional features and adaptation of features over time. Recently, graph representations have rendered helpful on capturing contextual information.", "Key idea": "The author proposed a graph convolutional network for predicting future events. The network model predicts the future events and identifies sequences of dynamic graphs as event context by learning graph representations from historical event documents, employing the hidden word graph features.", "Method": "The author utilizes real-world data sets to benchmark the proposed model and compare against various state-of-the-art methods for social event prediction.", "Outcome": "The results show that the proposed model is competitive against various state-of-the-art methods for social event prediction.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 51s "}}
{"id": "01f161fe-dd40-45dd-89bd-fb1562771d73", "displayed_text": "Title: Learning Dynamic Context Graphs for Predicting Social Events\n\nAbstract: Event forecasting with an aim at modeling contextual information is an important task for applications such as automated analysis generation and resource allocation. Captured contextual information for an event of interest can aid human analysts in understanding the factors associated with that event. However, capturing contextual information within event forecasting is challenging due to several factors: (i) uncertainty of context structure and formulation, (ii) high dimensional features, and (iii) adaptation of features over time. Recently, graph representations have demonstrated success in applications such as traffic forecasting, social influence prediction, and visual question answering systems. In this paper, we study graph representations in modeling social events to identify dynamic properties of event contexts as social indicators. Inspired by graph neural networks, we propose a novel graph convolutional network for predicting future events (e.g., civil unrest movements). We extract and learn graph representations from historical/prior event documents. By employing the hidden word graph features, our proposed model predicts the occurrence of future events and identifies sequences of dynamic graphs as event context. Experimental results on multiple real-world data sets show that the proposed method is competitive against various state-of-the-art methods for social event prediction.", "label_annotations": {"Multi-aspect Summary": {"Context": "Event forecasting with an aim at modeling contextual information is a challenging but important task for applications such as automated analysis generation and resource allocation. Recently, graph representations have demonstrated success in applications such as traffic forecasting, social influence prediction, and visual question answering systems.", "Key idea": "The authors propose a novel graph convolutional network for predicting future events.", "Method": "The authors evaluate their approach on multiple real-world data sets.", "Outcome": "Experimental results on multiple real-world data sets show that the proposed method is competitive against various state-of-the-art methods for social event prediction.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 6m 10s "}}
{"id": "07c3daea-a88c-4a67-9aac-20ef0ec62e79", "displayed_text": "Title: Enabling Biographical Cognitive Stimulation for People with Dementia\n\nAbstract: Non-pharmacological interventions are the most common and arguably most effective for people with dementia. Some of these approaches have been proven to benefit from the usage of biographical or personalized materials. These contents are not always easy to obtain. Alongside, it is a challenge to maintain awareness of what is meaningful for a certain person. Faced with an absence of tools to collect and manage biographical materials, we created a web platform that supports the work of psychologists, streamlining the collection of relevant information about people with dementia. This knowledge is then used as a starting point to perform reminiscence and other biographical cognitive stimulation practices. In this paper, we present the design of our platform and results from a case study with one psychologist and three patients, across a period of two weeks that showed improvements in the collection of meaningful data about a person, and on maintaining awareness of the therapy as a whole.", "label_annotations": {"Multi-aspect Summary": {"Context": "It is a challenge to maintain awareness of what is meaningful for a certain person, and absence of tools to collect and manage biographical materials happens to biographical for people with Dementia.", "Key idea": "The authors created a web platform that supports the work of psychologists, streamlining the collection of relevant information about people with dementia.", "Method": "The authors use a case study with one psychologist and three patients, across a period of two weeks to show the effectiveness of new web platform.", "Outcome": "The case study show that web platform make improvements in the collection of meaningful data about a person, and on maintaining awareness of the therapy as a whole.\r\n", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 14s "}}
{"id": "07c3daea-a88c-4a67-9aac-20ef0ec62e79", "displayed_text": "Title: Enabling Biographical Cognitive Stimulation for People with Dementia\n\nAbstract: Non-pharmacological interventions are the most common and arguably most effective for people with dementia. Some of these approaches have been proven to benefit from the usage of biographical or personalized materials. These contents are not always easy to obtain. Alongside, it is a challenge to maintain awareness of what is meaningful for a certain person. Faced with an absence of tools to collect and manage biographical materials, we created a web platform that supports the work of psychologists, streamlining the collection of relevant information about people with dementia. This knowledge is then used as a starting point to perform reminiscence and other biographical cognitive stimulation practices. In this paper, we present the design of our platform and results from a case study with one psychologist and three patients, across a period of two weeks that showed improvements in the collection of meaningful data about a person, and on maintaining awareness of the therapy as a whole.", "label_annotations": {"Multi-aspect Summary": {"Context": "Non-pharmacological interventions are the most common and arguably most effective for people with dementia. However, some of these approaches have been proven to benefit from the usage of biographical or personalized materials, which are not always easy to obtain and challenging to distinguish what are meaningful for a certain person.", "Key idea": "The authors create a web platform that supports the work of psychologists, streamlining the collection of relevant information about people with dementia.", "Method": "The authors conduct a case study with one psychologist and three patients, across a period of two weeks.", "Outcome": "Results from a case study with one psychologist and three patients, across a period of two weeks show improvements of the proposed platform in the collection of meaningful data about a person, and on maintaining awareness of the therapy as a whole.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 7m 45s "}}
{"id": "081d6673-3c7c-4aec-b101-cf55d75ac718", "displayed_text": "Title: Fast, Accurate and Memory-Efficient Partial Permutation Synchronization\n\nAbstract: Previous partial permutation synchronization (PPS) algorithms, which are commonly used for multi-object matching, often involve computation-intensive and memory-demanding matrix operations. These operations become intractable for large scale structure-from-motion datasets. For pure permutation synchronization, the recent Cycle-Edge Message Passing (CEMP) framework suggests a memory-efficient and fast solution. Here we overcome the restriction of CEMP to compact groups and propose an improved algorithm, CEMP-Partial, for estimating the corruption levels of the observed partial permutations. It allows us to subsequently implement a nonconvex weighted projected power method without the need of spectral initialization. The resulting new PPS algorithm, MatchFAME (Fast, Accurate and Memory-Efficient Matching), only involves sparse matrix operations, and thus enjoys lower time and space complexities in comparison to previous PPS algorithms. We prove that under adversarial corruption, though without additive noise and with certain assumptions, CEMP-Partial is able to exactly classify corrupted and clean partial permutations. We demonstrate the state-of-the-art accuracy, speed and memory efficiency of our method on both synthetic and real datasets.", "label_annotations": {"Multi-aspect Summary": {"Context": "Previous partial permutation synchronization (PPS) algorithms commonly used for multi-object matching is intractable for large scale structure-from-motion datasets.  For pure permutation synchronization, the recent Cycle-Edge Message Passing (CEMP) framework suggests a memory-efficient and fast solution.", "Key idea": "The authors propose an improved algorithm named CEMP-Partial, which can overcome the restriction of CEMP to compact groups, used to estimate the corruption levels of the observed partial permutations. CEMP-Partial allows people to subsequently implement a nonconvex weighted projected power method without the need of spectral initialization. ", "Method": "The authors apply CEMP-Partial algorithms on adversarial corruption and on both synthetic and real datasets.", "Outcome": "Compared to previous PPS algorithms, CEMP-Partial enjoys lower time and space complexities. Under adversarial corruption, CEMP-Partial is able to exactly classify corrupted and clean partial permutations without additive noise and with certain assumptions. ", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 51s "}}
{"id": "081d6673-3c7c-4aec-b101-cf55d75ac718", "displayed_text": "Title: Fast, Accurate and Memory-Efficient Partial Permutation Synchronization\n\nAbstract: Previous partial permutation synchronization (PPS) algorithms, which are commonly used for multi-object matching, often involve computation-intensive and memory-demanding matrix operations. These operations become intractable for large scale structure-from-motion datasets. For pure permutation synchronization, the recent Cycle-Edge Message Passing (CEMP) framework suggests a memory-efficient and fast solution. Here we overcome the restriction of CEMP to compact groups and propose an improved algorithm, CEMP-Partial, for estimating the corruption levels of the observed partial permutations. It allows us to subsequently implement a nonconvex weighted projected power method without the need of spectral initialization. The resulting new PPS algorithm, MatchFAME (Fast, Accurate and Memory-Efficient Matching), only involves sparse matrix operations, and thus enjoys lower time and space complexities in comparison to previous PPS algorithms. We prove that under adversarial corruption, though without additive noise and with certain assumptions, CEMP-Partial is able to exactly classify corrupted and clean partial permutations. We demonstrate the state-of-the-art accuracy, speed and memory efficiency of our method on both synthetic and real datasets.", "label_annotations": {"Multi-aspect Summary": {"Context": "Previous partial permutation synchronization (PPS) algorithms often involve computation-intensive and memory-demanding matrix operations, which are intractable for large scale structure-from-motion datasets. Pure permutation synchronization, such as Cycle-Edge Message Passing (CEMP) framework suggests a memory-efficient and fast solution.", "Key idea": "The authors propose an improved algorithm, CEMP-Partial, for estimating the corruption levels of the observed partial permutations, as well as the resulting new PPS algorithm, MatchFAME (Fast, Accurate and Memory-Efficient Matching), which only involves sparse matrix operations, and thus enjoys lower time and space complexities in comparison to previous PPS algorithms..", "Method": "The authors examine the performance of CEMP-Partial under adversarial corruption but without additive noise and with certain assumptions.", "Outcome": "The authors prove that CEMP-Partial is able to exactly classify corrupted and clean partial permutations and demonstrate the state-of-the-art accuracy, speed and memory efficiency of our method on both synthetic and real datasets.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 9m 48s "}}
{"id": "0b31e456-4944-47e5-80ed-deaf6421c375", "displayed_text": "Title: Directed dialogue protocols: verbal data for user interface design\n\nAbstract: The development of an interface design tool called \u201cdirected dialogue protocols\u201d is discussed. The tool is based upon Katou0027s (1986) method of verbal data collection, \u201cquestion-asking protocols.\u201d Three extensions to the question-asking method are detailed: 1) an experimental procedure of atomic tasks which facilitate the quantization of verbal data; 2) interventions by the experimenter that probe the subjectu0027s expectations and prompt verbalizations; and 3) a technique for answering subject queries called sequential disclosure. Also discussed are applications of the directed dialogue that have identified design choices which build learnability and usability into a productu0027s user-interface.", "label_annotations": {"Multi-aspect Summary": {"Context": "An interface design tool called \u201cdirected dialogue protocols\u201d is developed, based on Katou0027s (1986) method of verbal data collection, \u201cquestion-asking protocols.\u201d.", "Key idea": "Three extensions to the question-asking method are detailed: 1) an experimental procedure of atomic tasks which facilitate the quantization of verbal data; 2) interventions by the experimenter that probe the subjectu0027s expectations and prompt verbalizations; and 3) a technique for answering subject queries called sequential disclosure.", "Method": "N/A", "Outcome": "Authors develop applications of the directed dialogue that have identified design choices which build learnability and usability into a productu0027s user-interface.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 22s "}}
{"id": "0b31e456-4944-47e5-80ed-deaf6421c375", "displayed_text": "Title: Directed dialogue protocols: verbal data for user interface design\n\nAbstract: The development of an interface design tool called \u201cdirected dialogue protocols\u201d is discussed. The tool is based upon Katou0027s (1986) method of verbal data collection, \u201cquestion-asking protocols.\u201d Three extensions to the question-asking method are detailed: 1) an experimental procedure of atomic tasks which facilitate the quantization of verbal data; 2) interventions by the experimenter that probe the subjectu0027s expectations and prompt verbalizations; and 3) a technique for answering subject queries called sequential disclosure. Also discussed are applications of the directed dialogue that have identified design choices which build learnability and usability into a productu0027s user-interface.", "label_annotations": {"Multi-aspect Summary": {"Context": "In 1986, Kato propose a method of verbal data collection, \u201cquestion-asking protocols.\u201d", "Key idea": "The authors discuss the development of an interface design tool called \u201cdirected dialogue protocols\u201d, which has 3 extensions to Kato's question-asking method. They also discuss the applications of the directed dialogue that have identified design choices which build learnability and usability into a product's user-interface.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 22s "}}
{"id": "10c15fe5-c315-4b6d-8910-e6bc3279c817", "displayed_text": "Title: Knowledge-Preserving Incremental Social Event Detection via Heterogeneous GNNs\n\nAbstract: ABSTRACT Social events provide valuable insights into group social behaviors and public concerns and therefore have many applications in fields such as product recommendation and crisis management. The complexity and streaming nature of social messages make it appealing to address social event detection in an incremental learning setting, where acquiring, preserving, and extending knowledge are major concerns. Most existing methods, including those based on incremental clustering and community detection, learn limited amounts of knowledge as they ignore the rich semantics and structural information contained in social data. Moreover, they cannot memorize previously acquired knowledge. In this paper, we propose a novel Knowledge-Preserving Incremental Heterogeneous Graph Neural Network (KPGNN) for incremental social event detection. To acquire more knowledge, KPGNN models complex social messages into unified social graphs to facilitate data utilization and explores the expressive power of GNNs for knowledge extraction. To continuously adapt to the incoming data, KPGNN adopts contrastive loss terms that cope with a changing number of event classes. It also leverages the inductive learning ability of GNNs to efficiently detect events and extends its knowledge from previously unseen data. To deal with large social streams, KPGNN adopts a mini-batch subgraph sampling strategy for scalable training, and periodically removes obsolete data to maintain a dynamic embedding space. KPGNN requires no feature engineering and has few hyperparameters to tune. Extensive experiment results demonstrate the superiority of KPGNN over various baselines.", "label_annotations": {"Multi-aspect Summary": {"Context": "Most existing methods, including those based on incremental clustering and community detection, learn limited amounts of knowledge as they ignore the rich semantics and structural information contained in social data.  Moreover, they cannot memorize previously acquired knowledge.", "Key idea": "The authors propose a novel Knowledge-Preserving Incremental Heterogeneous Graph Neural Network (KPGNN) for incremental social event detection. ", "Method": "The authors conduct experiment to evaluate Knowledge-Preserving Incremental Heterogeneous Graph Neural Network(KPGNN)'s performance on incremental social event detection compared to baseline method's performance.", "Outcome": "Extensive experiment results demonstrate the superiority of KPGNN's performance on incremental social event detection over various baselines.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "10c15fe5-c315-4b6d-8910-e6bc3279c817", "displayed_text": "Title: Knowledge-Preserving Incremental Social Event Detection via Heterogeneous GNNs\n\nAbstract: ABSTRACT Social events provide valuable insights into group social behaviors and public concerns and therefore have many applications in fields such as product recommendation and crisis management. The complexity and streaming nature of social messages make it appealing to address social event detection in an incremental learning setting, where acquiring, preserving, and extending knowledge are major concerns. Most existing methods, including those based on incremental clustering and community detection, learn limited amounts of knowledge as they ignore the rich semantics and structural information contained in social data. Moreover, they cannot memorize previously acquired knowledge. In this paper, we propose a novel Knowledge-Preserving Incremental Heterogeneous Graph Neural Network (KPGNN) for incremental social event detection. To acquire more knowledge, KPGNN models complex social messages into unified social graphs to facilitate data utilization and explores the expressive power of GNNs for knowledge extraction. To continuously adapt to the incoming data, KPGNN adopts contrastive loss terms that cope with a changing number of event classes. It also leverages the inductive learning ability of GNNs to efficiently detect events and extends its knowledge from previously unseen data. To deal with large social streams, KPGNN adopts a mini-batch subgraph sampling strategy for scalable training, and periodically removes obsolete data to maintain a dynamic embedding space. KPGNN requires no feature engineering and has few hyperparameters to tune. Extensive experiment results demonstrate the superiority of KPGNN over various baselines.", "label_annotations": {"Multi-aspect Summary": {"Context": "The complexity and streaming nature of social messages make it appealing to address social event detection in an incremental learning setting, where acquiring, preserving, and extending knowledge are major concerns. Most existing methods, including those based on incremental clustering and community detection, learn limited amounts of knowledge as they ignore the rich semantics and structural information contained in social data. Moreover, they cannot memorize previously acquired knowledge.", "Key idea": "Authors propose a novel Knowledge-Preserving Incremental Heterogeneous Graph Neural Network (KPGNN) for incremental social event detection. To acquire more knowledge, KPGNN models complex social messages into unified social graphs to facilitate data utilization and explores the expressive power of GNNs for knowledge extraction. To continuously adapt to the incoming data, KPGNN adopts contrastive loss terms that cope with a changing number of event classes. It also leverages the inductive learning ability of GNNs to efficiently detect events and extends its knowledge from previously unseen data. To deal with large social streams, KPGNN adopts a mini-batch subgraph sampling strategy for scalable training, and periodically removes obsolete data to maintain a dynamic embedding space. ", "Method": "Authors setup experiments to compare KPGNN over various baselines.", "Outcome": "Extensive experiment results demonstrate the superiority of KPGNN over various baselines.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 28s "}}
{"id": "14b0ebd1-b654-4eed-bdd8-ebeb74250b15", "displayed_text": "Title: Relation-Guided Few-Shot Relational Triple Extraction\n\nAbstract: In few-shot relational triple extraction (FS-RTE), one seeks to extract relational triples from plain texts by utilizing only few annotated samples. Recent work first extracts all entities and then classifies their relations. Such an entity-then-relation paradigm ignores the entity discrepancy between relations. To address it, we propose a novel task decomposition strategy, Relation-then-Entity, for FS-RTE. It first detects relations occurred in a sentence and then extracts the corresponding head/tail entities of the detected relations. To instantiate this strategy, we further propose a model, RelATE, which builds a dual-level attention to aggregate relation-relevant information to detect the relation occurrence and utilizes the annotated samples of the detected relations to extract the corresponding head/tail entities. Experimental results show that our model outperforms previous work by an absolute gain (18.98%, 28.85% in F1 in two few-shot settings).", "label_annotations": {"Multi-aspect Summary": {"Context": "In few-shot relational triple extraction, one seeks to extract relational triples from plain texts by utilizing few annotated samples. Recent work first extracts all entities and then classifies their relations, which ignores the entity discrepancy between relations.", "Key idea": "The author first proposes a novel task decomposition strategy for FS-RTE, which detects relations occurred in a sentence and then extracts the corresponding head/tail entities of the detected relations. To instantiate this strategy, the author also proposes a model that builds a dual-level attention to aggregate relation-relevant information to detect the relation occurrence and utilizes the annotated samples of the detected relations to extract the corresponding head/tail entities.", "Method": "The author assesses the performance of the proposed strategy against previous works.", "Outcome": "Experimental results show that the proposed model outperforms previous work by an absolute gain (18.98%, 28.85% in F1 in two few-shot settings).", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 55s "}}
{"id": "14b0ebd1-b654-4eed-bdd8-ebeb74250b15", "displayed_text": "Title: Relation-Guided Few-Shot Relational Triple Extraction\n\nAbstract: In few-shot relational triple extraction (FS-RTE), one seeks to extract relational triples from plain texts by utilizing only few annotated samples. Recent work first extracts all entities and then classifies their relations. Such an entity-then-relation paradigm ignores the entity discrepancy between relations. To address it, we propose a novel task decomposition strategy, Relation-then-Entity, for FS-RTE. It first detects relations occurred in a sentence and then extracts the corresponding head/tail entities of the detected relations. To instantiate this strategy, we further propose a model, RelATE, which builds a dual-level attention to aggregate relation-relevant information to detect the relation occurrence and utilizes the annotated samples of the detected relations to extract the corresponding head/tail entities. Experimental results show that our model outperforms previous work by an absolute gain (18.98%, 28.85% in F1 in two few-shot settings).", "label_annotations": {"Multi-aspect Summary": {"Context": "Recent work in few-shot relational triple extraction (FS-RTE) first extracts all entities and then classifies their relations. Such an entity-then-relation paradigm ignores the entity discrepancy between relations.", "Key idea": "The authors propose a novel task decomposition strategy, Relation-then-Entity, for FS-RTE, which first detects relations occurred in a sentence and then extracts the corresponding head/tail entities of the detected relations. They further propose a model, RelATE, which builds a dual-level attention to aggregate relation-relevant information to detect the relation occurrence and utilizes the annotated samples of the detected relations to extract the corresponding head/tail entities.", "Method": "N/A", "Outcome": "Experimental results show that our model outperforms previous work by an absolute gain (18.98%, 28.85% in F1 in two few-shot settings).", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "16c1b4ae-73f8-4c23-8bdb-b931ade1baa5", "displayed_text": "Title: Computing optimal subsets\n\nAbstract: Various tasks in decision making and decision support require selecting a preferred subset of items from a given set of feasible items. Recent work in this area considered methods for specifying such preferences based on the attribute values of individual elements within the set. Of these, the approach of (Brafman et al. 2006) appears to be the most general. In this paper, we consider the problem of computing an optimal subset given such a specification. The problem is shown to be NP-hard in the general case, necessitating heuristic search methods. We consider two algorithm classes for this problem: direct set construction, and implicit enumeration as solutions to appropriate CSPs. New algorithms are presented in each class and compared empirically against previous results.", "label_annotations": {"Multi-aspect Summary": {"Context": "Various tasks in decision making and decision support require selecting a preferred subset of items from a given set of feasible items. Recent work in this area considered methods for specifying such preferences based on the attribute values of individual elements within the set. Of these, the approach of (Brafman et al. 2006) appears to be the most general.", "Key idea": "Authors consider the problem of computing an optimal subset given such a specification. Authors consider two algorithm classes for this problem: direct set construction, and implicit enumeration as solutions to appropriate CSPs. New algorithms are presented in each class and compared empirically against previous results.", "Method": "Authors compare proposed method with previous methods empirically .", "Outcome": "Authors present new algorithms  in each class and compared empirically against previous results.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 52s "}}
{"id": "16c1b4ae-73f8-4c23-8bdb-b931ade1baa5", "displayed_text": "Title: Computing optimal subsets\n\nAbstract: Various tasks in decision making and decision support require selecting a preferred subset of items from a given set of feasible items. Recent work in this area considered methods for specifying such preferences based on the attribute values of individual elements within the set. Of these, the approach of (Brafman et al. 2006) appears to be the most general. In this paper, we consider the problem of computing an optimal subset given such a specification. The problem is shown to be NP-hard in the general case, necessitating heuristic search methods. We consider two algorithm classes for this problem: direct set construction, and implicit enumeration as solutions to appropriate CSPs. New algorithms are presented in each class and compared empirically against previous results.", "label_annotations": {"Multi-aspect Summary": {"Context": "Various tasks in decision making and decision support require selecting a preferred subset of items from a given set of feasible items. Recent work in this area considered the general methods for specifying such preferences based on the attribute values of individual elements within the set.", "Key idea": "The authors contribute to two algorithm classes for the problem of computing an optimal subset: direct set construction, and implicit enumeration as solutions to appropriate CSPs.", "Method": "The authors compare their proposed algorithms empirically against previous results.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 37m 39s "}}
{"id": "18f27ced-9f37-45d7-9b76-6663c349d408", "displayed_text": "Title: Scalable Deep Multimodal Learning for Cross-Modal Retrieval\n\nAbstract: Cross-modal retrieval takes one type of data as the query to retrieve relevant data of another type. Most of existing cross-modal retrieval approaches were proposed to learn a common subspace in a joint manner, where the data from all modalities have to be involved during the whole training process. For these approaches, the optimal parameters of different modality-specific transformations are dependent on each other and the whole model has to be retrained when handling samples from new modalities. In this paper, we present a novel cross-modal retrieval method, called Scalable Deep Multimodal Learning (SDML). It proposes to predefine a common subspace, in which the between-class variation is maximized while the within-class variation is minimized. Then, it trains m modality-specific networks for m modalities (one network for each modality) to transform the multimodal data into the predefined common subspace to achieve multimodal learning. Unlike many of the existing methods, our method can train different modality-specific networks independently and thus be scalable to the number of modalities. To the best of our knowledge, the proposed SDML could be one of the first works to independently project data of an unfixed number of modalities into a predefined common subspace. Comprehensive experimental results on four widely-used benchmark datasets demonstrate that the proposed method is effective and efficient in multimodal learning and outperforms the state-of-the-art methods in cross-modal retrieval.", "label_annotations": {"Multi-aspect Summary": {"Context": "Most of existing cross-modal retrieval approaches were proposed to learn a common subspace in a joint manner, where the data from all modalities have to be involved during the whole training process. \r\nIn these approaches, the whole model has to be retrained when handling samples from new modalities.", "Key idea": "The paper introduces a novel cross-modal retrieval method namedScalable Deep Multimodal Learning (SDML), a method that predefines a common subspace to maximize between-class variation and minimize within-class variation. ", "Method": "The authors applied SDML on four widely-used benchmark datasets to figure its performance.", "Outcome": "SDML could be one of the first works to independently project data of an unfixed number of modalities into a predefined common subspace.\r\nExperimental results on four widely-used benchmark datasets demonstrate that the SDML is effective and efficient in multimodal learning and outperforms the state-of-the-art methods in cross-modal retrieval.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 23s "}}
{"id": "18f27ced-9f37-45d7-9b76-6663c349d408", "displayed_text": "Title: Scalable Deep Multimodal Learning for Cross-Modal Retrieval\n\nAbstract: Cross-modal retrieval takes one type of data as the query to retrieve relevant data of another type. Most of existing cross-modal retrieval approaches were proposed to learn a common subspace in a joint manner, where the data from all modalities have to be involved during the whole training process. For these approaches, the optimal parameters of different modality-specific transformations are dependent on each other and the whole model has to be retrained when handling samples from new modalities. In this paper, we present a novel cross-modal retrieval method, called Scalable Deep Multimodal Learning (SDML). It proposes to predefine a common subspace, in which the between-class variation is maximized while the within-class variation is minimized. Then, it trains m modality-specific networks for m modalities (one network for each modality) to transform the multimodal data into the predefined common subspace to achieve multimodal learning. Unlike many of the existing methods, our method can train different modality-specific networks independently and thus be scalable to the number of modalities. To the best of our knowledge, the proposed SDML could be one of the first works to independently project data of an unfixed number of modalities into a predefined common subspace. Comprehensive experimental results on four widely-used benchmark datasets demonstrate that the proposed method is effective and efficient in multimodal learning and outperforms the state-of-the-art methods in cross-modal retrieval.", "label_annotations": {"Multi-aspect Summary": {"Context": "Most of existing cross-modal retrieval approaches were proposed to learn a common subspace in a joint manner, where the data from all modalities have to be involved during the whole training process. The optimal parameters of different modality-specific transformations are dependent on each other and the whole model has to be retrained when handling samples from new modalities.", "Key idea": "The author proposes a cross-modal retrieval method that predefines a common subspace, maximizing between-class and minimizing within-class variation, and trains m modality-specific networks for m modalities (one network for each modality) to transform the multimodal data into the predefined common subspace to achieve multimodal learning. The model can train different modality-specific networks independently.", "Method": "The author tests the model against four widely-used benchmark datasets for effectiveness and efficiency in multimodal learning.", "Outcome": "Experimental results on four widely-used benchmark datasets demonstrate that the proposed method is effective and efficient in multimodal learning and outperforms the state-of-the-art methods in cross-modal retrieval.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 29s "}}
{"id": "192f7803-df4d-40c0-b816-ba34339026b3", "displayed_text": "Title: Amortised MAP Inference for Image Super-resolution\n\nAbstract: Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "label_annotations": {"Multi-aspect Summary": {"Context": "Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss, but the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible.", "Key idea": "The authors introduce new methods for amortised MAP inference whereby they calculate the MAP estimate directly using a convolutional neural network. They also propose three methods to solve this optimisation problem and establish a connection between GANs and amortised variational inference.", "Method": "N/A", "Outcome": "The authors show that, using their proposed architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. The experiments also show that the GAN based approach performs best on real image data.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 4s "}}
{"id": "192f7803-df4d-40c0-b816-ba34339026b3", "displayed_text": "Title: Amortised MAP Inference for Image Super-resolution\n\nAbstract: Image super-resolution (SR) is an underdetermined inverse problem, where a large number of plausible high resolution images can explain the same downsampled image. Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss.\nHowever, the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach would employ Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible. Direct MAP estimation for SR is non-trivial, as it requires us to build a model for the image prior from samples. Here we introduce new methods for \\emph{amortised MAP inference} whereby we calculate the MAP estimate directly using a convolutional neural network. We first introduce a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions ensuring that the high resolution output of the network is always consistent with the low resolution input. We show that, using this architecture, the amortised MAP inference problem reduces to minimising the cross-entropy between two distributions, similar to training generative models. We propose three methods to solve this optimisation problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior. Our experiments show that the GAN based approach performs best on real image data. Lastly, we establish a connection between GANs and amortised variational inference as in e.g. variational autoencoders.", "label_annotations": {"Multi-aspect Summary": {"Context": "Most current single image SR methods use empirical risk minimisation, often with a pixel-wise mean squared error (MSE) loss, but the outputs from such methods tend to be blurry, over-smoothed and generally appear implausible. A more desirable approach employs Maximum a Posteriori (MAP) inference, preferring solutions that always have a high probability under the image prior, and thus appear more plausible, which is not non-trivial as it requires us to build a model for the image prior from samples. ", "Key idea": "The author introduces new methods for amortised MAP inference by reducing the problem to minimising the cross-entropy between two distributions after using a novel neural network architecture that performs a projection to the affine subspace of valid SR solutions, and then proposes three methods to solve the reduced problem: (1) Generative Adversarial Networks (GAN) (2) denoiser-guided SR which backpropagates gradient-estimates from denoising to train the network, and (3) a baseline method using a maximum-likelihood-trained image prior.", "Method": "The author tests the proposed three reduced optimisation problem on real image data.", "Outcome": "The experiments show that the GAN based approach performs best on real image data.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 8m 20s "}}
{"id": "1946f496-f6cd-4736-8c30-a6ae70baa8b2", "displayed_text": "Title: Content-aware click modeling\n\nAbstract: Click models aim at extracting intrinsic relevance of documents to queries from biased user clicks. One basic modeling assumption made in existing work is to treat such intrinsic relevance as an atomic query-document-specific parameter, which is solely estimated from historical clicks without using any content information about a document or relationship among the clicked/skipped documents under the same query. Due to this overly simplified assumption, existing click models can neither fully explore the information about a document's relevance quality nor make predictions of relevance for any unseen documents. In this work, we proposed a novel Bayesian Sequential State model for modeling the user click behaviors, where the document content and dependencies among the sequential click events within a query are characterized by a set of descriptive features via a probabilistic graphical model. By applying the posterior regularized Expectation Maximization algorithm for parameter learning, we tailor the model to meet specific ranking-oriented properties, e.g., pairwise click preferences, so as to exploit richer information buried in the user clicks. Experiment results on a large set of real click logs demonstrate the effectiveness of the proposed model compared with several state-of-the-art click models.", "label_annotations": {"Multi-aspect Summary": {"Context": "Due to overly simplified assumption that treat such intrinsic relevance as an atomic query-document-specific parameter, which is solely estimated from historical clicks without using any content information about a document or relationship among the clicked/skipped documents under the same query, existing click models can neither fully explore the information about a document's relevance quality nor make predictions of relevance for any unseen documents.", "Key idea": "The authors proposed a novel Bayesian Sequential State model for modeling the user click behaviors, where the document content and dependencies among the sequential click events within a query are characterized by a set of descriptive features via a probabilistic graphical model.", "Method": "The authors conduct experiments on a large set of real click logs to evaluate effectiveness of novel Bayesian Sequential State model ", "Outcome": "Experiment results on a large set of real click logs demonstrate the effectiveness of novel Bayesian Sequential State model compared with several state-of-the-art click models.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 37s "}}
{"id": "1946f496-f6cd-4736-8c30-a6ae70baa8b2", "displayed_text": "Title: Content-aware click modeling\n\nAbstract: Click models aim at extracting intrinsic relevance of documents to queries from biased user clicks. One basic modeling assumption made in existing work is to treat such intrinsic relevance as an atomic query-document-specific parameter, which is solely estimated from historical clicks without using any content information about a document or relationship among the clicked/skipped documents under the same query. Due to this overly simplified assumption, existing click models can neither fully explore the information about a document's relevance quality nor make predictions of relevance for any unseen documents. In this work, we proposed a novel Bayesian Sequential State model for modeling the user click behaviors, where the document content and dependencies among the sequential click events within a query are characterized by a set of descriptive features via a probabilistic graphical model. By applying the posterior regularized Expectation Maximization algorithm for parameter learning, we tailor the model to meet specific ranking-oriented properties, e.g., pairwise click preferences, so as to exploit richer information buried in the user clicks. Experiment results on a large set of real click logs demonstrate the effectiveness of the proposed model compared with several state-of-the-art click models.", "label_annotations": {"Multi-aspect Summary": {"Context": "One basic modeling assumption of Click models made in existing work is to treat the intrinsic relevance of documents to queries as an atomic query-document-specific parameter. Due to this overly simplified assumption, existing click models can neither fully explore the information about a document's relevance quality nor make predictions of relevance for any unseen documents.", "Key idea": "The authors propose a novel Bayesian Sequential State model for modeling the user click behaviors, where the document content and dependencies among the sequential click events within a query are characterized by a set of descriptive features via a probabilistic graphical model.", "Method": "The authors evaluate their model on a large set of real click logs.", "Outcome": "Experiment results on a large set of real click logs demonstrate the effectiveness of the proposed model compared with several state-of-the-art click models.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 11s "}}
{"id": "1b0e4045-d39b-4bea-8dec-e747f5c674f5", "displayed_text": "Title: Mining uncertain data with probabilistic guarantees\n\nAbstract: Data uncertainty is inherent in applications such as sensor monitoring systems, location-based services, and biological databases. To manage this vast amount of imprecise information, probabilistic databases have been recently developed. In this paper, we study the discovery of frequent patterns and association rules from probabilistic data under the Possible World Semantics. This is technically challenging, since a probabilistic database can have an exponential number of possible worlds. We propose two effcient algorithms, which discover frequent patterns in bottom-up and top-down manners. Both algorithms can be easily extended to discover maximal frequent patterns. We also explain how to use these patterns to generate association rules. Extensive experiments, using real and synthetic datasets, were conducted to validate the performance of our methods.", "label_annotations": {"Multi-aspect Summary": {"Context": "Probabilistic databases have been recently developed to manage data uncertainty such as sensor monitoring systems, location-based services, and biological databases. The difficulty of a probabilistic database is that it can have an exponential number of possible worlds.", "Key idea": "The author studies the discovery of frequent patterns and association rules from probabilistic data under the Possible World Semantics. The author proposes two efficient algorithms, which discover frequent patterns in bottom-up and top-down manners, both of which can be easily extended to discover maximal frequent patterns.", "Method": "The author uses real and synthetic datasets to test the performance of their methods.", "Outcome": "The performance of their methods is validated through real and synthetic datasets.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 31s "}}
{"id": "1b0e4045-d39b-4bea-8dec-e747f5c674f5", "displayed_text": "Title: Mining uncertain data with probabilistic guarantees\n\nAbstract: Data uncertainty is inherent in applications such as sensor monitoring systems, location-based services, and biological databases. To manage this vast amount of imprecise information, probabilistic databases have been recently developed. In this paper, we study the discovery of frequent patterns and association rules from probabilistic data under the Possible World Semantics. This is technically challenging, since a probabilistic database can have an exponential number of possible worlds. We propose two effcient algorithms, which discover frequent patterns in bottom-up and top-down manners. Both algorithms can be easily extended to discover maximal frequent patterns. We also explain how to use these patterns to generate association rules. Extensive experiments, using real and synthetic datasets, were conducted to validate the performance of our methods.", "label_annotations": {"Multi-aspect Summary": {"Context": "To manage this vast amount of imprecise information, probabilistic databases have been recently developed.", "Key idea": "The authors propose two efficient algorithms, which discover frequent patterns in bottom-up and top-down manners.", "Method": "The authors perform extensive experiments, using real and synthetic datasets to evaluate two algorithms, which discover frequent patterns in bottom-up and top-down manners.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 4s "}}
{"id": "1dea5ec2-d311-4c03-bba5-e38d7a62fbd4", "displayed_text": "Title: Sign Spotting Using Hierarchical Sequential Patterns with Temporal Intervals\n\nAbstract: This paper tackles the problem of spotting a set of signs occuring in videos with sequences of signs. To achieve this, we propose to model the spatio-temporal signatures of a sign using an extension of sequential patterns that contain temporal intervals called Sequential Interval Patterns (SIP). We then propose a novel multi-class classifier that organises different sequential interval patterns in a hierarchical tree structure called a Hierarchical SIP Tree (HSP-Tree). This allows one to exploit any subsequence sharing that exists between different SIPs of different classes. Multiple trees are then combined together into a forest of HSP-Trees resulting in a strong classifier that can be used to spot signs. We then show how the HSP-Forest can be used to spot sequences of signs that occur in an input video. We have evaluated the method on both concatenated sequences of isolated signs and continuous sign sequences. We also show that the proposed method is superior in robustness and accuracy to a state of the art sign recogniser when applied to spotting a sequence of signs.", "label_annotations": {"Multi-aspect Summary": {"Context": "It's challenging to  tackles the problem of spotting a set of signs occuring in videos with sequences of signs. ", "Key idea": "The authors propose a novel multi-class classifier that organises different sequential interval patterns in a hierarchical tree structure called a Hierarchical SIP Tree (HSP-Tree) to tackles the problem of spotting a set of signs occuring in videos with sequences of signs.", "Method": "The authors evaluated the HSP-Forest on both concatenated sequences of isolated signs and continuous sign sequences. ", "Outcome": "The authors prove that HSP-Forest can be used to spot sequences of signs that occur in an input video and the proposed method is superior in robustness and accuracy to a state of the art sign recogniser when applied to spotting a sequence of signs.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 58s "}}
{"id": "1dea5ec2-d311-4c03-bba5-e38d7a62fbd4", "displayed_text": "Title: Sign Spotting Using Hierarchical Sequential Patterns with Temporal Intervals\n\nAbstract: This paper tackles the problem of spotting a set of signs occuring in videos with sequences of signs. To achieve this, we propose to model the spatio-temporal signatures of a sign using an extension of sequential patterns that contain temporal intervals called Sequential Interval Patterns (SIP). We then propose a novel multi-class classifier that organises different sequential interval patterns in a hierarchical tree structure called a Hierarchical SIP Tree (HSP-Tree). This allows one to exploit any subsequence sharing that exists between different SIPs of different classes. Multiple trees are then combined together into a forest of HSP-Trees resulting in a strong classifier that can be used to spot signs. We then show how the HSP-Forest can be used to spot sequences of signs that occur in an input video. We have evaluated the method on both concatenated sequences of isolated signs and continuous sign sequences. We also show that the proposed method is superior in robustness and accuracy to a state of the art sign recogniser when applied to spotting a sequence of signs.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "To tackle the problem of spotting a set of signs occurring in videos with sequences of signs, the authors propose to model the spatio-temporal signatures of a sign using an extension of sequential patterns that contain temporal intervals called Sequential Interval Patterns (SIP). They further propose a novel multi-class classifier that organises different sequential interval patterns in a hierarchical tree structure called a Hierarchical SIP Tree (HSP-Tree).", "Method": "The authors evaluate the method on both concatenated sequences of isolated signs and continuous sign sequences. ", "Outcome": "The authors show that the proposed method is superior in robustness and accuracy to a state of the art sign recogniser when applied to spotting a sequence of signs.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 9s "}}
{"id": "1e396f93-a73e-4d33-9a8e-56097a8c3c28", "displayed_text": "Title: Deep Active Learning for Named Entity Recognition\n\nAbstract: Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.", "label_annotations": {"Multi-aspect Summary": {"Context": "Although deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER), it typically requires large amounts of labeled data. ", "Key idea": "The authors introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a long short term memory (LSTM) tag decoder to speed up active learning.", "Method": "The authors evaluate CNN-CNN-LSTM model performance on standard datasets.", "Outcome": "CNN-CNN-LSTM model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. \r\nDuring the training process, incremental active learning has been carried out, CNN-CNN-LSTM mode are able to nearly match state-of-the-art performance with just 25% of the original training data.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 49m 13s "}}
{"id": "1e396f93-a73e-4d33-9a8e-56097a8c3c28", "displayed_text": "Title: Deep Active Learning for Named Entity Recognition\n\nAbstract: Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In this work, we demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning. While active learning is sample-efficient, it can be computationally expensive since it requires iterative retraining. To speed this up, we introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a long short term memory (LSTM) tag decoder. The model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. We carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25\\% of the original training data.", "label_annotations": {"Multi-aspect Summary": {"Context": "Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data.", "Key idea": "The authors demonstrate that the amount of labeled training data can be drastically reduced when deep learning is combined with active learning, and introduce a lightweight architecture for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and word encoders and a long short term memory (LSTM) tag decoder to speed up the process.", "Method": "The authors evaluate their method on standard datasets.", "Outcome": "The proposed model achieves nearly state-of-the-art performance on standard datasets for the task while being computationally much more efficient than best performing models. The authors carry out incremental active learning, during the training process, and are able to nearly match state-of-the-art performance with just 25% of the original training data.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 45s "}}
{"id": "1e5571af-71cf-40b8-ba2a-18b42cae5b42", "displayed_text": "Title: Knowledge-based sequence mining with ASP\n\nAbstract: We introduce a framework for knowledge-based sequence mining, based on Answer Set Programming (ASP). We begin by modeling the basic task and refine it in the sequel in several ways. First, we show how easily condensed patterns can be extracted by modular extensions of the basic approach. Second, we illustrate how ASPu0027s preference handling capacities can be exploited for mining patterns of interest. In doing so, we demonstrate the ease of incorporating knowledge into the ASP-based mining process. To assess the trade-off in effectiveness, we provide an empirical study comparing our approach with a related sequence mining mechanism.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors introduce a framework for knowledge-based sequence mining, based on Answer Set Programming (ASP). ", "Method": "The authors provide an empirical study comparing new approach with a related sequence mining mechanism to assess the trade-off in effectiveness.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 18s "}}
{"id": "1e5571af-71cf-40b8-ba2a-18b42cae5b42", "displayed_text": "Title: Knowledge-based sequence mining with ASP\n\nAbstract: We introduce a framework for knowledge-based sequence mining, based on Answer Set Programming (ASP). We begin by modeling the basic task and refine it in the sequel in several ways. First, we show how easily condensed patterns can be extracted by modular extensions of the basic approach. Second, we illustrate how ASPu0027s preference handling capacities can be exploited for mining patterns of interest. In doing so, we demonstrate the ease of incorporating knowledge into the ASP-based mining process. To assess the trade-off in effectiveness, we provide an empirical study comparing our approach with a related sequence mining mechanism.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors introduce a framework for knowledge-based sequence mining, based on Answer Set Programming (ASP).", "Method": "The authors demonstrate the ease of incorporating knowledge into the ASP-based mining process, and further provide an empirical study comparing the proposed approach with a related sequence mining mechanism.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 8m 59s "}}
{"id": "1ef9b762-e9be-46c5-ad19-090fe16200c4", "displayed_text": "Title: Visual estimation of pointed targets for robot guidance via fusion of face pose and hand orientation\n\nAbstract: In this paper we address an important issue in human-robot interaction, that of accurately deriving pointing information from a corresponding gesture. Based on the fact that in most applications it is the pointed object rather than the actual pointing direction which is important, we formulate a novel approach which takes into account prior information about the location of possible pointing targets. To decide about the pointed object, the proposed approach uses the Dempster-Shafer theory of evidence to fuse information from two different input streams: head pose, estimated by visually tracking the off-plane rotations of the face, and hand pointing orientation. Detailed experimental results are presented that validate the effectiveness of the method in realistic application setups.", "label_annotations": {"Multi-aspect Summary": {"Context": "Accurately deriving pointing information from a corresponding gesture is an important issue in human-robot interaction.", "Key idea": "Based on the fact that in most applications it is the pointed object rather than the actual pointing direction which is important, authors formulate a novel approach which takes into account prior information about the location of possible pointing targets. \r\nTo decide about the pointed object, the proposed approach uses the Dempster-Shafer theory of evidence to fuse information from two different input streams: head pose, estimated by visually tracking the off-plane rotations of the face, and hand pointing orientation.", "Method": "Authors design detailed experiments to validate the propose method in  realistic application setups..", "Outcome": "Authors present detailed experimental results that validate the effectiveness of the method in realistic application setups.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 59s "}}
{"id": "1ef9b762-e9be-46c5-ad19-090fe16200c4", "displayed_text": "Title: Visual estimation of pointed targets for robot guidance via fusion of face pose and hand orientation\n\nAbstract: In this paper we address an important issue in human-robot interaction, that of accurately deriving pointing information from a corresponding gesture. Based on the fact that in most applications it is the pointed object rather than the actual pointing direction which is important, we formulate a novel approach which takes into account prior information about the location of possible pointing targets. To decide about the pointed object, the proposed approach uses the Dempster-Shafer theory of evidence to fuse information from two different input streams: head pose, estimated by visually tracking the off-plane rotations of the face, and hand pointing orientation. Detailed experimental results are presented that validate the effectiveness of the method in realistic application setups.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors address the important issue of accurately deriving pointing information from a corresponding gesture in human-robot interaction. They formulate a novel approach which takes into account prior information about the location of possible pointing targets.", "Method": "The authors evaluate the proposed method in realistic application setups.", "Outcome": "Detailed experimental results are presented that validate the effectiveness of the method in realistic application setups.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 4s "}}
{"id": "2065b977-7782-4981-ad70-3121a2315687", "displayed_text": "Title: Correlated Bigram LSA for Unsupervised Language Model Adaptation\n\nAbstract: We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. The model is trained using efficient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. We address the scalability issue to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%-8% relative perplexity reduction and 2.5% relative character error rate reduction which is statistically significant compared to applying only unigram LSA. On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved which is also statistically significant.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The author presents a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. The model is trained using efficient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing. The scalability is achieved via bootstrapping of bigram LSA from unigram LSA, and LM adaptation is produced by integrating unigram and bigram LSA.", "Method": "The author tests the model against the Mandarin RT04 test set and on the large-scale evaluation on Arabic.", "Outcome": "Applying unigram and bigram LSA together yields 6%-8% relative perplexity reduction and 2.5% relative character error rate reduction on the Mandarin RT04 test set. 3% relative word error rate reduction is achieved on the large-scale evaluation on Arabic.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 28s "}}
{"id": "2065b977-7782-4981-ad70-3121a2315687", "displayed_text": "Title: Correlated Bigram LSA for Unsupervised Language Model Adaptation\n\nAbstract: We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. The model is trained using efficient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. We address the scalability issue to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%-8% relative perplexity reduction and 2.5% relative character error rate reduction which is statistically significant compared to applying only unigram LSA. On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved which is also statistically significant.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. The model is trained using efficient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. ", "Method": "The authors compare results of applying unigram and bigram LSA and applying only unigram LSA on the Mandarin RT04 test set and  large-scale evaluation on Arabic. ", "Outcome": "On the Mandarin RT04 test set, applying unigram and bigram LSA together yields 6%-8% relative perplexity reduction and 2.5% relative character error rate reduction compared to applying only unigram LSA. On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved by applyting unigram and bigram LSA together compared to applying only unigram LSA.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 3s "}}
{"id": "206d2d53-dbaf-4a2f-810d-856309d8eb83", "displayed_text": "Title: Neural Head Reenactment with Latent Pose Descriptors\n\nAbstract:   We propose a neural head reenactment system, which is driven by a latent pose representation and is capable of predicting the foreground segmentation alongside the RGB image. The latent pose representation is learned as a part of the entire reenactment system, and the learning process is based solely on image reconstruction losses. We show that despite its simplicity, with a large and diverse enough training dataset, such learning successfully decomposes pose from identity. The resulting system can then reproduce mimics of the driving person and, furthermore, can perform cross-person reenactment. Additionally, we show that the learned descriptors are useful for other pose-related tasks, such as keypoint prediction and pose-based retrieval. ", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The author proposes a neural head reenactment system to predict the foreground segmentation alongside the RGB image. The system is driven by a latent pose representation learned as a part of the entire reenactment system, based solely on image reconstruction losses.", "Method": "The author trains the system with a large and diverse dataset.", "Outcome": "Against a large and diverse training dataset, the learning successfully decomposes pose from identity, and resulting system can then reproduce mimics of the driving person and perform cross-person reenactment. The learned descriptors are useful for other pose-related tasks like keypoint prediction and pose-based retrieval.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 41s "}}
{"id": "206d2d53-dbaf-4a2f-810d-856309d8eb83", "displayed_text": "Title: Neural Head Reenactment with Latent Pose Descriptors\n\nAbstract:   We propose a neural head reenactment system, which is driven by a latent pose representation and is capable of predicting the foreground segmentation alongside the RGB image. The latent pose representation is learned as a part of the entire reenactment system, and the learning process is based solely on image reconstruction losses. We show that despite its simplicity, with a large and diverse enough training dataset, such learning successfully decomposes pose from identity. The resulting system can then reproduce mimics of the driving person and, furthermore, can perform cross-person reenactment. Additionally, we show that the learned descriptors are useful for other pose-related tasks, such as keypoint prediction and pose-based retrieval. ", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors propose a neural head reenactment system, which is driven by a latent pose representation and is capable of predicting the foreground segmentation alongside the RGB image.", "Method": "N/A", "Outcome": "The authors show that their proposed method successfully decomposes pose from identity, and that the learned descriptors are useful for other pose-related tasks, such as keypoint prediction and pose-based retrieval.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 44s "}}
{"id": "24e29617-a320-450a-aaa5-19d8700d74b7", "displayed_text": "Title: NOMAD: non-locking, stochastic multi-machine algorithm for asynchronous and decentralized matrix completion\n\nAbstract: We develop an efficient parallel distributed algorithm for matrix completion, named NOMAD (Non-locking, stOchastic Multi-machine algorithm for Asynchronous and Decentralized matrix completion). NOMAD is a decentralized algorithm with non-blocking communication between processors. One of the key features of NOMAD is that the ownership of a variable is asynchronously transferred between processors in a decentralized fashion. As a consequence it is a lock-free parallel algorithm. In spite of being asynchronous, the variable updates of NOMAD are serializable, that is, there is an equivalent update ordering in a serial implementation. NOMAD outperforms synchronous algorithms which require explicit bulk synchronization after every iteration: our extensive empirical evaluation shows that not only does our algorithm perform well in distributed setting on commodity hardware, but also outperforms state-of-the-art algorithms on a HPC cluster both in multi-core and distributed memory settings.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors develop an efficient parallel distributed algorithm for matrix completion, named NOMAD (Non-locking, stOchastic Multi-machine algorithm for Asynchronous and Decentralized matrix completion).", "Method": "The authors evaluate NOMAD algorithm performance on commodity hardware and on a HPC cluster both in multi-core and distributed memory settings.", "Outcome": "Extensive empirical evaluation shows that NOMAD algorithm performs well in distributed setting on commodity hardware, and outperforms state-of-the-art algorithms on a HPC cluster both in multi-core and distributed memory settings.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 1s "}}
{"id": "24e29617-a320-450a-aaa5-19d8700d74b7", "displayed_text": "Title: NOMAD: non-locking, stochastic multi-machine algorithm for asynchronous and decentralized matrix completion\n\nAbstract: We develop an efficient parallel distributed algorithm for matrix completion, named NOMAD (Non-locking, stOchastic Multi-machine algorithm for Asynchronous and Decentralized matrix completion). NOMAD is a decentralized algorithm with non-blocking communication between processors. One of the key features of NOMAD is that the ownership of a variable is asynchronously transferred between processors in a decentralized fashion. As a consequence it is a lock-free parallel algorithm. In spite of being asynchronous, the variable updates of NOMAD are serializable, that is, there is an equivalent update ordering in a serial implementation. NOMAD outperforms synchronous algorithms which require explicit bulk synchronization after every iteration: our extensive empirical evaluation shows that not only does our algorithm perform well in distributed setting on commodity hardware, but also outperforms state-of-the-art algorithms on a HPC cluster both in multi-core and distributed memory settings.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors develop an efficient parallel distributed algorithm for matrix completion, named NOMAD (Non-locking, stOchastic Multi-machine algorithm for Asynchronous and Decentralized matrix completion).", "Method": "The authors evaluate their NOMAD algorithm in distributed setting on commodity hardware and in multi-core and distributed memory settings on a HPC cluster.", "Outcome": "NOMAD outperforms synchronous algorithms which require explicit bulk synchronization after every iteration. The extensive empirical evaluation shows that not only does our algorithm perform well in distributed setting on commodity hardware, but also outperforms state-of-the-art algorithms on a HPC cluster both in multi-core and distributed memory settings.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 36s "}}
{"id": "29dd9fd3-6c98-4e4b-b70c-0474ff361419", "displayed_text": "Title: Birds of a Feather: Capturing Avian Shape Models from Images\n\nAbstract: Animals are diverse in shape, but building a deformable shape model for a new species is not always possible due to the lack of 3D data. We present a method to capture new species using an articulated template and images of that species. In this work, we focus mainly on birds. Although birds represent almost twice the number of species as mammals, no accurate shape model is available. To capture a novel species, we first fit the articulated template to each training sample. By disentangling pose and shape, we learn a shape space that captures variation both among species and within each species from image evidence. We learn models of multiple species from the CUB dataset, and contribute new species-specific and multi-species shape models that are useful for downstream reconstruction tasks. Using a low-dimensional embedding, we show that our learned 3D shape space better reflects the phylogenetic relationships among birds than learned perceptual features.", "label_annotations": {"Multi-aspect Summary": {"Context": "Animals are diverse in shape, but building a deformable shape model for a new species is not always possible due to the lack of 3D data.", "Key idea": "The authors present a method to capture new species using an articulated template and images of that species. They learn models of multiple species from the CUB dataset, and contribute new species-specific and multi-species shape models that are useful for downstream reconstruction tasks.", "Method": "The authors compare their learned 3D shape space with learned perceptual features.", "Outcome": "Using a low-dimensional embedding, the authors show that the learned 3D shape space better reflects the phylogenetic relationships among birds than learned perceptual features.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 17s "}}
{"id": "29dd9fd3-6c98-4e4b-b70c-0474ff361419", "displayed_text": "Title: Birds of a Feather: Capturing Avian Shape Models from Images\n\nAbstract: Animals are diverse in shape, but building a deformable shape model for a new species is not always possible due to the lack of 3D data. We present a method to capture new species using an articulated template and images of that species. In this work, we focus mainly on birds. Although birds represent almost twice the number of species as mammals, no accurate shape model is available. To capture a novel species, we first fit the articulated template to each training sample. By disentangling pose and shape, we learn a shape space that captures variation both among species and within each species from image evidence. We learn models of multiple species from the CUB dataset, and contribute new species-specific and multi-species shape models that are useful for downstream reconstruction tasks. Using a low-dimensional embedding, we show that our learned 3D shape space better reflects the phylogenetic relationships among birds than learned perceptual features.", "label_annotations": {"Multi-aspect Summary": {"Context": "Building a deformable shape model for a new species is not always possible due to the lack of 3D data.", "Key idea": "The authors propose a method to capture new species using an articulated template and images of that species. ", "Method": "The authors learn models of multiple species from the CUB dataset, and contribute new species-specific and multi-species shape models that are useful for downstream reconstruction tasks. Using a low-dimensional embedding, the authors compare learned 3D shape space with learned perceptual features.", "Outcome": "Experiments prove that learned 3D shape space better reflects the phylogenetic relationships among birds than learned perceptual features.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 24s "}}
{"id": "2a220303-8653-497f-b2b5-c829583c2714", "displayed_text": "Title: Polynomial-Time Algorithms For Counting And Sampling Markov Equivalent Dags\n\nAbstract: Counting and sampling directed acyclic graphs from a Markov equivalence class are fundamental tasks in graphical causal analysis. In this paper, we show that these tasks can be performed in polynomial time, solving a long-standing open problem in this area. Our algorithms are effective and easily implementable. Experimental results show that the algorithms significantly outperform state-of-the-art methods.", "label_annotations": {"Multi-aspect Summary": {"Context": "Counting and sampling directed acyclic graphs from a Markov equivalence class are fundamental tasks in graphical causal analysis. ", "Key idea": "The authors propose algorithms that can solve the problems that counting and sampling directed acyclic graphs from a Markov equivalence class in polynomial time.", "Method": "N/A", "Outcome": "Experimental results show that the algorithms significantly outperform state-of-the-art methods on tasks that counting and sampling directed acyclic graphs from a Markov equivalence class.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 27s "}}
{"id": "2a220303-8653-497f-b2b5-c829583c2714", "displayed_text": "Title: Polynomial-Time Algorithms For Counting And Sampling Markov Equivalent Dags\n\nAbstract: Counting and sampling directed acyclic graphs from a Markov equivalence class are fundamental tasks in graphical causal analysis. In this paper, we show that these tasks can be performed in polynomial time, solving a long-standing open problem in this area. Our algorithms are effective and easily implementable. Experimental results show that the algorithms significantly outperform state-of-the-art methods.", "label_annotations": {"Multi-aspect Summary": {"Context": "Counting and sampling directed acyclic graphs from a Markov equivalence class are fundamental tasks in graphical causal analysis.", "Key idea": "The authors show that counting and sampling directed acyclic graphs from a Markov equivalence class can be performed in polynomial time, solving a long-standing open problem in this area.", "Method": "N/A", "Outcome": "Experimental results show that the proposed algorithms significantly outperform state-of-the-art methods.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 7s "}}
{"id": "2b5cc037-4841-4fb1-85ff-673230198be1", "displayed_text": "Title: Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling\n\nAbstract: We study the problem of recovering an incomplete $m\\times n$ matrix of rank $r$ with columns arriving online over time. This is known as the problem of life-long matrix completion, and is widely applied to recommendation system, computer vision, system identification, etc. The challenge is to design provable algorithms tolerant to a large amount of noises, with small sample complexity. In this work, we give algorithms achieving strong guarantee under two realistic noise models. In bounded deterministic noise, an adversary can add any bounded yet unstructured noise to each column. For this problem, we present an algorithm that returns a matrix of a small error, with sample complexity almost as small as the best prior results in the noiseless case. For sparse random noise, where the corrupted columns are sparse and drawn randomly, we give an algorithm that exactly recovers an $\\mu_0$-incoherent matrix by probability at least $1-\\delta$ with sample complexity as small as $O(\\mu_0rn\\log(r/\\delta))$. This result advances the state-of-the-art work and matches the lower bound in a worst case. We also study the scenario where the hidden matrix lies on a mixture of subspaces and show that the sample complexity can be even smaller. Our proposed algorithms perform well experimentally in both synthetic and real-world datasets.", "label_annotations": {"Multi-aspect Summary": {"Context": "It's challenging to design provable algorithms that can recover an incomplete m* n matrix of rank r, while being tolerant to a large amount of noises, with small sample complexity. ", "Key idea": "The authors propose algorithms that can recover an incomplete m* n matrix of rank r, providing strong guarantee under two realistic noise models.", "Method": "The authors compare new algorithms performance in both bounded deterministic noise and noiseless case.\r\nThe authors study the new algorithms performance under scenario where the hidden matrix lies on a mixture of subspaces.\r\nThe authors conduct experiments to evaluate new algorithms performance in both synthetic and real-world datasets.", "Outcome": "The authors prove that new algorithms can return a matrix of a small error, with sample complexity almost as small as the best prior results in the noiseless case and new algorithms perform well experimentally in both synthetic and real-world datasets.\r\nThe sample complexity can be even smaller when apply new algorithms under scenario where the hidden matrix lies on a mixture of subspaces .", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 10m 31s "}}
{"id": "2b5cc037-4841-4fb1-85ff-673230198be1", "displayed_text": "Title: Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling\n\nAbstract: We study the problem of recovering an incomplete $m\\times n$ matrix of rank $r$ with columns arriving online over time. This is known as the problem of life-long matrix completion, and is widely applied to recommendation system, computer vision, system identification, etc. The challenge is to design provable algorithms tolerant to a large amount of noises, with small sample complexity. In this work, we give algorithms achieving strong guarantee under two realistic noise models. In bounded deterministic noise, an adversary can add any bounded yet unstructured noise to each column. For this problem, we present an algorithm that returns a matrix of a small error, with sample complexity almost as small as the best prior results in the noiseless case. For sparse random noise, where the corrupted columns are sparse and drawn randomly, we give an algorithm that exactly recovers an $\\mu_0$-incoherent matrix by probability at least $1-\\delta$ with sample complexity as small as $O(\\mu_0rn\\log(r/\\delta))$. This result advances the state-of-the-art work and matches the lower bound in a worst case. We also study the scenario where the hidden matrix lies on a mixture of subspaces and show that the sample complexity can be even smaller. Our proposed algorithms perform well experimentally in both synthetic and real-world datasets.", "label_annotations": {"Multi-aspect Summary": {"Context": "The problem of life-long matrix completion is widely applied to recommendation system, computer vision, system identification. The challenge is to design provable algorithms tolerant to a large amount of noises, with small sample complexity.", "Key idea": "The authors give algorithms achieving strong guarantee under two realistic noise models.", "Method": "The authors evaluate their method in both synthetic and real-world datasets.\r\n", "Outcome": "The theoretical result advances the state-of-the-art work and matches the lower bound in a worst case.. The proposed algorithms also perform well experimentally in both synthetic and real-world datasets.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 10s "}}
{"id": "2eb66e5a-472c-4db1-b02f-47fe5eb7e71e", "displayed_text": "Title: Exploiting Inductive Bias in Transformers for Unsupervised Disentanglement of Syntax and Semantics with VAEs\n\nAbstract:   We propose a generative model for text generation, which exhibits disentangled latent representations of syntax and semantics. Contrary to previous work, this model does not need syntactic information such as constituency parses, or semantic information such as paraphrase pairs. Our model relies solely on the inductive bias found in attention-based architectures such as Transformers.   In the attention of Transformers, keys handle information selection while values specify what information is conveyed. Our model, dubbed QKVAE, uses Attention in its decoder to read latent variables where one latent variable infers keys while another infers values. We run experiments on latent representations and experiments on syntax/semantics transfer which show that QKVAE displays clear signs of disentangled syntax and semantics. We also show that our model displays competitive syntax transfer capabilities when compared to supervised models and that comparable supervised models need a fairly large amount of data (more than 50K samples) to outperform it on both syntactic and semantic transfer. The code for our experiments is publicly available. ", "label_annotations": {"Multi-aspect Summary": {"Context": "How to use the Inductive Bias in Transformers for Unsupervised Disentanglement is under explored.", "Key idea": "Authors propose a generative model, dubbed QKVAE, that uses Attention in its decoder to read latent variables where one latent variable infers keys while another infers values.\r\nThe model relies solely on the inductive bias found in attention-based architectures such as Transformers.", "Method": "Authors run experiments on latent representations and experiments on syntax/semantics transfer.", "Outcome": "Authors show that QKVAE displays clear signs of disentangled syntax and semantics. Authors also show that the model displays competitive syntax transfer capabilities when compared to supervised models and that comparable supervised models need a fairly large amount of data (more than 50K samples) to outperform it on both syntactic and semantic transfer.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 50s "}}
{"id": "2eb66e5a-472c-4db1-b02f-47fe5eb7e71e", "displayed_text": "Title: Exploiting Inductive Bias in Transformers for Unsupervised Disentanglement of Syntax and Semantics with VAEs\n\nAbstract:   We propose a generative model for text generation, which exhibits disentangled latent representations of syntax and semantics. Contrary to previous work, this model does not need syntactic information such as constituency parses, or semantic information such as paraphrase pairs. Our model relies solely on the inductive bias found in attention-based architectures such as Transformers.   In the attention of Transformers, keys handle information selection while values specify what information is conveyed. Our model, dubbed QKVAE, uses Attention in its decoder to read latent variables where one latent variable infers keys while another infers values. We run experiments on latent representations and experiments on syntax/semantics transfer which show that QKVAE displays clear signs of disentangled syntax and semantics. We also show that our model displays competitive syntax transfer capabilities when compared to supervised models and that comparable supervised models need a fairly large amount of data (more than 50K samples) to outperform it on both syntactic and semantic transfer. The code for our experiments is publicly available. ", "label_annotations": {"Multi-aspect Summary": {"Context": "Previous generative model for text generation need syntactic information such as constituency parses, or semantic information such as paraphrase pairs.", "Key idea": "The authors propose a generative model for text generation, which exhibits disentangled latent representations of syntax and semantics, QKVAE.", "Method": "The authors use their proposed QKVAE run experiments on latent representations and experiments on syntax/semantics transfer", "Outcome": "The experiments on latent representations and experiments on syntax/semantics transfer show that QKVAE displays clear signs of disentangled syntax and semantics. The authors also show that the model displays competitive syntax transfer capabilities when compared to supervised models and that comparable supervised models need a fairly large amount of data (more than 50K samples) to outperform it on both syntactic and semantic transfer", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 19s "}}
{"id": "31ab88b5-e66f-4b69-98b6-7a470dce9875", "displayed_text": "Title: Conditioning Sequence-to-sequence Networks with Learned Activations\n\nAbstract: Conditional neural networks play an important role in a number of sequence-to-sequence modeling tasks, including personalized sound enhancement (PSE), speaker dependent automatic speech recognition (ASR), and generative modeling such as text-to-speech synthesis. In conditional neural networks, the output of a model is often influenced by a conditioning vector, in addition to the input. Common approaches of conditioning include input concatenation or modulation with the conditioning vector, which comes at a cost of increased model size. In this work, we introduce a novel approach of neural network conditioning by learning intermediate layer activations based on the conditioning vector. We systematically explore and show that learned activation functions can produce conditional models with comparable or better quality, while decreasing model sizes, thus making them ideal candidates for resource-efficient on-device deployment. As exemplary target use-cases we consider (i) the task of PSE as a pre-processing technique for improving telephony or pre-trained ASR performance under noise, and (ii) personalized ASR in single speaker scenarios. We find that conditioning via activation function learning is an effective modeling strategy, suggesting a broad applicability of the proposed technique across a number of application domains.", "label_annotations": {"Multi-aspect Summary": {"Context": "Common approaches of conditioning include input concatenation or modulation with the conditioning vector, which comes at a cost of increased model size.", "Key idea": "The authors introduce a novel approach of neural network conditioning by learning intermediate layer activations based on the conditioning vector.", "Method": "The authors systematically explore novel neural network conditioning approach and conduct experiment to compare novel neural network conditioning approach's performance to traditional conditioning methods.", "Outcome": "Experiments demonstrate that conditional  models using learned activation functions can have comparable or better quality than traditional methods, while decreasing model sizes, thus making them ideal candidates for resource-efficient on-device deployment.", "Future Impact": "This novel neural network conditioning approach may have a broad applicability of the proposed technique across a number of application domains."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "31ab88b5-e66f-4b69-98b6-7a470dce9875", "displayed_text": "Title: Conditioning Sequence-to-sequence Networks with Learned Activations\n\nAbstract: Conditional neural networks play an important role in a number of sequence-to-sequence modeling tasks, including personalized sound enhancement (PSE), speaker dependent automatic speech recognition (ASR), and generative modeling such as text-to-speech synthesis. In conditional neural networks, the output of a model is often influenced by a conditioning vector, in addition to the input. Common approaches of conditioning include input concatenation or modulation with the conditioning vector, which comes at a cost of increased model size. In this work, we introduce a novel approach of neural network conditioning by learning intermediate layer activations based on the conditioning vector. We systematically explore and show that learned activation functions can produce conditional models with comparable or better quality, while decreasing model sizes, thus making them ideal candidates for resource-efficient on-device deployment. As exemplary target use-cases we consider (i) the task of PSE as a pre-processing technique for improving telephony or pre-trained ASR performance under noise, and (ii) personalized ASR in single speaker scenarios. We find that conditioning via activation function learning is an effective modeling strategy, suggesting a broad applicability of the proposed technique across a number of application domains.", "label_annotations": {"Multi-aspect Summary": {"Context": "In conditional neural networks, the output of a model is often influenced by a conditioning vector, in addition to the input. Common approaches of conditioning include input concatenation or modulation with the conditioning vector, which comes at a cost of increased model size.", "Key idea": "The author introduces a novel approach of neural network conditioning by learning intermediate layer activations based on the conditioning vector.", "Method": "As exemplary target use-cases the author considers (i) the task of PSE as a pre-processing technique for improving telephony or pre-trained ASR performance under noise, and (ii) personalized ASR in single speaker scenarios.", "Outcome": "The learned activation functions can produce conditional models with comparable or better quality, while decreasing model sizes, thus making them ideal candidates for resource-efficient on-device deployment. The author finds that conditioning via activation function learning is an effective modeling strategy.", "Future Impact": "Conditioning via activation function learning has a broad applicability of the proposed technique across a number of application domains."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 34s "}}
{"id": "36d7073a-d006-4a4a-850f-ca7a3f0ca19b", "displayed_text": "Title: Learning convolutional neural networks for graphs\n\nAbstract: Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.", "label_annotations": {"Multi-aspect Summary": {"Context": "Numerous important problems can be framed as learning from graph data.", "Key idea": "The author proposes a framework for learning convolutional neural networks for arbitrary graphs, where the graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. The author presents a general approach to extracting locally connected regions from graphs analogous to image-based convolutional networks.", "Method": "The authors test the learned feature representations and computation efficiency proposed framework on established benchmark data sets.", "Outcome": "Using established benchmark data sets, the author demonstrates that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 26s "}}
{"id": "36d7073a-d006-4a4a-850f-ca7a3f0ca19b", "displayed_text": "Title: Learning convolutional neural networks for graphs\n\nAbstract: Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.", "label_annotations": {"Multi-aspect Summary": {"Context": "Numerous important problems can be framed as learning from graph data.", "Key idea": "The authors propose a framework for learning convolutional neural networks for arbitrary graphs.", "Method": "The authors evaluated their proposed framework on established benchmark data sets.", "Outcome": "The authors demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient on established benchmark data sets.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 58s "}}
{"id": "370141c7-e1bb-4010-9938-efcad6cf2e62", "displayed_text": "Title: GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training.\n\nAbstract: Innovations in neural architectures have fostered significant breakthroughs in language modeling and computer vision. Unfortunately, novel architectures often result in challenging hyper-parameter choices and training instability if the network parameters are not properly initialized. A number of architecture-specific initialization schemes have been proposed, but these schemes are not always portable to new architectures. This paper presents GradInit, an automated and architecture agnostic method for initializing neural networks. GradInit is based on a simple heuristic; the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. GradInit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation, enabling training it without learning rate warmup using either Adam or SGD under a wide range of learning rates and momentum coefficients. Code is available at https://github.com/zhuchen03/gradinit.", "label_annotations": {"Multi-aspect Summary": {"Context": "Novel architectures often result in challenging hyper-parameter choices and training instability if the network parameters are not properly initialized. A number of architecture-specific initialization schemes have been proposed, but these schemes are not always portable to new architectures.", "Key idea": "This paper presents GradInit, an automated and architecture agnostic method for initializing neural networks. GradInit is based on a simple heuristic; the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value.", "Method": "The authors test convergence and test performance of many convolutional architectures of Gradlnit. And compare Gradlnit's stability with original Transformer architectures' stability for machine translation.", "Outcome": "GradInit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. GradInit also improves the stability of the original Transformer architecture for machine translation, enabling training it without learning rate warmup using either Adam or SGD under a wide range of learning rates and momentum coefficients. ", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 2s "}}
{"id": "370141c7-e1bb-4010-9938-efcad6cf2e62", "displayed_text": "Title: GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training.\n\nAbstract: Innovations in neural architectures have fostered significant breakthroughs in language modeling and computer vision. Unfortunately, novel architectures often result in challenging hyper-parameter choices and training instability if the network parameters are not properly initialized. A number of architecture-specific initialization schemes have been proposed, but these schemes are not always portable to new architectures. This paper presents GradInit, an automated and architecture agnostic method for initializing neural networks. GradInit is based on a simple heuristic; the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. GradInit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation, enabling training it without learning rate warmup using either Adam or SGD under a wide range of learning rates and momentum coefficients. Code is available at https://github.com/zhuchen03/gradinit.", "label_annotations": {"Multi-aspect Summary": {"Context": "Novel neural architectures often result in challenging hyper-parameter choices and training instability if the network parameters are not properly initialized. Previously proposed architecture-specific initialization schemes are not always portable to new architectures.", "Key idea": "The author proposes an automated and architecture agnostic method for initializing neural networks, where the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. The adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme.", "Method": "The author tests the proposed approach\u2019s performance against many convolutional architectures, both with or without skip connections, or without normalization layers. They also test stability of the approach against the original Transformer architecture for machine translation.", "Outcome": "The proposed approach accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation, enabling training it without learning rate warmup using either Adam or SGD under a wide range of learning rates and momentum coefficients.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 27s "}}
{"id": "37da719b-8db9-4523-8a4a-3f2eef238978", "displayed_text": "Title: Tamper-proof image watermarking using self-embedding\n\nAbstract: We propose a fragile watermarking with self-embedding for recovery of tampered image that does not use authentication bits. We use a robust spread spectrum based watermarking scheme using block based embedding, DCT based compression, and other improvements. Simulation results showing recovery performance are presented.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors propose a fragile watermarking with self-embedding for recovery of tampered image that does not use authentication bits.", "Method": "The authors run simulation results of their proposed method.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 20s "}}
{"id": "37da719b-8db9-4523-8a4a-3f2eef238978", "displayed_text": "Title: Tamper-proof image watermarking using self-embedding\n\nAbstract: We propose a fragile watermarking with self-embedding for recovery of tampered image that does not use authentication bits. We use a robust spread spectrum based watermarking scheme using block based embedding, DCT based compression, and other improvements. Simulation results showing recovery performance are presented.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors propose a fragile watermarking with self-embedding for recovery of tampered image that does not use authentication bits.", "Method": "The authors use simulation to evaluate fragile watermarking.", "Outcome": "Simulation results prove fragile watermarking recovery performance.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 14s "}}
{"id": "3ac464c2-2214-4bf2-a6b6-03da2498cb03", "displayed_text": "Title: Distribution-based query scheduling\n\nAbstract: Query scheduling, a fundamental problem in database management systems, has recently received a renewed attention, perhaps in part due to the rise of the database as a service\" (DaaS) model for database deployment. While there has been a great deal of work investigating different scheduling algorithms, there has been comparatively little work investigating what the scheduling algorithms can or should know about the queries to be scheduled. In this work, we investigate the efficacy of using histograms describing the distribution of likely query execution times as input to the query scheduler. We propose a novel distribution-based scheduling algorithm, Shepherd, and show that Shepherd substantially outperforms state-of-the-art point-based methods through extensive experimentation with both synthetic and TPC workloads.", "label_annotations": {"Multi-aspect Summary": {"Context": "Query scheduling, a fundamental problem in database management systems, has recently received a renewed attention, perhaps in part due to the rise of the database as a service\" (DaaS) model for database deployment. While there has been a great deal of work investigating different scheduling algorithms, there has been comparatively little work investigating what the scheduling algorithms can or should know about the queries to be scheduled.", "Key idea": "The authors investigate the efficacy of using histograms describing the distribution of likely query execution times as input to the query scheduler and propose a novel distribution-based scheduling algorithm, Shepherd.", "Method": "The authors evaluate their Shepherd through extensive experimentation with both synthetic and TPC workloads and compare it with state-of-the-art point-based methods .", "Outcome": "The authors show that their Shepherd substantially outperforms state-of-the-art point-based methods through extensive experimentation with both synthetic and TPC workloads.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 54s "}}
{"id": "3ac464c2-2214-4bf2-a6b6-03da2498cb03", "displayed_text": "Title: Distribution-based query scheduling\n\nAbstract: Query scheduling, a fundamental problem in database management systems, has recently received a renewed attention, perhaps in part due to the rise of the database as a service\" (DaaS) model for database deployment. While there has been a great deal of work investigating different scheduling algorithms, there has been comparatively little work investigating what the scheduling algorithms can or should know about the queries to be scheduled. In this work, we investigate the efficacy of using histograms describing the distribution of likely query execution times as input to the query scheduler. We propose a novel distribution-based scheduling algorithm, Shepherd, and show that Shepherd substantially outperforms state-of-the-art point-based methods through extensive experimentation with both synthetic and TPC workloads.", "label_annotations": {"Multi-aspect Summary": {"Context": "While there has been a great deal of work investigating different scheduling algorithms in database management systems, there has been comparatively little work investigating what the scheduling algorithms can or should know about the queries to be scheduled.", "Key idea": "The author proposes a novel distribution-based scheduling algorithm.", "Method": "The author compares the proposed algorithm with state-of-the-art point-based methods through extensive experimentation with both synthetic and TPC workloads.", "Outcome": "The author shows that the proposed algorithm substantially outperforms state-of-the-art point-based methods through extensive experimentation with both synthetic and TPC workloads.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 40s "}}
{"id": "3b076d81-3ed0-4d57-84e8-4145f67052bd", "displayed_text": "Title: THE COMPUTATIONAL COMPLEXITY OF AVOIDING CONVERSATIONAL IMPLICATURES\n\nAbstract: Referring expressions and other object descriptions should be maximal under the Local Brevity, No Unnecessary Components, and Lexical Preference preference rules; otherwise, they may lead hearers to infer unwanted conversational implicatures. These preference rules can be incorporated into a polynomial time generation algorithm, while some alternative formalizations of conversational implicature make the generation task NP-Hard.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "N/A", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 21s "}}
{"id": "3b076d81-3ed0-4d57-84e8-4145f67052bd", "displayed_text": "Title: THE COMPUTATIONAL COMPLEXITY OF AVOIDING CONVERSATIONAL IMPLICATURES\n\nAbstract: Referring expressions and other object descriptions should be maximal under the Local Brevity, No Unnecessary Components, and Lexical Preference preference rules; otherwise, they may lead hearers to infer unwanted conversational implicatures. These preference rules can be incorporated into a polynomial time generation algorithm, while some alternative formalizations of conversational implicature make the generation task NP-Hard.", "label_annotations": {"Multi-aspect Summary": {"Context": "Referring expressions and other object descriptions should be maximal under the Local Brevity, No Unnecessary Components, and Lexical Preference preference rules, or they may lead hearers to infer unwanted conversational implicatures. ", "Key idea": "N/A", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 3s "}}
{"id": "3b6e7572-2ef8-4565-a3b7-301a3fd38acd", "displayed_text": "Title: SEDONA: Search for Decoupled Neural Networks toward Greedy Block-wise Learning\n\nAbstract: Backward locking and update locking are well-known sources of inefficiency in backpropagation that prevent from concurrently updating layers. Several works have recently suggested using local error signals to train network blocks asynchronously to overcome these limitations. However, they often require numerous iterations of trial-and-error to find the best configuration for local training, including how to decouple network blocks and which auxiliary networks to use for each block. In this work, we propose a differentiable search algorithm named SEDONA to automate this process. Experimental results show that our algorithm can consistently discover transferable decoupled architectures for VGG and ResNet variants, and significantly outperforms the ones trained with end-to-end backpropagation and other state-of-the-art greedy-leaning methods in CIFAR-10, Tiny-ImageNet and ImageNet. Thanks to improved parallelism by local training, we also report up to 2\u00d7 speedup over backpropagation in total training time.", "label_annotations": {"Multi-aspect Summary": {"Context": "Backward locking and update locking are sources of inefficiency in backpropagation that prevent from concurrently updating layers. Using local error signals to train network blocks asynchronously are suggested to overcome these limitations, but they often require numerous iterations of trial-and-error to find the best configuration for local training.", "Key idea": "The authors propose a differentiable search algorithm named SEDONA to automate process, carrying on numerous iterations of trial-and-error to find the best configuration for local training, including how to decouple network blocks and which auxiliary networks to use for each block.", "Method": "The authors apply SEDONA search algorithm on VGG and ResNet and record the time of backpropagation after using SEDONA search algorithm.", "Outcome": "SEDONA search algorithm consistently discover transferable decoupled architectures for VGG and ResNet variants, and significantly outperforms the ones trained with end-to-end backpropagation and other state-of-the-art greedy-leaning methods in CIFAR-10, Tiny-ImageNet and ImageNet.\r\nThere are 2\u00d7 speedup over backpropagation in total training time due to improved parallelism by local training.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 2s "}}
{"id": "3b6e7572-2ef8-4565-a3b7-301a3fd38acd", "displayed_text": "Title: SEDONA: Search for Decoupled Neural Networks toward Greedy Block-wise Learning\n\nAbstract: Backward locking and update locking are well-known sources of inefficiency in backpropagation that prevent from concurrently updating layers. Several works have recently suggested using local error signals to train network blocks asynchronously to overcome these limitations. However, they often require numerous iterations of trial-and-error to find the best configuration for local training, including how to decouple network blocks and which auxiliary networks to use for each block. In this work, we propose a differentiable search algorithm named SEDONA to automate this process. Experimental results show that our algorithm can consistently discover transferable decoupled architectures for VGG and ResNet variants, and significantly outperforms the ones trained with end-to-end backpropagation and other state-of-the-art greedy-leaning methods in CIFAR-10, Tiny-ImageNet and ImageNet. Thanks to improved parallelism by local training, we also report up to 2\u00d7 speedup over backpropagation in total training time.", "label_annotations": {"Multi-aspect Summary": {"Context": "Backward locking and update locking are well-known sources of inefficiency in backpropagation that prevent from concurrently updating layers. Several works have recently suggested using local error signals to train network blocks asynchronously to overcome these limitations. However, they often require numerous iterations of trial-and-error to find the best configuration for local training, including how to decouple network blocks and which auxiliary networks to use for each block. ", "Key idea": "Authors propose a differentiable search algorithm named SEDONA to automate this process.", "Method": "Authors test the proposed method against other SOTA methods, in CIFAR-10, Tiny-ImageNet and ImageNet. ", "Outcome": "Experimental results show that our algorithm can consistently discover transferable decoupled architectures for VGG and ResNet variants, and significantly outperforms the ones trained with end-to-end backpropagation and other state-of-the-art greedy-leaning methods in CIFAR-10, Tiny-ImageNet and ImageNet. Authors also report up to 2\u00d7 speedup over backpropagation in total training time.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 20s "}}
{"id": "4164fd9a-7ab8-4013-b416-a205231f10f2", "displayed_text": "Title: Probabilistic plan recognition using off-the-shelf classical planners\n\nAbstract: Plan recognition is the problem of inferring the goals and plans of an agent after observing its behavior. Recently, it has been shown that this problem can be solved efficiently, without the need of a plan library, using slightly modified planning algorithms. In this work, we extend this approach to the more general problem of probabilistic plan recognition where a probability distribution over the set of goals is sought under the assumptions that actions have deterministic effects and both agent and observer have complete information about the initial state. We show that this problem can be solved efficiently using classical planners provided that the probability of a partially observed execution given a goal is defined in terms of the cost difference of achieving the goal under two conditions: complying with the observations, and not complying with them. This cost, and hence the posterior goal probabilities, are computed by means of two calls to a classical planner that no longer has to be modified in any way. A number of examples is considered to illustrate the quality, flexibility, and scalability of the approach.", "label_annotations": {"Multi-aspect Summary": {"Context": "It has been shown that plan recognition, which is the problem of inferring the goals and plans of an agent after observing its behavior, can be solved efficiently, without the need of a plan library, using slightly modified planning algorithms.", "Key idea": "The authors extend the method, using slightly modified planning algorithms, to the more general problem of probabilistic plan recognition where a probability distribution over the set of goals is sought under the assumptions that actions have deterministic effects and both agent and observer have complete information about the initial state. ", "Method": "The authors consider a number of examples  to illustrate the quality, flexibility, and scalability of the new approach.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 18s "}}
{"id": "4164fd9a-7ab8-4013-b416-a205231f10f2", "displayed_text": "Title: Probabilistic plan recognition using off-the-shelf classical planners\n\nAbstract: Plan recognition is the problem of inferring the goals and plans of an agent after observing its behavior. Recently, it has been shown that this problem can be solved efficiently, without the need of a plan library, using slightly modified planning algorithms. In this work, we extend this approach to the more general problem of probabilistic plan recognition where a probability distribution over the set of goals is sought under the assumptions that actions have deterministic effects and both agent and observer have complete information about the initial state. We show that this problem can be solved efficiently using classical planners provided that the probability of a partially observed execution given a goal is defined in terms of the cost difference of achieving the goal under two conditions: complying with the observations, and not complying with them. This cost, and hence the posterior goal probabilities, are computed by means of two calls to a classical planner that no longer has to be modified in any way. A number of examples is considered to illustrate the quality, flexibility, and scalability of the approach.", "label_annotations": {"Multi-aspect Summary": {"Context": "Plan recognition is the problem of inferring the goals and plans of an agent after observing its behavior. Recently, it has been shown that this problem can be solved efficiently, without the need of a plan library, using slightly modified planning algorithms.", "Key idea": "The authors extend the slightly modified planning algorithms to the more general problem of probabilistic plan recognition, and show the efficiency of classical planners in solving this problem.", "Method": "N/A", "Outcome": "The authors illustrate the quality, flexibility, and scalability of the approach using a number of examples.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 8m 18s "}}
{"id": "432f5702-7b7d-4995-812e-40925f1a18dd", "displayed_text": "Title: The Landmark Selection Method for Multiple Output Prediction\n\nAbstract: Conditional modeling x \u2192 y is a central problem in machine learning. A substantial research effort is devoted to such modeling when x is high dimensional. We consider, instead, the case of a high dimensional y, where x is either low dimensional or high dimensional. Our approach is based on selecting a small subset yL of the dimensions of y, and proceed by modeling (i) x \u2192 yL and (ii) yL \u2192 y. Composing these two models, we obtain a conditional model x \u2192 y that possesses convenient statistical properties. Multilabel classification and multivariate regression experiments on several datasets show that this method outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods.", "label_annotations": {"Multi-aspect Summary": {"Context": "Conditional modeling x \u2192 y is a central problem in machine learning, and a substantial research effort is devoted to such modeling when x is high dimensional.", "Key idea": "The authors consider conditional modeling x \u2192 y with a high dimensional y by selecting a small subset yL of the dimensions of y, modeling (i) x \u2192 yL and (ii) yL \u2192 y and composing these two models.", "Method": "The authors run multilabel classification and multivariate regression experiments on several datasets.", "Outcome": "Multilabel classification and multivariate regression experiments on several datasets show that this method outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 50s "}}
{"id": "432f5702-7b7d-4995-812e-40925f1a18dd", "displayed_text": "Title: The Landmark Selection Method for Multiple Output Prediction\n\nAbstract: Conditional modeling x \u2192 y is a central problem in machine learning. A substantial research effort is devoted to such modeling when x is high dimensional. We consider, instead, the case of a high dimensional y, where x is either low dimensional or high dimensional. Our approach is based on selecting a small subset yL of the dimensions of y, and proceed by modeling (i) x \u2192 yL and (ii) yL \u2192 y. Composing these two models, we obtain a conditional model x \u2192 y that possesses convenient statistical properties. Multilabel classification and multivariate regression experiments on several datasets show that this method outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods.", "label_annotations": {"Multi-aspect Summary": {"Context": "Conditional modeling x \u2192 y is a central problem in machine learning.", "Key idea": "The authors propose an method that composing two models, one is modeling (i) x \u2192 yL and the other one is modeling (ii) yL \u2192 y on selecting a small subset yL of the dimensions of y, and obtain a conditional model x \u2192 y that possesses convenient statistical properties in the end.", "Method": "The authors product multilabel classification and multivariate regression experiments on several dataset to evaluate the performance of new method.", "Outcome": "The new method, composing two models, outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods on multilabel classification and multivariate regression tasks.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 2s "}}
{"id": "438caf1c-5c7e-4283-a3cd-bbab302df185", "displayed_text": "Title: Active Learning by Acquiring Contrastive Examples.\n\nAbstract:   Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting \\textit{contrastive examples}, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies. ", "label_annotations": {"Multi-aspect Summary": {"Context": "Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively.", "Key idea": "Leveraging the best of uncertainty and diversity sampling, the author proposes an acquisition function that opts for selecting contrastive examples, which are data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods.", "Method": "The author compares the proposed approach with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. The author also conducts an extensive ablation study of the method and analyzes all actively acquired datasets.", "Outcome": "The experiments show that proposed approach performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. The proposed method also achieves a better trade-off between uncertainty and diversity compared to other strategies.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 47s "}}
{"id": "438caf1c-5c7e-4283-a3cd-bbab302df185", "displayed_text": "Title: Active Learning by Acquiring Contrastive Examples.\n\nAbstract:   Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting \\textit{contrastive examples}, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies. ", "label_annotations": {"Multi-aspect Summary": {"Context": "Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively.", "Key idea": "The authors propose an acquisition function that opts for selecting \"contrastive examples\", i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods.", "Method": "The authors compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. They also conduct an extensive ablation study of the proposed method", "Outcome": "The experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. An extensive ablation study of the proposed method and further analysis show that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 23s "}}
{"id": "45b76955-9670-4664-939c-f5a61eb597df", "displayed_text": "Title: Quantifying the Impact of User Attentionon Fair Group Representation in Ranked Lists\n\nAbstract: In this work, we introduce a novel metric for auditing group fairness in ranked lists. Our approach offers two benefits compared to the state of the art. First, we offer a blueprint for modeling of user attention. Rather than assuming a logarithmic loss in importance as a function of the rank, we can account for varying user behaviors through parametrization. For example, we expect a user to see more items during a viewing of a social media feed than when they inspect the results list of a single web search query. Second, we allow non-binary protected attributes to enable investigating inherently continuous attributes (e.g., political alignment on the liberal to conservative spectrum) as well as to facilitate measurements across aggregated sets of search results, rather than separately for each result list. By combining these two elements into our metric, we are able to better address the human factors inherent in this problem. We measure the whole sociotechnical system, consisting of a ranking algorithm and individuals using it, instead of exclusively focusing on the ranking algorithm. Finally, we use our metric to perform three simulated fairness audits. We show that determining fairness of a ranked output necessitates knowledge (or a model) of the end-users of the particular service. Depending on their attention distribution function, a fixed ranking of results can appear biased both in favor and against a protected group1.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "auditing group fairness in ranked lists is an important problem.", "Key idea": "Authors introduce a novel metric for auditing group fairness in ranked lists. Authors first offer a blueprint for modeling of user attention. Second, authors allow non-binary protected attributes to enable investigating inherently continuous attributes (e.g., political alignment on the liberal to conservative spectrum) as well as to facilitate measurements across aggregated sets of search results, rather than separately for each result list.", "Method": "Authors use the proposed metric to perform three simulated fairness audits.", "Outcome": "The proposed metrics are able to better address the human factors inherent in this problem. Authors show that determining fairness of a ranked output necessitates knowledge (or a model) of the end-users of the particular service. Depending on their attention distribution function, a fixed ranking of results can appear biased both in favor and against a protected group1.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 8s "}}
{"id": "45b76955-9670-4664-939c-f5a61eb597df", "displayed_text": "Title: Quantifying the Impact of User Attentionon Fair Group Representation in Ranked Lists\n\nAbstract: In this work, we introduce a novel metric for auditing group fairness in ranked lists. Our approach offers two benefits compared to the state of the art. First, we offer a blueprint for modeling of user attention. Rather than assuming a logarithmic loss in importance as a function of the rank, we can account for varying user behaviors through parametrization. For example, we expect a user to see more items during a viewing of a social media feed than when they inspect the results list of a single web search query. Second, we allow non-binary protected attributes to enable investigating inherently continuous attributes (e.g., political alignment on the liberal to conservative spectrum) as well as to facilitate measurements across aggregated sets of search results, rather than separately for each result list. By combining these two elements into our metric, we are able to better address the human factors inherent in this problem. We measure the whole sociotechnical system, consisting of a ranking algorithm and individuals using it, instead of exclusively focusing on the ranking algorithm. Finally, we use our metric to perform three simulated fairness audits. We show that determining fairness of a ranked output necessitates knowledge (or a model) of the end-users of the particular service. Depending on their attention distribution function, a fixed ranking of results can appear biased both in favor and against a protected group1.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "Previous metrics for auditing group fairness in ranked lists cannot exclude the inherent human factors.", "Key idea": "The authors introduce a novel metric for auditing group fairness in ranked lists, which can be able to better address the human factors inherent in this problem.", "Method": "The authors use novel metric to perform three simulated fairness audits.", "Outcome": "The authors conclude that determining fairness of a ranked output necessitates knowledge (or a model) of the end-users of the particular service. Depending on attention distribution function, a fixed ranking of results can appear biased both in favor and against a protected group1.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "48bacac1-1ca9-4be8-90e6-470596de0e26", "displayed_text": "Title: Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents\n\nAbstract: Recognizing polarity requires a list of polar words and phrases. For the purpose of building such lexicon automatically, a lot of studies have investigated (semi-) unsupervised method of learning polarity of words and phrases. In this paper, we explore to use structural clues that can extract polar sentences from Japanese HTML documents, and build lexicon from the extracted polar sentences. The key idea is to develop the structural clues so that it achieves extremely high precision at the cost of recall. In order to compensate for the low recall, we used massive collection of HTML documents. Thus, we could prepare enough polar sentence corpus.", "label_annotations": {"Multi-aspect Summary": {"Context": "Recognizing polarity requires a list of polar words and phrases. For the purpose of building such lexicon automatically, a lot of studies have investigated (semi-) unsupervised method of learning polarity of words and phrases.", "Key idea": "Authors propose to use structural clues that can extract polar sentences from Japanese HTML documents, and build lexicon from the extracted polar sentences. The key idea is to develop the structural clues so that it achieves extremely high precision at the cost of recall.", "Method": "Authors design experiments with massive collection of HTML documents.", "Outcome": "The proposed method can effectively achieve high precision with low recall.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 57s "}}
{"id": "48bacac1-1ca9-4be8-90e6-470596de0e26", "displayed_text": "Title: Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents\n\nAbstract: Recognizing polarity requires a list of polar words and phrases. For the purpose of building such lexicon automatically, a lot of studies have investigated (semi-) unsupervised method of learning polarity of words and phrases. In this paper, we explore to use structural clues that can extract polar sentences from Japanese HTML documents, and build lexicon from the extracted polar sentences. The key idea is to develop the structural clues so that it achieves extremely high precision at the cost of recall. In order to compensate for the low recall, we used massive collection of HTML documents. Thus, we could prepare enough polar sentence corpus.", "label_annotations": {"Multi-aspect Summary": {"Context": "Recognizing polarity requires a list of polar words and phrases. In order to building such lexicon automatically, many studies have investigated (semi-)unsupervised method of learning polarity of words and phrases. ", "Key idea": "The authors explore to use structural clues that can extract polar sentences from Japanese HTML documents, and build lexicon from the extracted polar sentences and develop the structural clues so that it achieves extremely high precision at the cost of recall.", "Method": "The author use structural clues that can extract polar sentences from Japanese HTML documents, and build lexicon from the extracted polar sentences.", "Outcome": "The author prepare enough polar sentence corpus used to recognizing polarity.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 7s "}}
{"id": "49309d5a-5959-4f8f-ae30-9fd2350f0cbc", "displayed_text": "Title: Focused Quantization for Sparse CNNs\n\nAbstract: Deep convolutional neural networks (CNNs) are powerful tools for a wide range of vision tasks, but the enormous amount of memory and compute resources required by CNNs poses a challenge in deploying them on constrained devices. Existing compression techniques, while excelling at reducing model sizes, struggle to be computationally friendly. In this paper, we attend to the statistical properties of sparse CNNs and present focused quantization, a novel quantization strategy based on power-of-two values, which exploits the weight distributions after fine-grained pruning. The proposed method dynamically discovers the most effective numerical representation for weights in layers with varying sparsities, significantly reducing model sizes. Multiplications in quantized CNNs are replaced with much cheaper bit-shift operations for efficient inference. Coupled with lossless encoding, we build a compression pipeline that provides CNNs with high compression ratios (CR), low computation cost and minimal loss in accuracies. In ResNet-50, we achieved a 18.08x CR with only 0.24% loss in top-5 accuracy, outperforming existing compression methods. We fully compress a ResNet-18 and found that it is not only higher in CR and top-5 accuracy, but also more hardware efficient as it requires fewer logic gates to implement when compared to other state-of-the-art quantization methods assuming the same throughput.", "label_annotations": {"Multi-aspect Summary": {"Context": "CNNs poses a challenge in deploying them on constrained devices since it requires enormous amount of memory and compute resources. Existing compression techniques, while excelling at reducing model sizes, struggle to be computationally friendly.", "Key idea": "The authors propose a novel quantization strategy based on power-of-two values, which exploits the weight distributions after fine-grained pruning when attending to the statistical properties of sparse CNNs and present focused quantization.", "Method": "The authors evaluate novel quantization strategy CR and top-5 accuracy on ResNet-50 and ResNet-18", "Outcome": "In ResNet-50, novel quantization strategy achieved a 18.08x CR with only 0.24% loss in top-5 accuracy, outperforming existing compression methods.\r\nOn ResNet-18, novel quantization not only achieved higher R and top-5 accuracy, but also achieved more hardware efficient as it requires fewer logic gates to implement when compared to other state-of-the-art quantization methods assuming the same throughput.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 58s "}}
{"id": "49309d5a-5959-4f8f-ae30-9fd2350f0cbc", "displayed_text": "Title: Focused Quantization for Sparse CNNs\n\nAbstract: Deep convolutional neural networks (CNNs) are powerful tools for a wide range of vision tasks, but the enormous amount of memory and compute resources required by CNNs poses a challenge in deploying them on constrained devices. Existing compression techniques, while excelling at reducing model sizes, struggle to be computationally friendly. In this paper, we attend to the statistical properties of sparse CNNs and present focused quantization, a novel quantization strategy based on power-of-two values, which exploits the weight distributions after fine-grained pruning. The proposed method dynamically discovers the most effective numerical representation for weights in layers with varying sparsities, significantly reducing model sizes. Multiplications in quantized CNNs are replaced with much cheaper bit-shift operations for efficient inference. Coupled with lossless encoding, we build a compression pipeline that provides CNNs with high compression ratios (CR), low computation cost and minimal loss in accuracies. In ResNet-50, we achieved a 18.08x CR with only 0.24% loss in top-5 accuracy, outperforming existing compression methods. We fully compress a ResNet-18 and found that it is not only higher in CR and top-5 accuracy, but also more hardware efficient as it requires fewer logic gates to implement when compared to other state-of-the-art quantization methods assuming the same throughput.", "label_annotations": {"Multi-aspect Summary": {"Context": "Deep convolutional neural networks (CNNs) are powerful tools for a wide range of vision tasks, but the enormous amount of memory and compute resources required by CNNs poses a challenge in deploying them on constrained devices. Existing compression techniques, while excelling at reducing model sizes, struggle to be computationally friendly.", "Key idea": "The authors present focused quantization, a novel quantization strategy based on power-of-two values, which exploits the weight distributions after fine-grained pruning, based on the statistical properties of sparse CNNs.", "Method": "The authors apply their proposed compression method on ResNet-50 and ResNet-18.", "Outcome": "In ResNet-50, the proposed compression method achieved a 18.08x CR with only 0.24% loss in top-5 accuracy, outperforming existing compression methods. The authors also fully compress a ResNet-18 and find that it is not only higher in CR and top-5 accuracy, but also more hardware efficient", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 34s "}}
{"id": "4e6f8004-9384-4c5c-8d7f-265410a290df", "displayed_text": "Title: Multimodal Shape Completion via Conditional Generative Adversarial Networks\n\nAbstract: Several deep learning methods have been proposed for completing partial data from shape acquisition setups, i.e., filling the regions that were missing in the shape. These methods, however, only complete the partial shape with a single output, ignoring the ambiguity when reasoning the missing geometry. Hence, we pose a multi-modal shape completion problem, in which we seek to complete the partial shape with multiple outputs by learning a one-to-many mapping. We develop the first multimodal shape completion method that completes the partial shape via conditional generative modeling, without requiring paired training data. Our approach distills the ambiguity by conditioning the completion on a learned multimodal distribution of possible results. We extensively evaluate the approach on several datasets that contain varying forms of shape incompleteness, and compare among several baseline methods and variants of our methods qualitatively and quantitatively, demonstrating the merit of our method in completing partial shapes with both diversity and quality.", "label_annotations": {"Multi-aspect Summary": {"Context": "Several deep learning methods have been proposed for completing partial data from shape acquisition setups, i.e., filling the regions that were missing in the shape. These methods, however, only complete the partial shape with a single output, ignoring the ambiguity when reasoning the missing geometry. ", "Key idea": "Authors pose a multi-modal shape completion problem, in which authors seek to complete the partial shape with multiple outputs by learning a one-to-many mapping. Authors develop the first multimodal shape completion method that completes the partial shape via conditional generative modeling, without requiring paired training data. Authors' approach distills the ambiguity by conditioning the completion on a learned multimodal distribution of possible results.", "Method": "Authors build a testbed on several datasets that contain varying forms of shape incompleteness, and compare among several baseline methods and variants of proposed methods qualitatively and quantitatively.", "Outcome": "Authors extensively evaluate the approach on several datasets that contain varying forms of shape incompleteness, and compare among several baseline methods and variants of our methods qualitatively and quantitatively, demonstrating the merit of proposed method in completing partial shapes with both diversity and quality.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 20s "}}
{"id": "4e6f8004-9384-4c5c-8d7f-265410a290df", "displayed_text": "Title: Multimodal Shape Completion via Conditional Generative Adversarial Networks\n\nAbstract: Several deep learning methods have been proposed for completing partial data from shape acquisition setups, i.e., filling the regions that were missing in the shape. These methods, however, only complete the partial shape with a single output, ignoring the ambiguity when reasoning the missing geometry. Hence, we pose a multi-modal shape completion problem, in which we seek to complete the partial shape with multiple outputs by learning a one-to-many mapping. We develop the first multimodal shape completion method that completes the partial shape via conditional generative modeling, without requiring paired training data. Our approach distills the ambiguity by conditioning the completion on a learned multimodal distribution of possible results. We extensively evaluate the approach on several datasets that contain varying forms of shape incompleteness, and compare among several baseline methods and variants of our methods qualitatively and quantitatively, demonstrating the merit of our method in completing partial shapes with both diversity and quality.", "label_annotations": {"Multi-aspect Summary": {"Context": "Several deep learning methods have been proposed for completing partial data from shape acquisition setups, i.e., filling the regions that were missing in the shape. These methods, however, only complete the partial shape with a single output, ignoring the ambiguity when reasoning the missing geometry.", "Key idea": "The authors propose a multi-modal shape completion problem, in which they seek to complete the partial shape with multiple outputs by learning a one-to-many mapping. They further develop the first multimodal shape completion method that completes the partial shape via conditional generative modeling, without requiring paired training data.", "Method": "The authors extensively evaluate the approach on several datasets that contain varying forms of shape incompleteness, and compare among several baseline methods.", "Outcome": "The variants of the proposed methods qualitatively and quantitatively demonstrate the merit in completing partial shapes with both diversity and quality.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 3s "}}
{"id": "4fdcceeb-f50f-4c4f-8b92-5985498114f8", "displayed_text": "Title: The recovery of non-rigid motion from stereo images\n\nAbstract: The problem of recovering the three-dimensional motion of a non-rigid object from a sequence of stereo images is discussed. The object undergoes uniform expansion and three-dimensional shearing about an unknown point in space, in addition to being subjected to rigid motion. Feature correspondence over multiple frames is assumed. The problem of recovering the three-dimensional motion uniquely is reduced to the (unique) solution of a set of homogeneous polynomial equations using algebraic geometry, the commutative algebra software package, MACAULAY, and the Fortran polynomial continuation program POLSYS. It is shown that, with four points correspondence, only two (stereo) snapshots are needed to determine the motion uniquely. u003e", "label_annotations": {"Multi-aspect Summary": {"Context": "The problem of recovering the three-dimensional motion of a non-rigid object from a sequence of stereo images is complex. Because the object undergoes uniform expansion and three-dimensional shearing about an unknown point in space, in addition to being subjected to rigid motion. ", "Key idea": "The authors propose that the problem of recovering the three-dimensional motion uniquely can be reduced to the (unique) solution of a set of homogeneous polynomial equations using algebraic geometry, the commutative algebra software package, MACAULAY, and the Fortran polynomial continuation program POLSYS.", "Method": "The authors use this idea in solving the problem that how to determine the motion uniquely with four points correspondence.", "Outcome": "With four points correspondence, only two (stereo) snapshots are needed to determine the motion uniquely.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 11s "}}
{"id": "4fdcceeb-f50f-4c4f-8b92-5985498114f8", "displayed_text": "Title: The recovery of non-rigid motion from stereo images\n\nAbstract: The problem of recovering the three-dimensional motion of a non-rigid object from a sequence of stereo images is discussed. The object undergoes uniform expansion and three-dimensional shearing about an unknown point in space, in addition to being subjected to rigid motion. Feature correspondence over multiple frames is assumed. The problem of recovering the three-dimensional motion uniquely is reduced to the (unique) solution of a set of homogeneous polynomial equations using algebraic geometry, the commutative algebra software package, MACAULAY, and the Fortran polynomial continuation program POLSYS. It is shown that, with four points correspondence, only two (stereo) snapshots are needed to determine the motion uniquely. u003e", "label_annotations": {"Multi-aspect Summary": {"Context": "The problem of recovering the three-dimensional motion of a non-rigid object from a sequence of stereo images is discussed. The object undergoes uniform expansion and three-dimensional shearing about an unknown point in space, in addition to being subjected to rigid motion. Feature correspondence over multiple frames is assumed. ", "Key idea": "Authors reduce the problem of recovering the three-dimensional motion uniquely to the (unique) solution of a set of homogeneous polynomial equations using algebraic geometry, the commutative algebra software package, MACAULAY, and the Fortran polynomial continuation program POLSYS.\r\n", "Method": "N/A", "Outcome": "Authors show that, with four points correspondence, only two (stereo) snapshots are needed to determine the motion uniquely.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 5s "}}
{"id": "532e797a-4b72-488a-80e4-03713d3c8435", "displayed_text": "Title: Structure from Recurrent Motion: From Rigidity to Recurrency\n\nAbstract: This paper proposes a new method for Non-Rigid Structure-from-Motion (NRSfM) from a long monocular video sequence observing a non-rigid object performing recurrent and possibly repetitive dynamic action. Departing from the traditional idea of using linear low-order or low-rank shape model for the task of NRSfM, our method exploits the property of shape recurrency (i.e., many deforming shapes tend to repeat themselves in time). We show that recurrency is in fact a generalized rigidity. Based on this, we reduce NRSfM problems to rigid ones provided that certain recurrency condition is satisfied. Given such a reduction, standard rigid-SfM techniques are directly applicable (without any change) to the reconstruction of non-rigid dynamic shapes. To implement this idea as a practical approach, this paper develops efficient algorithms for automatic recurrency detection, as well as camera view clustering via a rigidity-check. Experiments on both simulated sequences and real data demonstrate the effectiveness of the method. Since this paper offers a novel perspective on rethinking structure-from-motion, we hope it will inspire other new problems in the field.", "label_annotations": {"Multi-aspect Summary": {"Context": "Previous research proposes the traditional idea of using linear low-order or low-rank shape model for the task of Non-Rigid Structure-from-Motion (NRSfM).", "Key idea": "The authors propose a new method for Non-Rigid Structure-from-Motion (NRSfM) from a long monocular video sequence observing a non-rigid object performing recurrent and possibly repetitive dynamic action.", "Method": "The authors evaluate their proposed method on both simulated sequences and real data.", "Outcome": "Experiments on both simulated sequences and real data demonstrate the effectiveness of the method.", "Future Impact": "This paper offers a novel perspective on rethinking structure-from-motion, so it can potentially inspire other new problems in the field."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 14s "}}
{"id": "532e797a-4b72-488a-80e4-03713d3c8435", "displayed_text": "Title: Structure from Recurrent Motion: From Rigidity to Recurrency\n\nAbstract: This paper proposes a new method for Non-Rigid Structure-from-Motion (NRSfM) from a long monocular video sequence observing a non-rigid object performing recurrent and possibly repetitive dynamic action. Departing from the traditional idea of using linear low-order or low-rank shape model for the task of NRSfM, our method exploits the property of shape recurrency (i.e., many deforming shapes tend to repeat themselves in time). We show that recurrency is in fact a generalized rigidity. Based on this, we reduce NRSfM problems to rigid ones provided that certain recurrency condition is satisfied. Given such a reduction, standard rigid-SfM techniques are directly applicable (without any change) to the reconstruction of non-rigid dynamic shapes. To implement this idea as a practical approach, this paper develops efficient algorithms for automatic recurrency detection, as well as camera view clustering via a rigidity-check. Experiments on both simulated sequences and real data demonstrate the effectiveness of the method. Since this paper offers a novel perspective on rethinking structure-from-motion, we hope it will inspire other new problems in the field.", "label_annotations": {"Multi-aspect Summary": {"Context": "Traditional idea to approach the task of Non-Rigid Structure-from-Motion is by using linear low-order or low-rank shape model.", "Key idea": "The author develops algorithms for automatic recurrency detection and camera view clustering via a rigidity-check by applying standard rigid-SfM techniques to the reconstruction of non-rigid dynamic shapes.", "Method": "The author performs experiments on the proposed algorithms with simulated sequences and real data.", "Outcome": "Experiments on both simulated sequences and real data demonstrate the effectiveness of the method.", "Future Impact": "Since this paper offers a novel perspective on rethinking structure-from-motion, the author hopes it will inspire other new problems in the field."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 27s "}}
{"id": "56992082-e04e-4a8b-a985-abfea27fc2e0", "displayed_text": "Title: Dynamic Network Pruning with Interpretable Layerwise Channel Selection\n\nAbstract: Dynamic network pruning achieves runtime acceleration by dynamically determining the inference paths based on different inputs. However, previous methods directly generate continuous decision values for each weight channel, which cannot reflect a clear and interpretable pruning process. In this paper, we propose to explicitly model the discrete weight channel selections, which encourages more diverse weights utilization, and achieves more sparse runtime inference paths. Meanwhile, with the help of interpretable layerwise channel selections in the dynamic network, we can visualize the network decision paths explicitly for model interpretability. We observe that there are clear differences in the layerwise decisions between normal and adversarial examples. Therefore, we propose a novel adversarial example detection algorithm by discriminating the runtime decision features. Experiments show that our dynamic network achieves higher prediction accuracy under the similar computing budgets on CIFAR10 and ImageNet datasets compared to traditional static pruning methods and other dynamic pruning approaches. The proposed adversarial detection algorithm can significantly improve the state-of-the-art detection rate across multiple attacks, which provides an opportunity to build an interpretable and robust model.", "label_annotations": {"Multi-aspect Summary": {"Context": "Dynamic network pruning achieves runtime acceleration by dynamically determining the inference paths based on different inputs. However, previous methods directly generate continuous decision values for each weight channel, which cannot reflect a clear and interpretable pruning process.", "Key idea": "The author proposes to explicitly model the discrete weight channel selections, which encourages more diverse weights utilization, and achieves more sparse runtime inference paths. The author also proposes a novel adversarial example detection algorithm by discriminating the runtime decision features.", "Method": "The author compares the prediction accuracy of the proposed network on the similar computing budgets on CIFAR10 and ImageNet datasets with traditional static pruning methods and other dynamic pruning approaches.", "Outcome": "Experiments show that the proposed dynamic network achieves higher prediction accuracy under the similar computing budgets on CIFAR10 and ImageNet datasets compared to traditional static pruning methods and other dynamic pruning approaches.", "Future Impact": "The proposed adversarial detection algorithm can significantly improve the state-of-the-art detection rate across multiple attacks, which provides an opportunity to build an interpretable and robust model."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 10m 16s "}}
{"id": "56992082-e04e-4a8b-a985-abfea27fc2e0", "displayed_text": "Title: Dynamic Network Pruning with Interpretable Layerwise Channel Selection\n\nAbstract: Dynamic network pruning achieves runtime acceleration by dynamically determining the inference paths based on different inputs. However, previous methods directly generate continuous decision values for each weight channel, which cannot reflect a clear and interpretable pruning process. In this paper, we propose to explicitly model the discrete weight channel selections, which encourages more diverse weights utilization, and achieves more sparse runtime inference paths. Meanwhile, with the help of interpretable layerwise channel selections in the dynamic network, we can visualize the network decision paths explicitly for model interpretability. We observe that there are clear differences in the layerwise decisions between normal and adversarial examples. Therefore, we propose a novel adversarial example detection algorithm by discriminating the runtime decision features. Experiments show that our dynamic network achieves higher prediction accuracy under the similar computing budgets on CIFAR10 and ImageNet datasets compared to traditional static pruning methods and other dynamic pruning approaches. The proposed adversarial detection algorithm can significantly improve the state-of-the-art detection rate across multiple attacks, which provides an opportunity to build an interpretable and robust model.", "label_annotations": {"Multi-aspect Summary": {"Context": "Previous dynamic network pruning methods directly generate continuous decision values for each weight channel, which cannot reflect a clear and interpretable pruning process. \r\nThere are clear differences in the layerwise decisions between normal and adversarial examples. ", "Key idea": "The authors propose to explicitly model the discrete weight channel selections, which encourages more diverse weights utilization, and achieves more sparse runtime inference paths.\r\nThe authors propose a novel adversarial example detection algorithm by discriminating the runtime decision features.", "Method": "The authors compare their novel dynamic network with  traditional static pruning methods and other dynamic pruning approaches on CIFAR10 and ImageNet datasets under the similar computing budgets.", "Outcome": "Novel dynamic network achieves higher prediction accuracy under the similar computing budgets on CIFAR10 and ImageNet datasets compared to traditional static pruning methods and other dynamic pruning approaches. \r\nThe proposed adversarial detection algorithm can significantly improve the state-of-the-art detection rate across multiple attacks.", "Future Impact": "The proposed adversarial detection algorithm provides an opportunity to build an interpretable and robust model."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 34s "}}
{"id": "58279154-e623-46d2-a431-cc409e094e2c", "displayed_text": "Title: A model of analogy-driven proof-plan construction\n\nAbstract: This paper addresses a model of analogy-driven theorem proving that is more general and cognitively more adequate than previous approaches. The model works at the level of proof-plans. More precisely, we consider analogy as a control strategy in proof planning that employs a source proof-plan to guide the construction of a proof-plan for the target problem. Our approach includes a reformulation of the source proof-plan. This is in accordance with the well known fact that constructing an analogy in maths often amounts to first finding the appropriate representation which brings out the similarity of two problems, i.e., finding the right concepts and the right level of abstraction. Several well known theorems were processed by our analogy-driven proof-plan construction that could not be proven analogically by previous approaches.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "Authors propose a model that works at the level of proof-plans. More precisely, authors consider analogy as a control strategy in proof planning that employs a source proof-plan to guide the construction of a proof-plan for the target problem.", "Method": "Authors design experiments to test whether the proposed analogy-driven proof-plan construction can prove some well known theorems.", "Outcome": "Several well known theorems were processed by our analogy-driven proof-plan construction that could not be proven analogically by previous approaches.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 23s "}}
{"id": "58279154-e623-46d2-a431-cc409e094e2c", "displayed_text": "Title: A model of analogy-driven proof-plan construction\n\nAbstract: This paper addresses a model of analogy-driven theorem proving that is more general and cognitively more adequate than previous approaches. The model works at the level of proof-plans. More precisely, we consider analogy as a control strategy in proof planning that employs a source proof-plan to guide the construction of a proof-plan for the target problem. Our approach includes a reformulation of the source proof-plan. This is in accordance with the well known fact that constructing an analogy in maths often amounts to first finding the appropriate representation which brings out the similarity of two problems, i.e., finding the right concepts and the right level of abstraction. Several well known theorems were processed by our analogy-driven proof-plan construction that could not be proven analogically by previous approaches.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors propose a model of analogy-driven theorem proving that is more general and cognitively more adequate than previous approaches.", "Method": "The authors use their proposed method to prove several well known theorems analogically.", "Outcome": "Several well known theorems were processed by the authors' analogy-driven proof-plan construction that could not be proven analogically by previous approaches.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 6s "}}
{"id": "5a3da6ef-67b0-41bc-a994-fc5ff455a27b", "displayed_text": "Title: Review spam detection via temporal pattern discovery\n\nAbstract: Online reviews play a crucial role in todayu0027s electronic commerce. It is desirable for a customer to read reviews of products or stores before making the decision of what or from where to buy. Due to the pervasive spam reviews, customers can be misled to buy low-quality products, while decent stores can be defamed by malicious reviews. We observe that, in reality, a great portion (u003e 90% in the data we study) of the reviewers write only one review (singleton review). These reviews are so enormous in number that they can almost determine a storeu0027s rating and impression. However, existing methods did not examine this larger part of the reviews. Are most of these singleton reviews truthful ones? If not, how to detect spam reviews in singleton reviews? We call this problem singleton review spam detection.   To address this problem, we observe that the normal reviewersu0027 arrival pattern is stable and uncorrelated to their rating pattern temporally. In contrast, spam attacks are usually bursty and either positively or negatively correlated to the rating. Thus, we propose to detect such attacks via unusually correlated temporal patterns. We identify and construct multidimensional time series based on aggregate statistics, in order to depict and mine such correlations. In this way, the singleton review spam detection problem is mapped to a abnormally correlated pattern detection problem. We propose a hierarchical algorithm to robustly detect the time windows where such attacks are likely to have happened. The algorithm also pinpoints such windows in different time resolutions to facilitate faster human inspection. Experimental results show that the proposed method is effective in detecting singleton review attacks. We discover that singleton review is a significant source of spam reviews and largely affects the ratings of online stores.", "label_annotations": {"Multi-aspect Summary": {"Context": "Due to the pervasive spam reviews, customers can be misled to buy low-quality products, while decent stores can be defamed by malicious reviews.\r\nExisting methods do not adequately address this large subset of reviews.", "Key idea": "The authors propose addressing the issue of spam in singleton reviews via unusually correlated temporal patterns. And the authors propose a hierarchical algorithm to robustly detect the time windows where spam attacks are likely to have happened. ", "Method": "The authors conduct the experiment to evaluate the ability of detecting singleton review attacks using unusually correlated temporal patterns and hierarchical algorithm.", "Outcome": "Experimental results show that the proposed method is effective in detecting singleton review attacks.  ", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 2s "}}
{"id": "5a3da6ef-67b0-41bc-a994-fc5ff455a27b", "displayed_text": "Title: Review spam detection via temporal pattern discovery\n\nAbstract: Online reviews play a crucial role in todayu0027s electronic commerce. It is desirable for a customer to read reviews of products or stores before making the decision of what or from where to buy. Due to the pervasive spam reviews, customers can be misled to buy low-quality products, while decent stores can be defamed by malicious reviews. We observe that, in reality, a great portion (u003e 90% in the data we study) of the reviewers write only one review (singleton review). These reviews are so enormous in number that they can almost determine a storeu0027s rating and impression. However, existing methods did not examine this larger part of the reviews. Are most of these singleton reviews truthful ones? If not, how to detect spam reviews in singleton reviews? We call this problem singleton review spam detection.   To address this problem, we observe that the normal reviewersu0027 arrival pattern is stable and uncorrelated to their rating pattern temporally. In contrast, spam attacks are usually bursty and either positively or negatively correlated to the rating. Thus, we propose to detect such attacks via unusually correlated temporal patterns. We identify and construct multidimensional time series based on aggregate statistics, in order to depict and mine such correlations. In this way, the singleton review spam detection problem is mapped to a abnormally correlated pattern detection problem. We propose a hierarchical algorithm to robustly detect the time windows where such attacks are likely to have happened. The algorithm also pinpoints such windows in different time resolutions to facilitate faster human inspection. Experimental results show that the proposed method is effective in detecting singleton review attacks. We discover that singleton review is a significant source of spam reviews and largely affects the ratings of online stores.", "label_annotations": {"Multi-aspect Summary": {"Context": "In reality, a great portion (u003e 90% in the data we study) of the reviewers write only one review (singleton review).  However, existing methods did not examine this larger part of the reviews.\r\n\r\nAuthors observe that the normal reviewersu0027 arrival pattern is stable and uncorrelated to their rating pattern temporally. In contrast, spam attacks are usually bursty and either positively or negatively correlated to the rating", "Key idea": "Authors identify and construct multidimensional time series based on aggregate statistics, in order to depict and mine such correlations. In this way, the singleton review spam detection problem is mapped to a abnormally correlated pattern detection problem. Authors propose a hierarchical algorithm to robustly detect the time windows where such attacks are likely to have happened.", "Method": "Authors setup benchmark to detect singleton review attacks.", "Outcome": " Experimental results show that the proposed method is effective in detecting singleton review attacks. We discover that singleton review is a significant source of spam reviews and largely affects the ratings of online stores.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 56s "}}
{"id": "5b47ca6b-ff9b-429c-adb1-ad9a171eea0e", "displayed_text": "Title: Domain Adaptation of Maximum Entropy Language Models\n\nAbstract: We investigate a recently proposed Bayesian adaptation method for building style-adapted maximum entropy language models for speech recognition, given a large corpus of written language data and a small corpus of speech transcripts. Experiments show that the method consistently outperforms linear interpolation which is typically used in such cases.", "label_annotations": {"Multi-aspect Summary": {"Context": "A Bayesian adaptation method for building style-adapted maximum entropy language models for speech recognition is recently proposed", "Key idea": "The authors investigate a recently proposed Bayesian adaptation method for building style-adapted maximum entropy language models for speech recognition, given a large corpus of written language data and a small corpus of speech transcripts.", "Method": "N/A", "Outcome": "Experiments show that the method consistently outperforms linear interpolation which is typically used in such cases.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 0s "}}
{"id": "5b47ca6b-ff9b-429c-adb1-ad9a171eea0e", "displayed_text": "Title: Domain Adaptation of Maximum Entropy Language Models\n\nAbstract: We investigate a recently proposed Bayesian adaptation method for building style-adapted maximum entropy language models for speech recognition, given a large corpus of written language data and a small corpus of speech transcripts. Experiments show that the method consistently outperforms linear interpolation which is typically used in such cases.", "label_annotations": {"Multi-aspect Summary": {"Context": "A Bayesian adaptation method is recently proposed.", "Key idea": "The authors investigate a Bayesian adaptation method for building style-adapted maximum entropy language models for speech recognition, given a large corpus of written language data and a small corpus of speech transcripts.", "Method": "N/A", "Outcome": "Experiments show that the method consistently outperforms linear interpolation which is typically used in such cases.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 25s "}}
{"id": "5b9f94f9-d93f-455d-a110-007ad67ada6d", "displayed_text": "Title: Coreference Handling in XMG\n\nAbstract: We claim that existing specification languages for tree based grammars fail to adequately support identifier managment. We then show that XMG (eXtensible Meta-Grammar) provides a sophisticated treatment of identifiers which is effective in supporting a linguist-friendly grammar design.", "label_annotations": {"Multi-aspect Summary": {"Context": "Existing specification languages for tree based grammars fail to adequately support identifier management.", "Key idea": "The authors show that XMG (eXtensible Meta-Grammar) provides a sophisticated treatment of identifiers which is effective in supporting a linguist-friendly grammar design.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 57s "}}
{"id": "5b9f94f9-d93f-455d-a110-007ad67ada6d", "displayed_text": "Title: Coreference Handling in XMG\n\nAbstract: We claim that existing specification languages for tree based grammars fail to adequately support identifier managment. We then show that XMG (eXtensible Meta-Grammar) provides a sophisticated treatment of identifiers which is effective in supporting a linguist-friendly grammar design.", "label_annotations": {"Multi-aspect Summary": {"Context": "Existing specification languages for tree based grammars fail to adequately support identifier managment. ", "Key idea": "The authors show that XMG (eXtensible Meta-Grammar) provides a sophisticated treatment of identifiers which is effective in supporting a linguist-friendly grammar design.", "Method": "N/A", "Outcome": "XMG is effective in supporting a linguist-friendly grammar design.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 51s "}}
{"id": "5c268324-e160-489d-9722-6b59d5e3471b", "displayed_text": "Title: Achieving accessibility with self-interested designers: a strategic knowledge-acquisition approach\n\nAbstract: We introduce a new approach towards a more accessible Web by means of more accessible knowledge acquisition mechanisms. Our strategy is to detect the Web designeru0027s needs for knowledge that can be collected from minorities of Web users, and subsequently to design mechanisms that allow the proper elicitation of such knowledge from Web users. We discuss how this scenario places marginal Web users in a privileged position that appeals for their inclusion. Additionally, we illustrate how this approach might help build a more accessible Web, to the benefit of visually-impaired knowledge contributors.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors introduce a new approach towards a more accessible Web by means of more accessible knowledge acquisition mechanisms. ", "Method": "N/A", "Outcome": "This new approach places marginal Web users in a privileged position that appeals for their inclusion and it might help build a more accessible Web, to the benefit of visually-impaired knowledge contributors.", "Future Impact": "The proposed approach might help build a more accessible Web, to the benefit of visually-impaired knowledge contributors."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 36s "}}
{"id": "5c268324-e160-489d-9722-6b59d5e3471b", "displayed_text": "Title: Achieving accessibility with self-interested designers: a strategic knowledge-acquisition approach\n\nAbstract: We introduce a new approach towards a more accessible Web by means of more accessible knowledge acquisition mechanisms. Our strategy is to detect the Web designeru0027s needs for knowledge that can be collected from minorities of Web users, and subsequently to design mechanisms that allow the proper elicitation of such knowledge from Web users. We discuss how this scenario places marginal Web users in a privileged position that appeals for their inclusion. Additionally, we illustrate how this approach might help build a more accessible Web, to the benefit of visually-impaired knowledge contributors.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors introduce a new approach towards a more accessible Web by means of more accessible knowledge acquisition mechanisms.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "The proposed approach might help build a more accessible Web, to the benefit of visually-impaired knowledge contributors."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 47s "}}
{"id": "5d87de73-77d4-4efd-b8e1-d7561b13f69f", "displayed_text": "Title: Disambiguation of Preposition Sense Using Linguistically Motivated Features\n\nAbstract: In this paper, we present a supervised classification approach for disambiguation of preposition senses. We use the SemEval 2007 Preposition Sense Disambiguation datasets to evaluate our system and compare its results to those of the systems participating in the workshop. We derived linguistically motivated features from both sides of the preposition. Instead of restricting these to a fixed window size, we utilized the phrase structure. Testing with five different classifiers, we can report an increased accuracy that outperforms the best system in the SemEval task.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The author presents a supervised classification approach for disambiguation of preposition senses.", "Method": "The author uses the SemEval 2007 Preposition Sense Disambiguation datasets to evaluate our system and compare its results to those of the systems participating in the workshop. The author also tests the proposed approach's performance on five different classifiers.", "Outcome": "The author derived linguistically motivated features from both sides of the preposition. Testing with five different classifiers, the author reports an increased accuracy that outperforms the best system in the SemEval task.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 49s "}}
{"id": "5d87de73-77d4-4efd-b8e1-d7561b13f69f", "displayed_text": "Title: Disambiguation of Preposition Sense Using Linguistically Motivated Features\n\nAbstract: In this paper, we present a supervised classification approach for disambiguation of preposition senses. We use the SemEval 2007 Preposition Sense Disambiguation datasets to evaluate our system and compare its results to those of the systems participating in the workshop. We derived linguistically motivated features from both sides of the preposition. Instead of restricting these to a fixed window size, we utilized the phrase structure. Testing with five different classifiers, we can report an increased accuracy that outperforms the best system in the SemEval task.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors present a supervised classification approach for disambiguation of preposition senses. ", "Method": "The authors use the SemEval 2007 Preposition Sense Disambiguation datasets to evaluate supervised classification approach and compare its results to those of the systems participating in the workshop.", "Outcome": "Testing with five different classifiers, the authors conclude that this supervised classification approach obtain an increased accuracy that outperforms the best system in the SemEval task.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 6s "}}
{"id": "5dccca98-2b58-47e3-9b8f-3b1888aa3976", "displayed_text": "Title: The zero-delay data warehouse: mobilizing heterogeneous database\n\nAbstract: Now is the time... for the real-time : In spite of this assertion from Gartner Group the heterogeneity of todayu0027s IT environments and the increasing demands from mobile users are major obstacles for the creation of this vision. Yet its technical foundation is available: software architectures based on innovative middleware components that offer a level of abstraction superior to conventional middleware solutions, including distributed transactions and the seamless integration of mobile devices using open standards, crossing the borders between heterogeneous platforms and systems. Space based computing is a new middleware paradigm meeting these demands. As an example we present the real time build-up of data warehouses.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "N/A", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 15s "}}
{"id": "5dccca98-2b58-47e3-9b8f-3b1888aa3976", "displayed_text": "Title: The zero-delay data warehouse: mobilizing heterogeneous database\n\nAbstract: Now is the time... for the real-time : In spite of this assertion from Gartner Group the heterogeneity of todayu0027s IT environments and the increasing demands from mobile users are major obstacles for the creation of this vision. Yet its technical foundation is available: software architectures based on innovative middleware components that offer a level of abstraction superior to conventional middleware solutions, including distributed transactions and the seamless integration of mobile devices using open standards, crossing the borders between heterogeneous platforms and systems. Space based computing is a new middleware paradigm meeting these demands. As an example we present the real time build-up of data warehouses.", "label_annotations": {"Multi-aspect Summary": {"Context": "The heterogeneity of current IT environments and the increasing demands from mobile users are major obstacles to achieve real-time data integration and processing", "Key idea": "The authors a new middleware paradigm, Space based computing, to meet all kinds of demands. ", "Method": "The authors use new middleware paradigm to create real-time data warehouses.  ", "Outcome": " Space based computing is a level of abstraction superior to conventional middleware solutions, including distributed transactions and the seamless integration of mobile devices using open standards, crossing the borders between heterogeneous platforms and systems. ", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 24s "}}
{"id": "5e1f387c-d883-4d1f-8397-e4a533a3387b", "displayed_text": "Title: A Trainable Spectral-Spatial Sparse Coding Model for Hyperspectral Image Restoration.\n\nAbstract: Hyperspectral imaging offers new perspectives for diverse applications, ranging from the monitoring of the environment using airborne or satellite remote sensing, precision farming, food safety, planetary exploration, or astrophysics. Unfortunately, the spectral diversity of information comes at the expense of various sources of degradation, and the lack of accurate ground-truth \"clean\" hyperspectral signals acquired on the spot makes restoration tasks challenging. In particular, training deep neural networks for restoration is difficult, in contrast to traditional RGB imaging problems where deep models tend to shine. In this paper, we advocate instead for a hybrid approach based on sparse coding principles that retain the interpretability of classical techniques encoding domain knowledge with handcrafted image priors, while allowing to train model parameters end-to-end without massive amounts of data. We show on various denoising benchmarks that our method is computationally efficient and significantly outperforms the state of the art.", "label_annotations": {"Multi-aspect Summary": {"Context": "Hyperspectral imaging offers new perspectives for diverse applications, but the spectral diversity of information comes at the expense of various sources of degradation, and the lack of accurate ground-truth \"clean\" hyperspectral signals acquired on the spot makes restoration tasks challenging.", "Key idea": "The authors propose a hybrid approach based on sparse coding principles that retain the interpretability of classical techniques encoding domain knowledge with handcrafted image priors, while allowing to train model parameters end-to-end without massive amounts of data.", "Method": "The authors evaluate their proposed method on various denoising benchmarks.", "Outcome": "The authors show on various denoising benchmarks that the proposed method is computationally efficient and significantly outperforms the state of the art.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 11m 14s "}}
{"id": "5e1f387c-d883-4d1f-8397-e4a533a3387b", "displayed_text": "Title: A Trainable Spectral-Spatial Sparse Coding Model for Hyperspectral Image Restoration.\n\nAbstract: Hyperspectral imaging offers new perspectives for diverse applications, ranging from the monitoring of the environment using airborne or satellite remote sensing, precision farming, food safety, planetary exploration, or astrophysics. Unfortunately, the spectral diversity of information comes at the expense of various sources of degradation, and the lack of accurate ground-truth \"clean\" hyperspectral signals acquired on the spot makes restoration tasks challenging. In particular, training deep neural networks for restoration is difficult, in contrast to traditional RGB imaging problems where deep models tend to shine. In this paper, we advocate instead for a hybrid approach based on sparse coding principles that retain the interpretability of classical techniques encoding domain knowledge with handcrafted image priors, while allowing to train model parameters end-to-end without massive amounts of data. We show on various denoising benchmarks that our method is computationally efficient and significantly outperforms the state of the art.", "label_annotations": {"Multi-aspect Summary": {"Context": "Hyperspectral imaging offers new perspectives for diverse applications. Unfortunately, the spectral diversity of information comes at the expense of various sources of degradation, and the lack of accurate ground-truth \"clean\" hyperspectral signals acquired on the spot makes restoration tasks challenging. In particular, training deep neural networks for restoration is difficult, in contrast to traditional RGB imaging problems.", "Key idea": "Authors advocate instead for a hybrid approach based on sparse coding principles that retain the interpretability of classical techniques encoding domain knowledge with handcrafted image priors, while allowing to train model parameters end-to-end without massive amounts of data.", "Method": "Authors conduct experiments on various denoising benchmarks.", "Outcome": "Authors show that, on various denoising benchmarks the proposed method is computationally efficient and significantly outperforms the state of the art.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 12s "}}
{"id": "68f8d058-1403-4066-b3d3-a8a2836b35e1", "displayed_text": "Title: Quantifying and Reducing Registration Uncertainty of Spatial Vector Labels on Earth Imagery\n\nAbstract: Given raster imagery features and imperfect vector training labels with registration uncertainty, this paper studies a deep learning framework that can quantify and reduce the registration uncertainty of training labels as well as train neural network parameters simultaneously. The problem is important in broad applications such as streamline classification on Earth imagery or tissue segmentation on medical imagery, whereby annotating precise vector labels is expensive and time-consuming. However, the problem is challenging due to the gap between the vector representation of class labels and the raster representation of image features and the need for training neural networks with uncertain label locations. Existing research on uncertain training labels often focuses on uncertainty in label class semantics or characterizes label registration uncertainty at the pixel level (not contiguous vectors). To fill the gap, this paper proposes a novel learning framework that explicitly quantifies vector labels' registration uncertainty. We propose a registration-uncertainty-aware loss function and design an iterative uncertainty reduction algorithm by re-estimating the posterior of true vector label locations distribution based on a Gaussian process. Evaluations on real-world datasets in National Hydrography Dataset refinement show that the proposed approach significantly outperforms several baselines in the registration uncertainty estimations performance and classification performance.", "label_annotations": {"Multi-aspect Summary": {"Context": "Raster imagery features and imperfect vector training labels have registration uncertainty.\r\nThe problem is challenging due to the gap between the vector representation of class labels and the raster representation of image features and the need for training neural networks with uncertain label locations.", "Key idea": "The authors propose a novel learning framework that explicitly quantifies vector labels' registration uncertainty and a registration-uncertainty-aware loss function and design an iterative uncertainty reduction algorithm by re-estimating the posterior of true vector label locations distribution based on a Gaussian process.", "Method": "The authors evaluate novel learning framework on real-world datasets in National Hydrography Dataset refinement ", "Outcome": "Evaluations on real-world datasets in National Hydrography Dataset refinement show that the novel learning framework significantly outperforms several baselines in the registration uncertainty estimations performance and classification performance.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 31s "}}
{"id": "68f8d058-1403-4066-b3d3-a8a2836b35e1", "displayed_text": "Title: Quantifying and Reducing Registration Uncertainty of Spatial Vector Labels on Earth Imagery\n\nAbstract: Given raster imagery features and imperfect vector training labels with registration uncertainty, this paper studies a deep learning framework that can quantify and reduce the registration uncertainty of training labels as well as train neural network parameters simultaneously. The problem is important in broad applications such as streamline classification on Earth imagery or tissue segmentation on medical imagery, whereby annotating precise vector labels is expensive and time-consuming. However, the problem is challenging due to the gap between the vector representation of class labels and the raster representation of image features and the need for training neural networks with uncertain label locations. Existing research on uncertain training labels often focuses on uncertainty in label class semantics or characterizes label registration uncertainty at the pixel level (not contiguous vectors). To fill the gap, this paper proposes a novel learning framework that explicitly quantifies vector labels' registration uncertainty. We propose a registration-uncertainty-aware loss function and design an iterative uncertainty reduction algorithm by re-estimating the posterior of true vector label locations distribution based on a Gaussian process. Evaluations on real-world datasets in National Hydrography Dataset refinement show that the proposed approach significantly outperforms several baselines in the registration uncertainty estimations performance and classification performance.", "label_annotations": {"Multi-aspect Summary": {"Context": "The problem of quantifying and reducing the registration uncertainty of training labels is important but challenging. Existing research on uncertain training labels often focuses on uncertainty in label class semantics or characterizes label registration uncertainty at the pixel level (not contiguous vectors).", "Key idea": "The authors propose a deep learning framework that can quantify and reduce the registration uncertainty of training labels as well as train neural network parameters simultaneously, specifically, a registration-uncertainty-aware loss function and an iterative uncertainty reduction algorithm by re-estimating the posterior of true vector label locations distribution based on a Gaussian process.", "Method": "The authors evaluate the proposed method on real-world datasets in National Hydrography Dataset refinement and compare it with several baselines.", "Outcome": "Evaluations on real-world datasets in National Hydrography Dataset refinement show that the proposed approach significantly outperforms several baselines in the registration uncertainty estimations performance and classification performance.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 43s "}}
{"id": "69aacc53-6730-4db0-b420-9a45b96a642e", "displayed_text": "Title: Planning from first principles for geometric constraint satisfaction\n\nAbstract: An important problem in geometric reasoning is to find the configuration of a collection of geometric bodies so as to satisfy a set of given constraints. Recently, it has been suggested that this problem can be solved efficiently by symbolically reasoning about geometry using a degrees of freedom analysis. The approach employs a set of specialized routines called plan fragments that specify how to change the configuration of a set of bodies to satisfy a new constraint while preserving existing constraints. In this paper we show how these plan fragments can be automatically synthesized using first principles about geometric bodies, actions, and topology.", "label_annotations": {"Multi-aspect Summary": {"Context": "An important problem in geometric reasoning is to find the configuration of a collection of geometric bodies so as to satisfy a set of given constraints. ", "Key idea": "The authors introduce an approach solving geometric reasoning problem efficiently by symbolically reasoning about geometry using a degrees of freedom analysis. ", "Method": "The authors use new method to show how these plan fragments can be automatically synthesized using first principles about geometric bodies, actions, and topology.", "Outcome": "New method can efficiently solve geometric configuration problems, satisfying a new constraint while preserving existing constraints.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "69aacc53-6730-4db0-b420-9a45b96a642e", "displayed_text": "Title: Planning from first principles for geometric constraint satisfaction\n\nAbstract: An important problem in geometric reasoning is to find the configuration of a collection of geometric bodies so as to satisfy a set of given constraints. Recently, it has been suggested that this problem can be solved efficiently by symbolically reasoning about geometry using a degrees of freedom analysis. The approach employs a set of specialized routines called plan fragments that specify how to change the configuration of a set of bodies to satisfy a new constraint while preserving existing constraints. In this paper we show how these plan fragments can be automatically synthesized using first principles about geometric bodies, actions, and topology.", "label_annotations": {"Multi-aspect Summary": {"Context": "An important problem in geometric reasoning is to find the configuration of a collection of geometric bodies so as to satisfy a set of given constraints. Recently, it has been suggested that this problem can be solved efficiently by symbolically reasoning about geometry using a degrees of freedom analysis.", "Key idea": "Authors use some ways to show that how plan fragments, i.e. a set of specialized routines, can be automatically synthesized using first principles about geometric bodies, actions, and topology.", "Method": "N/A", "Outcome": "Authors show that how plan fragments can be automatically synthesized using first principles about geometric bodies, actions, and topology.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 2s "}}
{"id": "6dc39f88-d613-4ec0-b70d-d5daa6f3643c", "displayed_text": "Title: Demonstration of BitGourmet: Data Analysis via Deterministic Approximation\n\nAbstract: We demonstrate BitGourmet, a novel data analysis system that supports deterministic approximate query processing (DAQ). The system executes aggregation queries and produces deterministic bounds that are guaranteed to contain the true value. The system allows users to set a precision constraint on query results. Given a user-defined target precision, we operate on a carefully selected data subset to satisfy the precision constraint. More precisely, we divide each column vertically, bit-by-bit. Our specialized query processing engine evaluates queries on subsets of these bit vectors. This involves a scenario-specific query optimizer which relies on quality and cost models to decide the optimal bit selection and execution plan. In our demonstration, we show that DAQ realizes an interesting trade-off between result quality and execution time, making data analysis more interactive. We also offer manual control over the query plan, i.e., the bit selection and the execution plan, so that users can gain more insights into our system and DAQ in general.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors propose a novel data analysis system that supports deterministic approximate query processing (DAQ) named BitGourmet.", "Method": "The authors give a demonstration to show BitGourmet performance on deterministic approximate query processing task.", "Outcome": "BitGourmet realizes an interesting trade-off between result quality and execution time, making data analysis more interactive and it also allows users manual control over the query plan for deeper system insights.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 12s "}}
{"id": "6dc39f88-d613-4ec0-b70d-d5daa6f3643c", "displayed_text": "Title: Demonstration of BitGourmet: Data Analysis via Deterministic Approximation\n\nAbstract: We demonstrate BitGourmet, a novel data analysis system that supports deterministic approximate query processing (DAQ). The system executes aggregation queries and produces deterministic bounds that are guaranteed to contain the true value. The system allows users to set a precision constraint on query results. Given a user-defined target precision, we operate on a carefully selected data subset to satisfy the precision constraint. More precisely, we divide each column vertically, bit-by-bit. Our specialized query processing engine evaluates queries on subsets of these bit vectors. This involves a scenario-specific query optimizer which relies on quality and cost models to decide the optimal bit selection and execution plan. In our demonstration, we show that DAQ realizes an interesting trade-off between result quality and execution time, making data analysis more interactive. We also offer manual control over the query plan, i.e., the bit selection and the execution plan, so that users can gain more insights into our system and DAQ in general.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors propose BitGourmet, a novel data analysis system that supports deterministic approximate query processing (DAQ).", "Method": "N/A", "Outcome": "The authors show that DAQ realizes an interesting trade-off between result quality and execution time, making data analysis more interactive in their demonstration.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 34s "}}
{"id": "6de74297-fb80-448f-b7ae-41f8d9701044", "displayed_text": "Title: Can We Consider Central Catadioptric Cameras and Fisheye Cameras within a Unified Imaging Model\n\nAbstract: There are two kinds of omnidirectional cameras often used in computer vision: central catadioptric cameras and fisheye cameras. Previous literatures use different imaging models to describe them separately. A unified imaging model is however presented in this paper. The unified model in this paper can be considered as an extension of the unified imaging model for central catadioptric cameras proposed by Geyer and Daniilidis. We show that our unified model can cover some existing models for fisheye cameras and fit well for many actual fisheye cameras used in previous literatures. Under our unified model, central catadioptric cameras and fisheye cameras can be classified by the model\u2019s characteristic parameter, and a fisheye image can be transformed into a central catadioptric one, vice versa. An important merit of our new unified model is that existing calibration methods for central catadioptric cameras can be directly applied to fisheye cameras. Furthermore, the metric calibration from single fisheye image only using projections of lines becomes possible via our unified model but the existing methods for fisheye cameras in the literatures till now are all non-metric under the same conditions. Experimental results of calibration from some central catadioptric and fisheye images confirm the validity and usefulness of our new unified model.", "label_annotations": {"Multi-aspect Summary": {"Context": "Since there are two kinds of omnidirectional cameras often used in computer vision: central catadioptric cameras and fisheye cameras, previous literatures use different imaging models to describe them separately. ", "Key idea": "The authors propose a unified imaging model, which can cover some existing models for fisheye cameras and fit well for many actual fisheye cameras used in previous literatures.", "Method": "The authors compare unified imaging model with other. existing methods for fisheye cameras in the literatures on the metric calibration from single fisheye image and evaluate unified imaging model on alibration from some central catadioptric and fisheye images.", "Outcome": "While the existing methods for fisheye cameras in the literatures are all non-metric, unified imaging model only use projections of lines on the metric calibration from single fisheye image and experimental results of calibration from some central catadioptric and fisheye images confirm the validity and usefulness of our new unified model.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 22m 39s "}}
{"id": "6de74297-fb80-448f-b7ae-41f8d9701044", "displayed_text": "Title: Can We Consider Central Catadioptric Cameras and Fisheye Cameras within a Unified Imaging Model\n\nAbstract: There are two kinds of omnidirectional cameras often used in computer vision: central catadioptric cameras and fisheye cameras. Previous literatures use different imaging models to describe them separately. A unified imaging model is however presented in this paper. The unified model in this paper can be considered as an extension of the unified imaging model for central catadioptric cameras proposed by Geyer and Daniilidis. We show that our unified model can cover some existing models for fisheye cameras and fit well for many actual fisheye cameras used in previous literatures. Under our unified model, central catadioptric cameras and fisheye cameras can be classified by the model\u2019s characteristic parameter, and a fisheye image can be transformed into a central catadioptric one, vice versa. An important merit of our new unified model is that existing calibration methods for central catadioptric cameras can be directly applied to fisheye cameras. Furthermore, the metric calibration from single fisheye image only using projections of lines becomes possible via our unified model but the existing methods for fisheye cameras in the literatures till now are all non-metric under the same conditions. Experimental results of calibration from some central catadioptric and fisheye images confirm the validity and usefulness of our new unified model.", "label_annotations": {"Multi-aspect Summary": {"Context": "There are two kinds of omnidirectional cameras often used in computer vision: central catadioptric cameras and fisheye cameras. Previous literatures use different imaging models to describe them separately.", "Key idea": "The authors present a unified imaging model, which can be considered as an extension of the unified imaging model for central catadioptric cameras proposed by Geyer and Daniilidis.", "Method": "The authors evaluate their unified model by calibrating some central catadioptric and fisheye images.", "Outcome": "Experimental results of calibration from some central catadioptric and fisheye images confirm the validity and usefulness of the new unified model.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 36s "}}
{"id": "6ecf725b-661e-4897-8169-22d71826d0e8", "displayed_text": "Title: OIE@OIA: an Adaptable and Efficient Open Information Extraction Framework\n\nAbstract: Different Open Information Extraction (OIE) tasks require different types of information, so the OIE field requires strong adaptability of OIE algorithms to meet different task requirements. This paper discusses the adaptability problem in existing OIE systems and designs a new adaptable and efficient OIE system - OIE@OIA as a solution. OIE@OIA follows the methodology of Open Information eXpression (OIX): parsing a sentence to an Open Information Annotation (OIA) Graph and then adapting the OIA graph to different OIE tasks with simple rules. As the core of our OIE@OIA system, we implement an end-to-end OIA generator by annotating a dataset (we make it open available) and designing an efficient learning algorithm for the complex OIA graph. We easily adapt the OIE@OIA system to accomplish three popular OIE tasks. The experimental show that our OIE@OIA achieves new SOTA performances on these tasks, showing the great adaptability of our OIE@OIA system. Furthermore, compared to other end-to-end OIE baselines that need millions of samples for training, our OIE@OIA needs much fewer training samples (12K), showing a significant advantage in terms of efficiency.", "label_annotations": {"Multi-aspect Summary": {"Context": "Different Open Information Extraction (OIE) tasks require different types of information, so the OIE field requires strong adaptability of OIE algorithms to meet different task requirements.", "Key idea": "The authors designs a new adaptable and efficient OIE system - OIE@OIA as a solution to the adaptability problem in existing OIE systems.", "Method": "The authors conduct experiments to evaluate OIE@OIA system on three popular OIE tasks and compare it with other end-to-end OIE baselines.", "Outcome": "On three popular OIE tasks, OIE@OIA achieves new SOTA performances,  showing the great adaptability. And compared to other end-to-end OIE baselines that need millions of samples for training, OIE@OIA needs much fewer training samples (12K), showing a significant advantage in terms of efficiency.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 7m 13s "}}
{"id": "6ecf725b-661e-4897-8169-22d71826d0e8", "displayed_text": "Title: OIE@OIA: an Adaptable and Efficient Open Information Extraction Framework\n\nAbstract: Different Open Information Extraction (OIE) tasks require different types of information, so the OIE field requires strong adaptability of OIE algorithms to meet different task requirements. This paper discusses the adaptability problem in existing OIE systems and designs a new adaptable and efficient OIE system - OIE@OIA as a solution. OIE@OIA follows the methodology of Open Information eXpression (OIX): parsing a sentence to an Open Information Annotation (OIA) Graph and then adapting the OIA graph to different OIE tasks with simple rules. As the core of our OIE@OIA system, we implement an end-to-end OIA generator by annotating a dataset (we make it open available) and designing an efficient learning algorithm for the complex OIA graph. We easily adapt the OIE@OIA system to accomplish three popular OIE tasks. The experimental show that our OIE@OIA achieves new SOTA performances on these tasks, showing the great adaptability of our OIE@OIA system. Furthermore, compared to other end-to-end OIE baselines that need millions of samples for training, our OIE@OIA needs much fewer training samples (12K), showing a significant advantage in terms of efficiency.", "label_annotations": {"Multi-aspect Summary": {"Context": "Different Open Information Extraction (OIE) tasks require different types of information, so the OIE field requires strong adaptability of OIE algorithms to meet different task requirements.", "Key idea": "The authors discuss the adaptability problem in existing OIE systems and design a new adaptable and efficient OIE system - OIE@OIA as a solution.", "Method": "The authors use the proposed OIE@OIA system to accomplish three popular OIE tasks, and compare it to other end-to-end OIE baselines.", "Outcome": "The experimental show that the proposed OIE@OIA achieves new SOTA performances on three popular OIE tasks, showing the great adaptability of OIE@OIA system. Furthermore, compared to other end-to-end OIE baselines that need millions of samples for training, OIE@OIA needs much fewer training samples (12K), showing a significant advantage in terms of efficiency.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 52s "}}
{"id": "73e353a8-e0d6-466f-af93-6fccf38fcb18", "displayed_text": "Title: Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues\n\nAbstract: Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. We propose to discover information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. We then introduce a new approach that learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.", "label_annotations": {"Multi-aspect Summary": {"Context": "Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level.", "Key idea": "The authors propose to introduce a new approach that learns to predict reasoning paths over this semantic graph for video-grounded dialogues.", "Method": "N/A", "Outcome": "The experimental results demonstrate the effectiveness of t he proposed method.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 38s "}}
{"id": "73e353a8-e0d6-466f-af93-6fccf38fcb18", "displayed_text": "Title: Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues\n\nAbstract: Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. We propose to discover information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. We then introduce a new approach that learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.", "label_annotations": {"Multi-aspect Summary": {"Context": "Video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting, but previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modeling the inherent information flows at the turn level.", "Key idea": "The author introduces an approach that a path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question, and than a reasoning model processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer.", "Method": "N/A", "Outcome": "The proposed method is demonstrated as effective, and insights on how models use semantic dependencies in a dialogue context to retrieve visual cues is are provided.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 30s "}}
{"id": "754f792b-fcf3-42f4-bb2c-5c1dcce21d7a", "displayed_text": "Title: Anaphora and logical form: on formal meaning representations for natural language\n\nAbstract: We argue, on general grounds, in favor of formal meaning representations for natural language. We then adopt, as a Forcing function\" for the adequacy of such as representation the problem of identifying the possible antecedents of anachoric expressions. This suggests certain structural properties of a representation which facilitate the identification of a possible antecedents. Given an appropriate representation. language with such properties, it is then possible to deal with a surprisingly rich class of anachora.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors adopt, as a Forcing function\" for the adequacy of such as representation the problem of identifying the possible antecedents of anachoric expressions.", "Method": "N/A", "Outcome": "Certain structural properties of a representation which facilitate the identification of a possible antecedents and language with such properties, it is then possible to deal with a surprisingly rich class of anachora.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 25m 59s "}}
{"id": "754f792b-fcf3-42f4-bb2c-5c1dcce21d7a", "displayed_text": "Title: Anaphora and logical form: on formal meaning representations for natural language\n\nAbstract: We argue, on general grounds, in favor of formal meaning representations for natural language. We then adopt, as a Forcing function\" for the adequacy of such as representation the problem of identifying the possible antecedents of anachoric expressions. This suggests certain structural properties of a representation which facilitate the identification of a possible antecedents. Given an appropriate representation. language with such properties, it is then possible to deal with a surprisingly rich class of anachora.", "label_annotations": {"Multi-aspect Summary": {"Context": "People argue, on general grounds, in favor of formal meaning representations for natural language, and then adopt, as a \"Forcing function\" for the adequacy of such as representation the problem of identifying the possible antecedents of anachoric expressions.", "Key idea": "The authors investigate certain structural properties of a representation which facilitate the identification of a possible antecedents.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 41s "}}
{"id": "7b0671d7-c8f6-4e81-828b-c73958a6a63a", "displayed_text": "Title: Education, entertainment and authenticity: lessons learned from designing an interactive exhibit about medieval music\n\nAbstract: In this paper we describe the design experience gathered from creating an interactive exhibit about medieval music. This system was designed as an educational exhibit that relies on audio as its only feedback channel. We focused our work on three major goals: educational value, entertainment aspects, and historic authenticity. We present insight into the challenges in designing a system with these goals, and how they could be solved.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors describe the design experience gathered from creating an interactive exhibit about medieval music.", "Method": "There are three primary goals, educational value, entertainment aspects, and historic authenticity, that design experience focus on.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "7b0671d7-c8f6-4e81-828b-c73958a6a63a", "displayed_text": "Title: Education, entertainment and authenticity: lessons learned from designing an interactive exhibit about medieval music\n\nAbstract: In this paper we describe the design experience gathered from creating an interactive exhibit about medieval music. This system was designed as an educational exhibit that relies on audio as its only feedback channel. We focused our work on three major goals: educational value, entertainment aspects, and historic authenticity. We present insight into the challenges in designing a system with these goals, and how they could be solved.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors describe the design experience of an interactive exhibit about medieval music, which relies on audio as its only feedback channel and serves an educational purpose.", "Method": "N/A", "Outcome": "The authors reveal the challenges in designing a system with the goals of educational value, entertainment aspects, and historic authenticity, and how they could be solved.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 37s "}}
{"id": "7b21425c-a2b7-4d19-b030-a8350b2a7a80", "displayed_text": "Title: Conditional set generation using Seq2seq models\n\nAbstract:   Conditional set generation learns a mapping from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and dialogue emotion tagging, are instances of set generation. Seq2Seq models, a popular choice for set generation, treat a set as a sequence and do not fully leverage its key properties, namely order-invariance and cardinality. We propose a novel algorithm for effectively sampling informative orders over the combinatorial space of label orders. We jointly model the set cardinality and output by prepending the set size and taking advantage of the autoregressive factorization used by Seq2Seq models. Our method is a model-independent data augmentation approach that endows any Seq2Seq model with the signals of order-invariance and cardinality. Training a Seq2Seq model on this augmented data (without any additional annotations) gets an average relative improvement of 20% on four benchmark datasets across various models: BART, T5, and GPT-3. Code to use SETAUG available at: https://setgen.structgen.com. ", "label_annotations": {"Multi-aspect Summary": {"Context": "Conditional set generation learns a mapping from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and dialogue emotion tagging, are instances of set generation. Seq2Seq models, a popular choice for set generation, treat a set as a sequence and do not fully leverage its key properties, namely order-invariance and cardinality.", "Key idea": "The authors propose a novel algorithm for effectively sampling informative orders over the combinatorial space of label orders, which is a model-independent data augmentation approach that endows any Seq2Seq model with the signals of order-invariance and cardinality.", "Method": "The authors train several Seq2Seq models based their method and evaluate these models on four benchmark datasets.", "Outcome": "Training a Seq2Seq model on this augmented data (without any additional annotations) gets an average relative improvement of 20% on four benchmark datasets across various models: BART, T5, and GPT-3.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 54s "}}
{"id": "7b21425c-a2b7-4d19-b030-a8350b2a7a80", "displayed_text": "Title: Conditional set generation using Seq2seq models\n\nAbstract:   Conditional set generation learns a mapping from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and dialogue emotion tagging, are instances of set generation. Seq2Seq models, a popular choice for set generation, treat a set as a sequence and do not fully leverage its key properties, namely order-invariance and cardinality. We propose a novel algorithm for effectively sampling informative orders over the combinatorial space of label orders. We jointly model the set cardinality and output by prepending the set size and taking advantage of the autoregressive factorization used by Seq2Seq models. Our method is a model-independent data augmentation approach that endows any Seq2Seq model with the signals of order-invariance and cardinality. Training a Seq2Seq model on this augmented data (without any additional annotations) gets an average relative improvement of 20% on four benchmark datasets across various models: BART, T5, and GPT-3. Code to use SETAUG available at: https://setgen.structgen.com. ", "label_annotations": {"Multi-aspect Summary": {"Context": "Seq2Seq models, a popular choice for set generation, treat a set as a sequence and do not fully leverage its key properties, namely order-invariance and cardinality.", "Key idea": "The authors propose a novel algorithm for effectively sampling informative orders over the combinatorial space of label orders. This method is a model-independent data augmentation approach that endows any Seq2Seq model with the signals of order-invariance and cardinality. ", "Method": "The authors train a Seq2Seq model on new augmented data (without any additional annotations) to evaluate performance across various models: BART, T5, and GPT-3.", "Outcome": "Training a Seq2Seq model on augmented data (without any additional annotations) gets an average relative improvement of 20% on four benchmark datasets across various models: BART, T5, and GPT-3. ", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 13s "}}
{"id": "7c065e41-7c2e-430e-a918-6a776037bf50", "displayed_text": "Title: Report on the DB/IR panel at SIGMOD 2005\n\nAbstract: This paper summarizes the salient aspects of the SIGMOD 2005 panel on Databases and Information Retrieval: Rethinking the Great . The goal of the panel was to discuss whether we should rethink data management systems architectures to truly merge Database (DB) and Information Retrieval (IR) technologies. The panel had very high attendance and generated lively discussions.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "N/A", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 18s "}}
{"id": "7c065e41-7c2e-430e-a918-6a776037bf50", "displayed_text": "Title: Report on the DB/IR panel at SIGMOD 2005\n\nAbstract: This paper summarizes the salient aspects of the SIGMOD 2005 panel on Databases and Information Retrieval: Rethinking the Great . The goal of the panel was to discuss whether we should rethink data management systems architectures to truly merge Database (DB) and Information Retrieval (IR) technologies. The panel had very high attendance and generated lively discussions.", "label_annotations": {"Multi-aspect Summary": {"Context": "The SIGMOD 2005 panel on Databases and Information Retrieval: Rethinking the Great aims ad discussing whether we should rethink data management systems architectures to truly merge Database (DB) and Information Retrieval (IR) technologies. The panel had very high attendance and generated lively discussions.", "Key idea": "The authors summarize the salient aspects of the SIGMOD 2005 panel on Databases and Information Retrieval: Rethinking the Great.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 28s "}}
{"id": "7d850699-212d-466b-976f-0afed0653fef", "displayed_text": "Title: Informing determiner and preposition error correction with word clusters\n\nAbstract: We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al., 2011) as part of the HOO 2012 Shared Task. Our system focuses on three error categories: missing determiner, incorrect determiner, and incorrect preposition. Approximately two-thirds of the errors annotated in HOO 2012 training and test data fall into these three categories. To improve our approach, we developed a missing determiner detector and incorporated word clustering (Brown et al., 1992) into the n-gram prediction approach.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors extend their n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al., 2011) as part of the HOO 2012 Shared Task.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 29s "}}
{"id": "7d850699-212d-466b-976f-0afed0653fef", "displayed_text": "Title: Informing determiner and preposition error correction with word clusters\n\nAbstract: We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al., 2011) as part of the HOO 2012 Shared Task. Our system focuses on three error categories: missing determiner, incorrect determiner, and incorrect preposition. Approximately two-thirds of the errors annotated in HOO 2012 training and test data fall into these three categories. To improve our approach, we developed a missing determiner detector and incorporated word clustering (Brown et al., 1992) into the n-gram prediction approach.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors extend their n-gram-based data-driven prediction approach. They develop a missing determiner detector and incorporating word clustering into the n-gram prediction approach.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 12m 25s "}}
{"id": "802a5b78-a022-4d38-bfb3-f28eee4ef89a", "displayed_text": "Title: Transition-based Dependency Parsing Using Two Heterogeneous Gated Recursive Neural Networks\n\nAbstract: Recently, neural network based dependency parsing has attracted much interest, which can effectively alleviate the problems of data sparsity and feature engineering by using the dense features. However, it is still a challenge problem to sufficiently model the complicated syntactic and semantic compositions of the dense features in neural network based methods. In this paper, we propose two heterogeneous gated recursive neural networks: tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN). Then we integrate them to automatically learn the compositions of the dense features for transition-based dependency parsing. Specifically, Tree-GRNN models the feature combinations for the trees in stack, which already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model.", "label_annotations": {"Multi-aspect Summary": {"Context": "It is still a challenge problem to sufficiently model the complicated syntactic and semantic compositions of the dense features in neural network based methods.", "Key idea": "The authors propose two heterogeneous gated recursive neural networks: tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN) and integrate them to automatically learn the compositions of the dense features for transition-based dependency parsing.", "Method": "The authors applied tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN) on two prevalent benchmark datasets (PTB3 and CTB5) to check their effectiveness.", "Outcome": "Experiments conducted on two benchmark datasets, PTB3 and CTB5, demonstrate the effectiveness of the proposed models, tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN).", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 14s "}}
{"id": "802a5b78-a022-4d38-bfb3-f28eee4ef89a", "displayed_text": "Title: Transition-based Dependency Parsing Using Two Heterogeneous Gated Recursive Neural Networks\n\nAbstract: Recently, neural network based dependency parsing has attracted much interest, which can effectively alleviate the problems of data sparsity and feature engineering by using the dense features. However, it is still a challenge problem to sufficiently model the complicated syntactic and semantic compositions of the dense features in neural network based methods. In this paper, we propose two heterogeneous gated recursive neural networks: tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN). Then we integrate them to automatically learn the compositions of the dense features for transition-based dependency parsing. Specifically, Tree-GRNN models the feature combinations for the trees in stack, which already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model.", "label_annotations": {"Multi-aspect Summary": {"Context": "Recently, neural network based dependency parsing has attracted much interest, which can effectively alleviate the problems of data sparsity and feature engineering by using the dense features. However, it is still a challenge problem to sufficiently model the complicated syntactic and semantic compositions of the dense features in neural network based methods.", "Key idea": "The authors propose two heterogeneous gated recursive neural networks: tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN), and the integration of both networks.", "Method": "The authors evaluated their proposed networks on two prevalent benchmark datasets (PTB3 and CTB5).", "Outcome": "Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 59s "}}
{"id": "80698baf-89cb-4a50-9f5c-0c74151b798b", "displayed_text": "Title: Grading the Graders: Motivating Peer Graders in a MOOC\n\nAbstract: In this paper, we detail our efforts at creating and running a controlled study designed to examine how students in a MOOC might be motivated to do a better job during peer grading. This study involves more than one thousand students of a popular MOOC. We ask two specific questions: (1) When a student knows that his or her own peer grading efforts are being examined by peers, does this knowledge alone tend to motivate the student to do a better job when grading assignments? And (2) when a student not only knows that his or her own peer grading efforts are being examined by peers, but he or she is also given a number of other peer grading efforts to evaluate (so the peer graders see how other peer graders evaluate assignments), do both of these together tend to motivate the student to do a better job when grading assignments? We find strong statistical evidence that ``grading the graders'' does in fact tend to increase the quality of peer grading.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The author details their efforts at creating and running a controlled study designed to examine how students in a MOOC might be motivated to do a better job during peer grading on two scenarios: (1) When a student knows that his or her own peer grading efforts are being examined by peers, and (2) when a student not only knows that his or her own peer grading efforts are being examined by peers, but is also given a number of other peer grading efforts to evaluate.", "Method": "The author collects samples from more than one thousand students of a popular MOOC and perform statistical analysis on whether \"grading the graders\" increases the quality of peer grading.", "Outcome": "The author finds strong statistical evidence that \"grading the graders\" does in fact tend to increase the quality of peer grading.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 5h 52m 32s "}}
{"id": "80698baf-89cb-4a50-9f5c-0c74151b798b", "displayed_text": "Title: Grading the Graders: Motivating Peer Graders in a MOOC\n\nAbstract: In this paper, we detail our efforts at creating and running a controlled study designed to examine how students in a MOOC might be motivated to do a better job during peer grading. This study involves more than one thousand students of a popular MOOC. We ask two specific questions: (1) When a student knows that his or her own peer grading efforts are being examined by peers, does this knowledge alone tend to motivate the student to do a better job when grading assignments? And (2) when a student not only knows that his or her own peer grading efforts are being examined by peers, but he or she is also given a number of other peer grading efforts to evaluate (so the peer graders see how other peer graders evaluate assignments), do both of these together tend to motivate the student to do a better job when grading assignments? We find strong statistical evidence that ``grading the graders'' does in fact tend to increase the quality of peer grading.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors create and run a controlled study designed to examine how students in a MOOC might be motivated to do a better job during peer grading.", "Method": "The authors gather more than one thousand students of a popular MOOC, and ask them two specific questions regarding their graders.", "Outcome": "The authors find strong statistical evidence that \"grading the graders\" does in fact tend to increase the quality of peer grading.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 42s "}}
{"id": "816707fd-9214-4435-ac40-b2655e55c9d0", "displayed_text": "Title: CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment\n\nAbstract: Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for k words, but not the words themselves, the task is to align the k definitions with the k contexts. CoDA21 requires a deep understanding of contexts and definitions, including complex inference and world knowledge. We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.(1)", "label_annotations": {"Multi-aspect Summary": {"Context": "Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. ", "Key idea": "The authors introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs.", "Method": "The authors use CoDA21 as benchmark to compare human and PLM's natural language understanding (NLU) capabilities.", "Outcome": "Human has much better performance on natural language understanding(NLU) capabilities than Pretrained language models (PLM).\r\nCoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 11s "}}
{"id": "816707fd-9214-4435-ac40-b2655e55c9d0", "displayed_text": "Title: CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment\n\nAbstract: Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for k words, but not the words themselves, the task is to align the k definitions with the k contexts. CoDA21 requires a deep understanding of contexts and definitions, including complex inference and world knowledge. We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.(1)", "label_annotations": {"Multi-aspect Summary": {"Context": "Pretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks.", "Key idea": "We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for k words, but not the words themselves, the task is to align the k definitions with the k contexts.", "Method": "The author compares the performance of human and pretrained language models on the proposed benchmark.", "Outcome": "There is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 24s "}}
{"id": "8737b031-f77c-4f32-8a68-4be9b0c9ecf8", "displayed_text": "Title: Effective Use of Synthetic Data for Urban Scene Semantic Segmentation\n\nAbstract: Training a deep network to perform semantic segmentation requires large amounts of labeled data. To alleviate the manual effort of annotating real images, researchers have investigated the use of synthetic data, which can be labeled automatically. Unfortunately, a network trained on synthetic data performs relatively poorly on real images. While this can be addressed by domain adaptation, existing methods all require having access to real images during training. In this paper, we introduce a drastically different way to handle synthetic images that does not require seeing any real images at training time. Our approach builds on the observation that foreground and background classes are not affected in the same manner by the domain shift, and thus should be treated differently. In particular, the former should be handled in a detection-based manner to better account for the fact that, while their texture in synthetic images is not photo-realistic, their shape looks natural. Our experiments evidence the effectiveness of our approach on Cityscapes and CamVid with models trained on synthetic data only.", "label_annotations": {"Multi-aspect Summary": {"Context": "A network trained on synthetic data performs relatively poorly on real images. Although this can be addressed by domain adaptation, existing methods all require having access to real images during training.", "Key idea": "The authors introduce a drastically different way to handle synthetic images that does not require seeing any real images at training time.", "Method": "The authors evaluate new method on Cityscapes and CamVid with models trained on synthetic data only.", "Outcome": "Experiments results prove the effectiveness of new method on Cityscapes and CamVid with models trained on synthetic data only.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 23s "}}
{"id": "8737b031-f77c-4f32-8a68-4be9b0c9ecf8", "displayed_text": "Title: Effective Use of Synthetic Data for Urban Scene Semantic Segmentation\n\nAbstract: Training a deep network to perform semantic segmentation requires large amounts of labeled data. To alleviate the manual effort of annotating real images, researchers have investigated the use of synthetic data, which can be labeled automatically. Unfortunately, a network trained on synthetic data performs relatively poorly on real images. While this can be addressed by domain adaptation, existing methods all require having access to real images during training. In this paper, we introduce a drastically different way to handle synthetic images that does not require seeing any real images at training time. Our approach builds on the observation that foreground and background classes are not affected in the same manner by the domain shift, and thus should be treated differently. In particular, the former should be handled in a detection-based manner to better account for the fact that, while their texture in synthetic images is not photo-realistic, their shape looks natural. Our experiments evidence the effectiveness of our approach on Cityscapes and CamVid with models trained on synthetic data only.", "label_annotations": {"Multi-aspect Summary": {"Context": "Training a deep network to perform semantic segmentation requires large amounts of labeled data. The use of synthetic data to alleviate the manual effort of annotating will result in poorer performance, and existing methods to resolve this all require having access to real images during training.", "Key idea": "The authors introduce a way to handle synthetic images that does not require seeing any real images at training time.", "Method": "The authors train their models using the proposed approach on Cityscapes and CamVid, using synthetic data only.", "Outcome": "The experiments evidence the effectiveness of the proposed approach on Cityscapes and CamVid with models trained on synthetic data only.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 9m 23s "}}
{"id": "8ae36735-e4ac-48da-bd2b-5538a6a00a73", "displayed_text": "Title: Overcoming Relational Learning Biases to Accurately Predict Preferences in Large Scale Networks\n\nAbstract: Many individuals on social networking sites provide traits about themselves, such as interests or demographics. Social networking sites can use this information to provide better content to match their usersu0027 interests, such as recommending scheduled events or various relevant products. These tasks require accurate probability estimates to determine the correct answer to return. Relational machine learning (RML) is an excellent framework for these problems as it jointly models the user labels given their attributes and the relational structure. Further, semi-supervised learning methods could enable RML methods to exploit the large amount of unlabeled data in networks.   However, existing RML approaches have limitations that prevent their application in large scale domains. First, semi-supervised methods for RML do not fully utilize all the unlabeled instances in the network. Second, the collective inference procedures necessary to jointly infer the missing labels are generally viewed as too expensive to apply in large scale domains. In this work, we address each of these limitations. We analyze the effect of full semi-supervised RML and find that collective inference methods can introduce considerable bias into predictions. We correct this by implementing a maximum entropy constraint on the inference step, forcing the predictions to have the same distribution as the observed labels. Next, we outline a massively scalable variational inference algorithm for large scale relational network domains. We extend this inference algorithm to incorporate the maximum entropy constraint, proving that it only requires a constant amount of overhead while remaining massively parallel. We demonstrate our methodu0027s improvement over a variety of baselines on seven real world datasets, including large scale networks with over five million edges.", "label_annotations": {"Multi-aspect Summary": {"Context": "Relational machine learning (RML) is an excellent framework for problems that require accurate probability estimates to determine the correct answer as it jointly models the user labels given their attributes and the relational structure. However, existing RML approaches have limitations that prevent their application in large scale domains.", "Key idea": "The authors propose a method, implementing a maximum entropy constraint on the inference step, can solve the limitations of Relational machine learning (RML).", "Method": "The authors conduct experiments over a variety of baselines on seven real world datasets to evaluate new method.", "Outcome": "Experiments prove new method's improvement over a variety of baselines on seven real world datasets, including large scale networks with over five million edges.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 28s "}}
{"id": "8ae36735-e4ac-48da-bd2b-5538a6a00a73", "displayed_text": "Title: Overcoming Relational Learning Biases to Accurately Predict Preferences in Large Scale Networks\n\nAbstract: Many individuals on social networking sites provide traits about themselves, such as interests or demographics. Social networking sites can use this information to provide better content to match their usersu0027 interests, such as recommending scheduled events or various relevant products. These tasks require accurate probability estimates to determine the correct answer to return. Relational machine learning (RML) is an excellent framework for these problems as it jointly models the user labels given their attributes and the relational structure. Further, semi-supervised learning methods could enable RML methods to exploit the large amount of unlabeled data in networks.   However, existing RML approaches have limitations that prevent their application in large scale domains. First, semi-supervised methods for RML do not fully utilize all the unlabeled instances in the network. Second, the collective inference procedures necessary to jointly infer the missing labels are generally viewed as too expensive to apply in large scale domains. In this work, we address each of these limitations. We analyze the effect of full semi-supervised RML and find that collective inference methods can introduce considerable bias into predictions. We correct this by implementing a maximum entropy constraint on the inference step, forcing the predictions to have the same distribution as the observed labels. Next, we outline a massively scalable variational inference algorithm for large scale relational network domains. We extend this inference algorithm to incorporate the maximum entropy constraint, proving that it only requires a constant amount of overhead while remaining massively parallel. We demonstrate our methodu0027s improvement over a variety of baselines on seven real world datasets, including large scale networks with over five million edges.", "label_annotations": {"Multi-aspect Summary": {"Context": "Relational machine learning (RML), combined with semi-supervised learning methods, is an excellent framework to provide better contents to uses based on the traits they posted on social networking sites. However, existing RML approaches have several limitations that prevent their application in large scale domains.", "Key idea": "The authors find the limitation of the collective inference methods of full semi-supervised RML and implement a maximum entropy constraint on the inference step to correct this.", "Method": "The authors evaluate their proposed method on seven real world datasets, including large scale networks with over five million edges, and compare it to a variety of baselines.", "Outcome": "The authors demonstrate their method's improvement over a variety of baselines on seven real world datasets, including large scale networks with over five million edges.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 9m 53s "}}
{"id": "8aedb046-2f51-4229-bc19-ea6db98355cb", "displayed_text": "Title: AggregateRank: bringing order to web sites\n\nAbstract: Since the website is one of the most important organizational structures of the Web, how to effectively rank websites has been essential to many Web applications, such as Web search and crawling. In order to get the ranks of websites, researchers used to describe the inter-connectivity among websites with a so-called HostGraph in which the nodes denote websites and the edges denote linkages between websites (if and only if there are hyperlinks from the pages in one website to the pages in the other, there will be an edge between these two websites), and then adopted the random walk model in the HostGraph. However, as pointed in this paper, the random walk over such a HostGraph is not reasonable because it is not in accordance with the browsing behavior of web surfers. Therefore, the derivate rank cannot represent the true probability of visiting the corresponding website.In this work, we mathematically proved that the probability of visiting a website by the random web surfer should be equal to the sum of the PageRank values of the pages inside that website. Nevertheless, since the number of web pages is much larger than that of websites, it is not feasible to base the calculation of the ranks of websites on the calculation of PageRank. To tackle this problem, we proposed a novel method named AggregateRank rooted in the theory of stochastic complement, which cannot only approximate the sum of PageRank accurately, but also have a lower computational complexity than PageRank. Both theoretical analysis and experimental evaluation show that AggregateRank is a better method for ranking websites than previous methods.", "label_annotations": {"Multi-aspect Summary": {"Context": " In order to get the ranks of websites, researchers used to describe the inter-connectivity among websites with a so-called HostGraph in which the nodes denote websites and the edges denote linkages between websites, and then adopted the random walk model in the HostGraph. However, the random walk over such a HostGraph is not reasonable because it is not in accordance with the browsing behavior of web surfers. Therefore, the derivate rank cannot represent the true probability of visiting the corresponding website.", "Key idea": "Authors mathematically prove that the probability of visiting a website by the random web surfer should be equal to the sum of the PageRank values of the pages inside that website. Authors also propose a novel method named AggregateRank, which can not only approximate the sum of PageRank accurately, but also have a lower computational complexity than PageRank. ", "Method": "Authors discuss the propose methods in theory and experiments.", "Outcome": "Both theoretical analysis and experimental evaluation show that AggregateRank is a better method for ranking websites than previous methods.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 22s "}}
{"id": "8aedb046-2f51-4229-bc19-ea6db98355cb", "displayed_text": "Title: AggregateRank: bringing order to web sites\n\nAbstract: Since the website is one of the most important organizational structures of the Web, how to effectively rank websites has been essential to many Web applications, such as Web search and crawling. In order to get the ranks of websites, researchers used to describe the inter-connectivity among websites with a so-called HostGraph in which the nodes denote websites and the edges denote linkages between websites (if and only if there are hyperlinks from the pages in one website to the pages in the other, there will be an edge between these two websites), and then adopted the random walk model in the HostGraph. However, as pointed in this paper, the random walk over such a HostGraph is not reasonable because it is not in accordance with the browsing behavior of web surfers. Therefore, the derivate rank cannot represent the true probability of visiting the corresponding website.In this work, we mathematically proved that the probability of visiting a website by the random web surfer should be equal to the sum of the PageRank values of the pages inside that website. Nevertheless, since the number of web pages is much larger than that of websites, it is not feasible to base the calculation of the ranks of websites on the calculation of PageRank. To tackle this problem, we proposed a novel method named AggregateRank rooted in the theory of stochastic complement, which cannot only approximate the sum of PageRank accurately, but also have a lower computational complexity than PageRank. Both theoretical analysis and experimental evaluation show that AggregateRank is a better method for ranking websites than previous methods.", "label_annotations": {"Multi-aspect Summary": {"Context": "How to effectively rank websites is essential to many Web applications, but the previous research, the random walk over HostGraph is not reasonable because it is not in accordance with the browsing behavior of web surfers. Therefore, the derivate rank cannot represent the true probability of visiting the corresponding website.", "Key idea": "The authors mathematically prove that the probability of visiting a website by the random web surfer should be equal to the sum of the PageRank values of the pages inside that website. They further propose a novel method named AggregateRank rooted in the theory of stochastic complement, which cannot only approximate the sum of PageRank accurately, but also have a lower computational complexity than PageRank.", "Method": "N/A", "Outcome": "Both theoretical analysis and experimental evaluation show that the proposed AggregateRank is a better method for ranking websites than previous methods.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "927df1bd-273a-4088-8c56-2e79cac37072", "displayed_text": "Title: Scalable k-means++\n\nAbstract: Over half a century old and showing no signs of aging, k-means remains one of the most popular data processing algorithms. As is well-known, a proper initialization of k-means is crucial for obtaining a good final solution. The recently proposed k-means++ initialization algorithm achieves this, obtaining an initial set of centers that is provably close to the optimum solution. A major downside of the k-means++ is its inherent sequential nature, which limits its applicability to massive data: one must make k passes over the data to find a good initial set of centers. In this work we show how to drastically reduce the number of passes needed to obtain, in parallel, a good initialization. This is unlike prevailing efforts on parallelizing k-means that have mostly focused on the post-initialization phases of k-means. We prove that our proposed initialization algorithm k-means|| obtains a nearly optimal solution after a logarithmic number of passes, and then show that in practice a constant number of passes suffices. Experimental evaluation on real-world large-scale data demonstrates that k-means|| outperforms k-means++ in both sequential and parallel settings.", "label_annotations": {"Multi-aspect Summary": {"Context": "k-means remains one of the most popular data processing algorithms.  The recently proposed k-means++ initialization algorithm achieves a proper initialization, obtaining an initial set of centers that is provably close to the optimum solution. A major downside of the k-means++ is its inherent sequential nature, which limits its applicability to massive data: one must make k passes over the data to find a good initial set of centers.", "Key idea": "Authors show how to drastically reduce the number of passes needed to obtain, in parallel, a good initialization. \r\nAuthors prove that our proposed initialization algorithm k-means|| obtains a nearly optimal solution after a logarithmic number of passes, and then show that in practice a constant number of passes suffices.", "Method": "Authors evaluate  k-means|| and k-means++ (as a baseline) on real-world large-scale data.", "Outcome": "Experimental evaluation on real-world large-scale data demonstrates that k-means|| outperforms k-means++ in both sequential and parallel settings.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 3s "}}
{"id": "927df1bd-273a-4088-8c56-2e79cac37072", "displayed_text": "Title: Scalable k-means++\n\nAbstract: Over half a century old and showing no signs of aging, k-means remains one of the most popular data processing algorithms. As is well-known, a proper initialization of k-means is crucial for obtaining a good final solution. The recently proposed k-means++ initialization algorithm achieves this, obtaining an initial set of centers that is provably close to the optimum solution. A major downside of the k-means++ is its inherent sequential nature, which limits its applicability to massive data: one must make k passes over the data to find a good initial set of centers. In this work we show how to drastically reduce the number of passes needed to obtain, in parallel, a good initialization. This is unlike prevailing efforts on parallelizing k-means that have mostly focused on the post-initialization phases of k-means. We prove that our proposed initialization algorithm k-means|| obtains a nearly optimal solution after a logarithmic number of passes, and then show that in practice a constant number of passes suffices. Experimental evaluation on real-world large-scale data demonstrates that k-means|| outperforms k-means++ in both sequential and parallel settings.", "label_annotations": {"Multi-aspect Summary": {"Context": "K-means remains one of the most popular data processing algorithms. K-means++ initialization algorithm achieves this, obtaining an initial set of centers that is provably close to the optimum solution.  A major downside of the k-means++ is its inherent sequential nature, which limits its applicability to massive data.", "Key idea": "This paper introduces an initialization algorithm k-means||, designed to reduce the number of sequential passes required by k-means++ while maintaining nearly optimal initialization quality. ", "Method": "The authors try to use experiment to prove that k-means|| algorithm obtains a nearly optimal solution after a logarithmic number of passes, and a constant number of passes suffices in practice.", "Outcome": "Experimental evaluation on real-world large-scale data demonstrates that k-means|| outperforms k-means++ in both sequential and parallel settings.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 3s "}}
{"id": "9292bc08-a70f-4adb-a6d7-920728893a39", "displayed_text": "Title: Propositional argumentation and causal reasoning\n\nAbstract: The paper introduces a number of propositional argumentation systems obtained by gradually extending the underlying language and associated monotonic logics. An assumption-based argumentation framework [Bondarenko et al., 1997] will constitute a special case of this construction. In addition, a stronger argumentation system in a full classical language will be shown to be equivalent to a system of causal reasoning [Giunchiglia et al., 2004]. The implications of this correspondence for the respective nonmonotonic theories of argumentation and causal reasoning are discussed.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "Authors introduces a number of propositional argumentation systems obtained by gradually extending the underlying language and associated monotonic logics. An assumption-based argumentation framework [Bondarenko et al., 1997] will constitute a special case of authors' construction. In addition, a stronger argumentation system in a full classical language will be shown to be equivalent to a system of causal reasoning", "Method": "Author make theoretical framework to introduce propositional argumentation systems obtained by gradually extending the underlying language and associated monotonic logics.", "Outcome": "The implications of this correspondence for the respective nonmonotonic theories of argumentation and causal reasoning are discussed.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 9m 43s "}}
{"id": "9292bc08-a70f-4adb-a6d7-920728893a39", "displayed_text": "Title: Propositional argumentation and causal reasoning\n\nAbstract: The paper introduces a number of propositional argumentation systems obtained by gradually extending the underlying language and associated monotonic logics. An assumption-based argumentation framework [Bondarenko et al., 1997] will constitute a special case of this construction. In addition, a stronger argumentation system in a full classical language will be shown to be equivalent to a system of causal reasoning [Giunchiglia et al., 2004]. The implications of this correspondence for the respective nonmonotonic theories of argumentation and causal reasoning are discussed.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors introduce a number of propositional argumentation systems obtained by gradually extending the underlying language and associated monotonic logics.", "Method": "The authors show the equivalence of a stronger argumentation system in a full classical language and a system of causal reasoning, and discuss the implications of this correspondence.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 51s "}}
{"id": "93bf12a1-2174-43e3-9cd4-c2b8aeed2f93", "displayed_text": "Title: One-sided unsupervised domain mapping\n\nAbstract: In unsupervised domain mapping, the learner is given two unmatched datasets A and B. The goal is to learn a mapping GAB that translates a sample in A to the analog sample in B. Recent approaches have shown that when learning simultaneously both GAB and the inverse mapping GBA, convincing mappings are obtained. In this work, we present a method of learning GAB without learning GBA. This is done by learning a mapping that maintains the distance between a pair of samples. Moreover, good mappings are obtained, even by maintaining the distance between different parts of the same sample before and after mapping. We present experimental results that the new method not only allows for one sided mapping learning, but also leads to preferable numerical results over the existing circularity-based constraint. Our entire code is made publicly available at https://github.com/sagiebenaim/DistanceGAN.", "label_annotations": {"Multi-aspect Summary": {"Context": "Recent approaches in unsupervised domain mapping has shown that convincing mappings are obtained when learning simultaneously both GAB and the inverse mapping GBA, where learning a mapping GAB means translating a sample in dataset A to the analog sample in dataset B, vise versa.", "Key idea": "The author presents a method of learning GAB without learning GBA by learning a mapping that maintains the distance between a pair of samples.", "Method": "N/A", "Outcome": "Good mappings are obtained, even by maintaining the distance between different parts of the same sample before and after mapping. The proposed method allows for one sided mapping learning and leads to preferable numerical results over the existing circularity-based constraint.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 37s "}}
{"id": "93bf12a1-2174-43e3-9cd4-c2b8aeed2f93", "displayed_text": "Title: One-sided unsupervised domain mapping\n\nAbstract: In unsupervised domain mapping, the learner is given two unmatched datasets A and B. The goal is to learn a mapping GAB that translates a sample in A to the analog sample in B. Recent approaches have shown that when learning simultaneously both GAB and the inverse mapping GBA, convincing mappings are obtained. In this work, we present a method of learning GAB without learning GBA. This is done by learning a mapping that maintains the distance between a pair of samples. Moreover, good mappings are obtained, even by maintaining the distance between different parts of the same sample before and after mapping. We present experimental results that the new method not only allows for one sided mapping learning, but also leads to preferable numerical results over the existing circularity-based constraint. Our entire code is made publicly available at https://github.com/sagiebenaim/DistanceGAN.", "label_annotations": {"Multi-aspect Summary": {"Context": "The goal of unsupervised domain mapping is to learn a mapping GAB that translates a sample in A to the analog sample in B, where A and B are two unmatched datasets. Recent approaches have shown that when learning simultaneously both GAB and the inverse mapping GBA, convincing mappings are obtained.", "Key idea": "The authors present a method of learning the mapping GAB without learning the inverse mapping GBA in unsupervised domain mapping.", "Method": "N/A", "Outcome": "The authors present experimental results that the new method not only allows for one sided mapping learning, but also leads to preferable numerical results over the existing circularity-based constraint.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 38m 12s "}}
{"id": "9748b061-590a-4602-9015-a68e879ffced", "displayed_text": "Title: xPrint: A Modularized Liquid Printer for Smart Materials Deposition\n\nAbstract: To meet the increasing requirements of HCI researchers who are looking into using liquid-based materials (e.g., hydrogels) to create novel interfaces, we present a design strategy for HCI researchers to build and customize a liquid-based smart material printing platform with off-the-shelf or easy-to-machine parts. For the hardware, we suggest a magnetic assembly-based modular design. These modularized parts can be easily and precisely reconfigured with off-the-shelf or easy-to-machine parts that can meet different processing requirements such as mechanical mixing, chemical reaction, light activation, and solution vaporization. In addition, xPrint supports an open-source, highly customizable software design and simulation platform, which is applicable for simulating and facilitating smart material constructions. Furthermore, compared to inkjet or pneumatic syringe-based printing systems, xPrint has a large range of printable materials from synthesized polymers to natural micro-organism-living cells with a printing resolution from 10\u03bcm up to 5mm (droplet size). In this paper, we will introduce the system design in detail and three use cases to demonstrate the material variability and the customizability for users with different demands (e.g., designers, scientific researchers, or artists).", "label_annotations": {"Multi-aspect Summary": {"Context": "HCI researchers who are looking into using liquid-based materials (e.g., hydrogels) to create novel interfaces have the increasing requirements.", "Key idea": "The paper introduces a design strategy named xPrint, that allows HCI researchers to build and customize a smart material printing platform using off-the-shelf or easily machinable parts. \r\n", "Method": "These modularized parts of xPrint can be easily and precisely reconfigured with off-the-shelf or easy-to-machine parts that can meet different processing requirements such as mechanical mixing, chemical reaction, light activation, and solution vaporization.  ", "Outcome": "xPrint supports an open-source, highly customizable software design and simulation platform, which is applicable for simulating and facilitating smart material constructions.  And xPrint has a large range of printable materials from synthesized polymers to natural micro-organism-living cells with a printing resolution from 10\u03bcm up to 5mm (droplet size).", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 2s "}}
{"id": "9748b061-590a-4602-9015-a68e879ffced", "displayed_text": "Title: xPrint: A Modularized Liquid Printer for Smart Materials Deposition\n\nAbstract: To meet the increasing requirements of HCI researchers who are looking into using liquid-based materials (e.g., hydrogels) to create novel interfaces, we present a design strategy for HCI researchers to build and customize a liquid-based smart material printing platform with off-the-shelf or easy-to-machine parts. For the hardware, we suggest a magnetic assembly-based modular design. These modularized parts can be easily and precisely reconfigured with off-the-shelf or easy-to-machine parts that can meet different processing requirements such as mechanical mixing, chemical reaction, light activation, and solution vaporization. In addition, xPrint supports an open-source, highly customizable software design and simulation platform, which is applicable for simulating and facilitating smart material constructions. Furthermore, compared to inkjet or pneumatic syringe-based printing systems, xPrint has a large range of printable materials from synthesized polymers to natural micro-organism-living cells with a printing resolution from 10\u03bcm up to 5mm (droplet size). In this paper, we will introduce the system design in detail and three use cases to demonstrate the material variability and the customizability for users with different demands (e.g., designers, scientific researchers, or artists).", "label_annotations": {"Multi-aspect Summary": {"Context": "There have been increasing requirements of HCI researchers who are looking into using liquid-based materials (e.g., hydrogels) to create novel interfaces.", "Key idea": "The author presents a design for HCI researchers to build and customize a liquid-based smart material printing platform. The design consists a hardware of a magnetic assembly-based modular design, modularized to be easily and precisely reconfigured with off-the-shelf or easy-to-machine parts to meet different processing requirements, and it also supports open-source, highly customizable software design and simulation platform.", "Method": "The author compares the design with inkjet or pneumatic syringe-based printing systems on its range of printable material, and applies the design on three use cases to demonstrate the material variability and the customizability for users with different demands.", "Outcome": "Compared to inkjet or pneumatic syringe-based printing systems, the proposed design has a large range of printable materials from synthesized polymers to natural micro-organism-living cells with a printing resolution from 10\u03bcm up to 5mm (droplet size). The design is also demonstrated for material variability and the customizability for users with different demands.", "Future Impact": "The design strategy is beneficial for HCI researchers to build and customize a liquid-based smart material printing platform with off-the-shelf or easy-to-machine parts."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 8m 59s "}}
{"id": "977698a6-56ef-4e57-94c5-b3a68a451a80", "displayed_text": "Title: A Question Answering Approach for Emotion Cause Extraction\n\nAbstract: Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01% in F-measure.", "label_annotations": {"Multi-aspect Summary": {"Context": "Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text, which is a much more difficult task compared to emotion classification. Recent research has advances in using deep memory networks for question answering (QA).", "Key idea": "The authors propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, they also propose a new mechanism to store relevant context in different memory slots to model context information.", "Method": "The authors evaluate their proposed approach on a recently released emotion cause dataset.", "Outcome": "The proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that the proposed method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01% in F-measure.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 11s "}}
{"id": "977698a6-56ef-4e57-94c5-b3a68a451a80", "displayed_text": "Title: A Question Answering Approach for Emotion Cause Extraction\n\nAbstract: Emotion cause extraction aims to identify the reasons behind a certain emotion expressed in text. It is a much more difficult task compared to emotion classification. Inspired by recent advances in using deep memory networks for question answering (QA), we propose a new approach which considers emotion cause identification as a reading comprehension task in QA. Inspired by convolutional neural networks, we propose a new mechanism to store relevant context in different memory slots to model context information. Our proposed approach can extract both word level sequence features and lexical features. Performance evaluation shows that our method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01% in F-measure.", "label_annotations": {"Multi-aspect Summary": {"Context": "Emotion cause extraction is more challenging than emotion classification and aims to identify the reasons behind a certain emotion expressed in text. The recent approach uses deep memory networks for question answering.", "Key idea": "The author proposes a mechanism to emotion cause extraction to store relevant context in different memory slots to model context information. The proposed approach can extract both word level sequence features and lexical features.", "Method": "The author assesses the performance of the proposed approach against a recently released emotion cause database.", "Outcome": "Performance evaluation shows that the author\u2019s method achieves the state-of-the-art performance on a recently released emotion cause dataset, outperforming a number of competitive baselines by at least 3.01% in F-measure.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 26s "}}
{"id": "9821d5f7-72b0-4841-a54f-d2af4a04ea3a", "displayed_text": "Title: Inverse rendering in SUV space with a linear texture model\n\nAbstract: In this paper, we consider the problem of inverse rendering in the case where surface texture can be approximated by a linear basis. Assuming a dichromatic reflectance model, we show that spherical harmonic illumination coefficients and texture parameters can be estimated in a specular invariant colour subspace by solving a system of bilinear equations. We focus on the case of faces, where both shape and texture can be efficiently described by a linear statistical model. In this context, we are able to fit a 3D morphable model to a single colour image, accounting for both non-Lambertian specular reflectance and complex illumination of the same light source colour. We are able to recover statistical texture model parameters with an accuracy comparable to more computationally expensive analysis-by-synthesis approaches. Moreover, our approach requires only the solution of convex optimisation problems.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors a method, assuming a dichromatic reflectance model and solving spherical harmonic illumination coefficients and texture parameters can be estimated in a specular invariant colour subspace by solving a system of bilinear equations. ", "Method": "The authors apply new method on problems focusing on the case of faces.\r\nThe authors compare new method with more computationally expensive analysis-by-synthesis approaches on statistical texture model parameters recovering problem. \r\n", "Outcome": "When focusing on the case of faces, new method is able to fit a 3D morphable model to a single colour image, accounting for both non-Lambertian specular reflectance and complex illumination of the same light source colour.\r\nNew method is able to recover statistical texture model parameters with an accuracy comparable to more computationally expensive analysis-by-synthesis approaches, while only requires the solution of convex optimisation problems.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 8m 57s "}}
{"id": "9821d5f7-72b0-4841-a54f-d2af4a04ea3a", "displayed_text": "Title: Inverse rendering in SUV space with a linear texture model\n\nAbstract: In this paper, we consider the problem of inverse rendering in the case where surface texture can be approximated by a linear basis. Assuming a dichromatic reflectance model, we show that spherical harmonic illumination coefficients and texture parameters can be estimated in a specular invariant colour subspace by solving a system of bilinear equations. We focus on the case of faces, where both shape and texture can be efficiently described by a linear statistical model. In this context, we are able to fit a 3D morphable model to a single colour image, accounting for both non-Lambertian specular reflectance and complex illumination of the same light source colour. We are able to recover statistical texture model parameters with an accuracy comparable to more computationally expensive analysis-by-synthesis approaches. Moreover, our approach requires only the solution of convex optimisation problems.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors show that for a dichromatic reflectance model, spherical harmonic illumination coefficients and texture parameters can be estimated in a specular invariant colour subspace by solving a system of bilinear equations.", "Method": "The authors recover statistical texture model parameters using their proposed method and compare them with the results of more computationally expensive analysis-by-synthesis approaches.", "Outcome": "The authors are able to recover statistical texture model parameters with an accuracy comparable to more computationally expensive analysis-by-synthesis approaches.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 34m 59s "}}
{"id": "9999e1df-f439-4f62-bd03-337e494e9da3", "displayed_text": "Title: A data type encapsulation scheme utilizing base language operators\n\nAbstract: A data type encapsulation scheme in which the \u201cspace\u201d operations are expressed naturally in terms of the base language operators is described. The scheme results from a conceptual separation of operators and procedure calls in the base language and produces a language of considerable expressive power. The scheme has been implemented and several examples are given.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "Authors propose a data type encapsulation scheme in which the \u201cspace\u201d operations are expressed naturally in terms of the base language operators. The scheme results from a conceptual separation of operators and procedure calls in the base language and produces a language of considerable expressive power. ", "Method": "N/A", "Outcome": "The data type encapsulation scheme has been implemented and several examples are given.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 6s "}}
{"id": "9999e1df-f439-4f62-bd03-337e494e9da3", "displayed_text": "Title: A data type encapsulation scheme utilizing base language operators\n\nAbstract: A data type encapsulation scheme in which the \u201cspace\u201d operations are expressed naturally in terms of the base language operators is described. The scheme results from a conceptual separation of operators and procedure calls in the base language and produces a language of considerable expressive power. The scheme has been implemented and several examples are given.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors propose a data type encapsulation scheme in which the \u201cspace\u201d operations are expressed naturally in terms of the base language operators.", "Method": "N/A", "Outcome": "The scheme has been implemented and several examples are given.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 46s "}}
{"id": "99bef0ca-923c-480d-a89e-9a2f27e02157", "displayed_text": "Title: Abstraction for solving large incomplete-information games\n\nAbstract: Most real-world games and many recreational games are games of incomplete information. Over the last dozen years, abstraction has emerged as a key enabler for solving large incomplete-information games. First, the game is abstracted to generate a smaller, abstract game that is strategically similar to the original game. Second, an approximate equilibrium is computed in the abstract game. Third, the strategy from the abstract game is mapped back to the original game.\r\n\r\nIn this paper, I will review key developments in the field. I present reasons for abstracting games, and point out the issue of abstraction pathology. I then review the practical algorithms for information abstraction and action abstraction. I then cover recent theoretical breakthroughs that beget bounds on the quality of the strategy from the abstract game, when measured in the original game. I then discuss how to reverse map the opponentu0027s action into the abstraction if the opponent makes a move that is not in the abstraction. Finally, I discuss other topics of current and future research.", "label_annotations": {"Multi-aspect Summary": {"Context": "Abstraction has emerged as a key enabler for solving large incomplete-information games. ", "Key idea": "The authors review key developments of abstraction, the practical algorithms for information abstraction and action abstraction and  recent theoretical breakthroughs.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 2s "}}
{"id": "99bef0ca-923c-480d-a89e-9a2f27e02157", "displayed_text": "Title: Abstraction for solving large incomplete-information games\n\nAbstract: Most real-world games and many recreational games are games of incomplete information. Over the last dozen years, abstraction has emerged as a key enabler for solving large incomplete-information games. First, the game is abstracted to generate a smaller, abstract game that is strategically similar to the original game. Second, an approximate equilibrium is computed in the abstract game. Third, the strategy from the abstract game is mapped back to the original game.\r\n\r\nIn this paper, I will review key developments in the field. I present reasons for abstracting games, and point out the issue of abstraction pathology. I then review the practical algorithms for information abstraction and action abstraction. I then cover recent theoretical breakthroughs that beget bounds on the quality of the strategy from the abstract game, when measured in the original game. I then discuss how to reverse map the opponentu0027s action into the abstraction if the opponent makes a move that is not in the abstraction. Finally, I discuss other topics of current and future research.", "label_annotations": {"Multi-aspect Summary": {"Context": "Most real-world games and many recreational games are games of incomplete information.\r\n\r\nAbstraction has emerged as a key enabler for solving large incomplete-information games. First, the game is abstracted to generate a smaller, abstract game that is strategically similar to the original game. Second, an approximate equilibrium is computed in the abstract game. Third, the strategy from the abstract game is mapped back to the original game.", "Key idea": "The author review key developments and present reasons for abstracting games, and point out the issue of abstraction pathology. \r\n\r\nThe author  then review the practical algorithms for information abstraction and action abstraction. She then cover recent theoretical breakthroughs that beget bounds on the quality of the strategy from the abstract game, when measured in the original game. \r\n\r\nShe then discuss how to reverse map the opponentu0027s action into the abstraction if the opponent makes a move that is not in the abstraction. Finally, she discuss other topics of current and future research.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 3s "}}
{"id": "9b95b592-1562-4ef9-b0ed-e0655fadc73b", "displayed_text": "Title: Surface Matching with Large Deformations and Arbitrary Topology: A Geodesic Distance Evolution Scheme on a 3-Manifold\n\nAbstract: A general formulation for geodesic distance propagation of surfaces is presented. Starting from a surface lying on a 3-manifold in IR4, we set up a partial differential equation governing the propagation of surfaces at equal geodesic distance (on the 3-manifold) from the given original surface. This propagation scheme generalizes a result of Kimmel et al. [11] and provides a way to compute distance maps on manifolds. Moreover, the propagation equation is generalized to any number of dimensions. Using an eulerian formulation with level-sets, it gives stable numerical algorithms for computing distance maps. This theory is used to present a new method for surface matching which generalizes a curve matching method [5]. Matching paths are obtained as the orbits of the vector field defined as the sum of two distance mapsu0027 gradient values. This surface matching technique applies to the case of large deformation and topological changes.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The author proposes a new method for surface matching which generalizes a curve matching method. It is based on a partial differential equation governing the propagation of surfaces at equal geodesic distance from the given original surface, which uses an eulerian formulation with level-sets to give stable numerical algorithms for computing distance maps.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 48s "}}
{"id": "9b95b592-1562-4ef9-b0ed-e0655fadc73b", "displayed_text": "Title: Surface Matching with Large Deformations and Arbitrary Topology: A Geodesic Distance Evolution Scheme on a 3-Manifold\n\nAbstract: A general formulation for geodesic distance propagation of surfaces is presented. Starting from a surface lying on a 3-manifold in IR4, we set up a partial differential equation governing the propagation of surfaces at equal geodesic distance (on the 3-manifold) from the given original surface. This propagation scheme generalizes a result of Kimmel et al. [11] and provides a way to compute distance maps on manifolds. Moreover, the propagation equation is generalized to any number of dimensions. Using an eulerian formulation with level-sets, it gives stable numerical algorithms for computing distance maps. This theory is used to present a new method for surface matching which generalizes a curve matching method [5]. Matching paths are obtained as the orbits of the vector field defined as the sum of two distance mapsu0027 gradient values. This surface matching technique applies to the case of large deformation and topological changes.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors propose a general formulation for geodesic distance propagation of surfaces.\r\n\r\n", "Method": "The authors evaluate propagation scheme on computing distance maps on manifolds", "Outcome": "This propagation scheme provide a way to compute distance maps on manifolds.\r\nThe propagation equation can be generalized to any number of dimensions and gives stable numerical algorithms for computing distance maps.\r\nThis theory presents a new method for surface matching which generalizes a curve matching method and surface matching technique can be applies to the case of large deformation and topological changes.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 15m 30s "}}
{"id": "9cd7e7e1-8893-4db6-8327-48f098187699", "displayed_text": "Title: Identifying Relevant Messages in a Twitter-based Citizen Channel for Natural Disaster Situations\n\nAbstract: During recent years the online social networks (in particular Twitter) have become an important alternative information channel to traditional media during natural disasters, but the amount and diversity of messages poses the challenge of information overload to end users. The goal of our research is to develop an automatic classifier of tweets to feed a mobile application that reduces the difficulties that citizens face to get relevant information during natural disasters. In this paper, we present in detail the process to build a classifier that filters tweets relevant and non-relevant to an earthquake. By using a dataset from the Chilean earthquake of 2010, we first build and validate a ground truth, and then we contribute by presenting in detail the effect of class imbalance and dimensionality reduction over 5 classifiers. We show how the performance of these models is affected by these variables, providing important considerations at the moment of building these systems.", "label_annotations": {"Multi-aspect Summary": {"Context": "During recent years the online social networks (in particular Twitter) have become an important alternative information channel to traditional media during natural disasters, but the amount and diversity of messages poses the challenge of information overload to end users.", "Key idea": "Authors develop an automatic classifier of tweets to feed a mobile application that reduces the difficulties that citizens face to get relevant information during natural disasters. Authors present in detail the process to build a classifier that filters tweets relevant and non-relevant to an earthquake.", "Method": "Using a dataset from the Chilean earthquake of 2010, authors build and validate a ground truth, and then they contribute by presenting in detail the effect of class imbalance and dimensionality reduction over 5 classifiers.", "Outcome": "Authors show that the performance of classifiers is affected by these variables, providing important considerations at the moment of building these systems.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 13s "}}
{"id": "9cd7e7e1-8893-4db6-8327-48f098187699", "displayed_text": "Title: Identifying Relevant Messages in a Twitter-based Citizen Channel for Natural Disaster Situations\n\nAbstract: During recent years the online social networks (in particular Twitter) have become an important alternative information channel to traditional media during natural disasters, but the amount and diversity of messages poses the challenge of information overload to end users. The goal of our research is to develop an automatic classifier of tweets to feed a mobile application that reduces the difficulties that citizens face to get relevant information during natural disasters. In this paper, we present in detail the process to build a classifier that filters tweets relevant and non-relevant to an earthquake. By using a dataset from the Chilean earthquake of 2010, we first build and validate a ground truth, and then we contribute by presenting in detail the effect of class imbalance and dimensionality reduction over 5 classifiers. We show how the performance of these models is affected by these variables, providing important considerations at the moment of building these systems.", "label_annotations": {"Multi-aspect Summary": {"Context": "During recent years the online social networks (in particular Twitter) have become an important alternative information channel to traditional media during natural disasters, but the amount and diversity of messages poses the challenge of information overload to end users.", "Key idea": "The authors present in detail the process to build a classifier that filters tweets relevant and non-relevant to an earthquake.", "Method": "The authors build their models and validate a ground truth on a dataset from the Chilean earthquake of 2010.", "Outcome": "The authors show how the performance of classifiers that filters tweets relevant and non-relevant to an earthquake is affected by class imbalance and dimensionality reduction.", "Future Impact": "The research can provide important considerations at the moment of building classifiers that filters information about natural disasters."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "9f641e23-3886-4ac6-b65e-28db86ee48be", "displayed_text": "Title: GSP: The Cinderella of Mechanism Design\n\nAbstract: Nearly fifteen years ago, Google unveiled the generalized second price (GSP) auction. By all theoretical accounts including their own [Varian 14], this was the wrong auction --- the Vickrey-Clarke-Groves (VCG) auction would have been the proper choice --- yet GSP has succeeded spectacularly.\r\n\r\nWe give a deep justification for GSPu0027s success: advertisersu0027 preferences map to a model we call value maximization; they do not maximize profit as the standard theory would believe. For value maximizers, GSP is the truthful auction [Aggarwal 09]. Moreover, this implies an axiomatization of GSP --- it is an auction whose prices are truthful for value maximizers --- that can be applied much more broadly than the simple model for which GSP was originally designed. In particular, applying it to arbitrary single-parameter domains recovers the folklore definition of GSP. Through the lens of value maximization, GSP metamorphosizes into a powerful auction, sound in its principles and elegant in its simplicity.", "label_annotations": {"Multi-aspect Summary": {"Context": " Nearly fifteen years ago, Google unveiled the generalized second price (GSP) auction. By all theoretical accounts including their own [Varian 14], this was the wrong auction --- the Vickrey-Clarke-Groves (VCG) auction would have been the proper choice --- yet GSP has succeeded spectacularly.", "Key idea": "Authors justify for GSPu0027s success: advertisersu0027 preferences map to a model we call value maximization; they do not maximize profit as the standard theory would believe. For value maximizers, authors argue that GSP is the truthful auction. Moreover, this implies an axiomatization of GSP that can be applied much more broadly than the simple model for which GSP was originally designed.", "Method": "N/A", "Outcome": "Authors justify GSPu0027s success: advertisers preferences map to a model we call value maximization. For value maximizers, Authors argue that GSP is the truthful auction.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 18s "}}
{"id": "9f641e23-3886-4ac6-b65e-28db86ee48be", "displayed_text": "Title: GSP: The Cinderella of Mechanism Design\n\nAbstract: Nearly fifteen years ago, Google unveiled the generalized second price (GSP) auction. By all theoretical accounts including their own [Varian 14], this was the wrong auction --- the Vickrey-Clarke-Groves (VCG) auction would have been the proper choice --- yet GSP has succeeded spectacularly.\r\n\r\nWe give a deep justification for GSPu0027s success: advertisersu0027 preferences map to a model we call value maximization; they do not maximize profit as the standard theory would believe. For value maximizers, GSP is the truthful auction [Aggarwal 09]. Moreover, this implies an axiomatization of GSP --- it is an auction whose prices are truthful for value maximizers --- that can be applied much more broadly than the simple model for which GSP was originally designed. In particular, applying it to arbitrary single-parameter domains recovers the folklore definition of GSP. Through the lens of value maximization, GSP metamorphosizes into a powerful auction, sound in its principles and elegant in its simplicity.", "label_annotations": {"Multi-aspect Summary": {"Context": "Nearly fifteen years ago, Google unveiled the generalized second price (GSP) auction, which is theoretically wrong. However, GSP succeeded spectacularly despite the fact that the Vickrey-Clarke-Groves (VCG) auction would have been the proper choice.", "Key idea": "The authors justify for GSP's success.", "Method": "The authors make justification based on advertisers' preferences map to a model called value maximization.", "Outcome": "The authors show that through the lens of value maximization, GSP becomes a powerful auction, sound in its principles and elegant in its simplicity.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 16m 46s "}}
{"id": "a10dafe9-6093-47f0-8429-7b62c46566ea", "displayed_text": "Title: Enabling enterprise mashups over unstructured text feeds with InfoSphere MashupHub and SystemT\n\nAbstract: Enterprise mashup scenarios often involve feeds derived from data created primarily for eye consumption, such as email, news, calendars, blogs, and web feeds. These data sources can test the capabilities of current data mashup products, as the attributes needed to perform join, aggregation, and other operations are often buried within unstructured feed text. Information extraction technology is a key enabler in such scenarios, using annotators to convert unstructured text into structured information that can facilitate mashup operations.   Our demo presents the integration of SystemT, an information extraction system from IBM Research, with IBMu0027s InfoSphere MashupHub. We show how to build domain-specific annotators with SystemTu0027s declarative rule language, AQL, and how to use these annotators to combine structured and unstructured information in an enterprise mashup.", "label_annotations": {"Multi-aspect Summary": {"Context": "Enterprise mashup scenarios often involve feeds derived from data created primarily for eye consumption, such as email, news, calendars, blogs, and web feeds, which can test the capabilities of current data mashup products. Information extraction technology is a key enabler in such scenarios, using annotators to convert unstructured text into structured information that can facilitate mashup operations.", "Key idea": "The authors present the integration of SystemT, an information extraction system from IBM Research, with IBM's InfoSphere MashupHub.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 1h 45m 26s "}}
{"id": "a10dafe9-6093-47f0-8429-7b62c46566ea", "displayed_text": "Title: Enabling enterprise mashups over unstructured text feeds with InfoSphere MashupHub and SystemT\n\nAbstract: Enterprise mashup scenarios often involve feeds derived from data created primarily for eye consumption, such as email, news, calendars, blogs, and web feeds. These data sources can test the capabilities of current data mashup products, as the attributes needed to perform join, aggregation, and other operations are often buried within unstructured feed text. Information extraction technology is a key enabler in such scenarios, using annotators to convert unstructured text into structured information that can facilitate mashup operations.   Our demo presents the integration of SystemT, an information extraction system from IBM Research, with IBMu0027s InfoSphere MashupHub. We show how to build domain-specific annotators with SystemTu0027s declarative rule language, AQL, and how to use these annotators to combine structured and unstructured information in an enterprise mashup.", "label_annotations": {"Multi-aspect Summary": {"Context": "Enterprise mashup scenarios often involve feeds derived from data created primarily for eye consumption, such as email, news, calendars, blogs, and web feeds. These data sources can test the capabilities of current data mashup products, as the attributes needed to perform join, aggregation, and other operations are often buried within unstructured feed text. Information extraction technology is a key enabler in such scenarios, using annotators to convert unstructured text into structured information that can facilitate mashup operations.", "Key idea": "Authors present the integration of SystemT, an information extraction system from IBM Research, with IBMu0027s InfoSphere MashupHub.", "Method": "Authors show demos on how to build domain-specific annotators with SystemTu0027s declarative rule language, AQL, and how to use these annotators to combine structured and unstructured information in an enterprise mashup.", "Outcome": "Authors show that the proposed SystemT can be used to build domain-specific annotators with declarative rule language, AQL, and these annotators can combine structured and unstructured information in an enterprise mashup.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 12s "}}
{"id": "a1739057-ef00-4b01-9c26-4ab2b5d5708e", "displayed_text": "Title: A framework to support multiple query optimization for complex mining tasks\n\nAbstract: With an increasing use of data mining tools and techniques, we envision that a Knowledge Discovery and Data Mining System (KDDMS) will have to support and optimize for the following scenarios: 1) Sequence of Queries: A user may analyze one or more datasets by issuing a sequence of related complex mining queries, and 2) Multiple Simultaneous Queries: Several users may be analyzing a set of datasets concurrently, and may issue related complex queries.This paper presents a systematic mechanism to optimize for the above cases, targetting the class of mining queries involving frequent pattern mining on one or multiple datasets. We present a system architecture and propose new algorithms for this purpose. We show the design of a knowledgeable cache which can store the past query results from queries on multiple datasets. We present algorithms which enable the use of the results stored in such a cache to further optimize multiple queries.We have implemented and evaluated our system with both real and synthetic datasets. Our experimental results show that our techniques can achieve a speedup of up to a factor of 9, compared with the systems which do not support caching or optimize for multiple queries.", "label_annotations": {"Multi-aspect Summary": {"Context": "The use of data mining tools and techniques is increasing.", "Key idea": "The authors envision that a Knowledge Discovery and Data Mining System (KDDMS) will have to support and optimize for two scenarios. They further propose a systematic mechanism to optimize for these two cases, and a system architecture along with new algorithms for this purpose.", "Method": "The authors implement and evaluate their system with both real and synthetic datasets.", "Outcome": "The experimental results show that our techniques can achieve a speedup of up to a factor of 9, compared with the systems which do not support caching or optimize for multiple queries.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 5s "}}
{"id": "a1739057-ef00-4b01-9c26-4ab2b5d5708e", "displayed_text": "Title: A framework to support multiple query optimization for complex mining tasks\n\nAbstract: With an increasing use of data mining tools and techniques, we envision that a Knowledge Discovery and Data Mining System (KDDMS) will have to support and optimize for the following scenarios: 1) Sequence of Queries: A user may analyze one or more datasets by issuing a sequence of related complex mining queries, and 2) Multiple Simultaneous Queries: Several users may be analyzing a set of datasets concurrently, and may issue related complex queries.This paper presents a systematic mechanism to optimize for the above cases, targetting the class of mining queries involving frequent pattern mining on one or multiple datasets. We present a system architecture and propose new algorithms for this purpose. We show the design of a knowledgeable cache which can store the past query results from queries on multiple datasets. We present algorithms which enable the use of the results stored in such a cache to further optimize multiple queries.We have implemented and evaluated our system with both real and synthetic datasets. Our experimental results show that our techniques can achieve a speedup of up to a factor of 9, compared with the systems which do not support caching or optimize for multiple queries.", "label_annotations": {"Multi-aspect Summary": {"Context": "With an increasing use of data mining tools and techniques, authors envision that a Knowledge Discovery and Data Mining System (KDDMS) will have to support and optimize for the following scenarios: 1) Sequence of Queries: A user may analyze one or more datasets by issuing a sequence of related complex mining queries, and 2) Multiple Simultaneous Queries: Several users may be analyzing a set of datasets concurrently, and may issue related complex queries.", "Key idea": "Authors presents a systematic mechanism to optimize for the above cases, targetting the class of mining queries involving frequent pattern mining on one or multiple datasets. Authors present a system architecture and propose new algorithms for this purpose. Authors show the design of a knowledgeable cache which can store the past query results from queries on multiple datasets. Authors present algorithms which enable the use of the results stored in such a cache to further optimize multiple queries.", "Method": "Authors evaluate the system with both real and synthetic datasets", "Outcome": "Experimental results show that our techniques can achieve a speedup of up to a factor of 9, compared with the systems which do not support caching or optimize for multiple queries.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 48s "}}
{"id": "a5bc5ab8-70fb-4dff-824b-7606228e44a9", "displayed_text": "Title: Adding math to Web pages with EzMath\n\nAbstract: EzMath provides an easy to learn notation for embedding mathematical expressions in Web pages.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors introduce EzMath, which provides an easy to learn notation for embedding mathematical expressions in Web pages.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 33s "}}
{"id": "a5bc5ab8-70fb-4dff-824b-7606228e44a9", "displayed_text": "Title: Adding math to Web pages with EzMath\n\nAbstract: EzMath provides an easy to learn notation for embedding mathematical expressions in Web pages.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors propose EzMath, which provides an easy to learn notation for embedding mathematical expressions in Web pages.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 32s "}}
{"id": "a722b600-3725-4738-a47b-435aebd63e13", "displayed_text": "Title: A Design Philosophy for Agents in the Smart Home\n\nAbstract: The home is often the most private space in people's lives, and not one in which they expect to be surveilled. However, today's market for smart home devices has quickly evolved to include products that monitor, automate, and present themselves as human. After documenting some of the more unusual emergent problems with contemporary devices, this body of work seeks to develop a design philosophy for intelligent agents in the smart home that can act as an alternative to the ways that these devices are currently built. This is then applied to the design of privacy empowering technologies, representing the first steps from the devices of the present towards a more respectful future.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "The home is often the most private space in people's lives, and not one in which they expect to be surveilled. However, today's market for smart home devices has quickly evolved to include products that monitor, automate, and present themselves as human.", "Key idea": "The authors develop a design philosophy for intelligent agents in the smart home that can act as an alternative to the ways that these devices are currently built.", "Method": "N/A", "Outcome": "The proposed philosophy is applied to the design of privacy empowering technologies.", "Future Impact": "The authors' work represent the first steps from the devices of the present towards a more respectful future"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 8m 27s "}}
{"id": "a722b600-3725-4738-a47b-435aebd63e13", "displayed_text": "Title: A Design Philosophy for Agents in the Smart Home\n\nAbstract: The home is often the most private space in people's lives, and not one in which they expect to be surveilled. However, today's market for smart home devices has quickly evolved to include products that monitor, automate, and present themselves as human. After documenting some of the more unusual emergent problems with contemporary devices, this body of work seeks to develop a design philosophy for intelligent agents in the smart home that can act as an alternative to the ways that these devices are currently built. This is then applied to the design of privacy empowering technologies, representing the first steps from the devices of the present towards a more respectful future.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "Today's market for smart home devices has quickly evolved to include products that monitor, automate, and present themselves as human. ", "Key idea": "Authors develop a design philosophy for intelligent agents in the smart home that can act as an alternative to the ways that these devices are currently built.", "Method": "Authors apply the design philosophy to the design of privacy empowering technologies.", "Outcome": "Authors argue that this work marks the first steps from the devices of the present towards a more respectful future.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 15s "}}
{"id": "ad69e31c-2c7f-4db0-916c-3deccaab37fd", "displayed_text": "Title: A Scalable Machine Learning Approach to Go\n\nAbstract: Go is an ancient board game that poses unique opportunities and challenges for AI and machine learning. Here we develop a machine learning approach to Go, and related board games, focusing primarily on the problem of learning a good evaluation function in a scalable way. Scalability is essential at multiple levels, from the library of local tactical patterns, to the integration of patterns across the board, to the size of the board itself. The system we propose is capable of automatically learning the propensity of local patterns from a library of games. Propensity and other local tactical information are fed into a recursive neural network, derived from a Bayesian network architecture. The network integrates local information across the board and produces local outputs that represent local territory ownership probabilities. The aggregation of these probabilities provides an effective strategic evaluation function that is an estimate of the expected area at the end (or at other stages) of the game. Local area targets for training can be derived from datasets of human games. A system trained using only 9 \u00d7 9 amateur game data performs surprisingly well on a test set derived from 19 \u00d7 19 professional game data. Possible directions for further improvements are briefly discussed.", "label_annotations": {"Multi-aspect Summary": {"Context": "Go is an ancient board game that poses unique opportunities and challenges for AI and machine learning.", "Key idea": "The author proposes a system that is capable of automatically learning the propensity of local patterns from a library of games through a recursive neural network. The network integrates local information across the board and produces local outputs that represent local territory ownership probabilities, which then provides an effective strategic evaluation function on the expected area at the end of the game.", "Method": "The author tests the system by training on 9 \u00d7 9 amateur game data and testing on 19 \u00d7 19 professional game data.", "Outcome": "A system trained using only 9 \u00d7 9 amateur game data performs surprisingly well on a test set derived from 19 \u00d7 19 professional game data.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 24s "}}
{"id": "ad69e31c-2c7f-4db0-916c-3deccaab37fd", "displayed_text": "Title: A Scalable Machine Learning Approach to Go\n\nAbstract: Go is an ancient board game that poses unique opportunities and challenges for AI and machine learning. Here we develop a machine learning approach to Go, and related board games, focusing primarily on the problem of learning a good evaluation function in a scalable way. Scalability is essential at multiple levels, from the library of local tactical patterns, to the integration of patterns across the board, to the size of the board itself. The system we propose is capable of automatically learning the propensity of local patterns from a library of games. Propensity and other local tactical information are fed into a recursive neural network, derived from a Bayesian network architecture. The network integrates local information across the board and produces local outputs that represent local territory ownership probabilities. The aggregation of these probabilities provides an effective strategic evaluation function that is an estimate of the expected area at the end (or at other stages) of the game. Local area targets for training can be derived from datasets of human games. A system trained using only 9 \u00d7 9 amateur game data performs surprisingly well on a test set derived from 19 \u00d7 19 professional game data. Possible directions for further improvements are briefly discussed.", "label_annotations": {"Multi-aspect Summary": {"Context": "Go is an ancient board game that poses unique opportunities and challenges for AI and machine learning.", "Key idea": "The authors develop a machine learning approach to Go, and related board games, focusing primarily on the problem of learning a good evaluation function in a scalable way. They propose a system capable of automatically learning the propensity of local patterns from a library of games.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "The authors discuss possible directions for further improvements"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 16m 28s "}}
{"id": "adfd8058-64b3-4062-953c-034b732e2fa0", "displayed_text": "Title: Video Compression Scheme Using DEMD Based Texture Synthesis\n\nAbstract: In this paper we present a video coding scheme based on texture synthesis through Directional Empirical Mode Decomposition (DEMD). In this scheme P and B-frames of the video sequence are decomposed and parametrically coded with the help of DEMD algorithm, while I-frames are coded with the help of H.264. All P and B frames are decomposed into Intrinsic Mode Function (IMF) image and its residue. Only the first level IMF image for P and B frames are coded. At decoder stage subsequent IMF images are synthesized with the help of correlation search. Wavelet decomposition is performed over residual image and energy level at the HH band is used as a decision criterion for number of decomposition to be performed for optimum synthesis. The experimental result demonstrates the effectiveness of the algorithm in multi-resolution parametric modeling of image data which can be efficiently coded to achieve significant compression with acceptable quality. This scheme also enables to perform scalable coding of IMF parameters to achieve higher compression with perceptual quality.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The author presents a video coding scheme based on texture synthesis through Directional Empirical Mode Decomposition (DEMD). At encoding stage, scheme P and B-frames of the video sequence are decomposed into Intrinsic Mode Function (IMF) image and its residue and parametrically coded on only their first level with the help of DEMD algorithm and I-frames are coded with the help of H.264, while at encoding stage subsequent IMF images are synthesized with the help of correlation search.\r\n\r\nIn this paper we present a video coding scheme based on texture synthesis through Directional Empirical Mode Decomposition (DEMD). In this scheme P and B-frames of the video sequence are decomposed and parametrically coded with the help of DEMD algorithm, while I-frames are coded with the help of H.264. All P and B frames are decomposed into Intrinsic Mode Function (IMF) image and its residue. Only the first level IMF image for P and B frames are coded. At decoder stage subsequent IMF images are synthesized with the help of correlation search. Wavelet decomposition is performed over residual image and energy level at the HH band is used as a decision criterion for number of decomposition to be performed for optimum synthesis.", "Method": "The author tests the effectiveness of the algorithm in multi-resolution parametric modeling of image data. The author also assesses the scalability of coding of IMF parameters.", "Outcome": "The experimental result demonstrates the effectiveness of the algorithm in multi-resolution parametric modeling of image data which can be efficiently coded to achieve significant compression with acceptable quality. This scheme also enables to perform scalable coding of IMF parameters to achieve higher compression with perceptual quality.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 12m 54s "}}
{"id": "adfd8058-64b3-4062-953c-034b732e2fa0", "displayed_text": "Title: Video Compression Scheme Using DEMD Based Texture Synthesis\n\nAbstract: In this paper we present a video coding scheme based on texture synthesis through Directional Empirical Mode Decomposition (DEMD). In this scheme P and B-frames of the video sequence are decomposed and parametrically coded with the help of DEMD algorithm, while I-frames are coded with the help of H.264. All P and B frames are decomposed into Intrinsic Mode Function (IMF) image and its residue. Only the first level IMF image for P and B frames are coded. At decoder stage subsequent IMF images are synthesized with the help of correlation search. Wavelet decomposition is performed over residual image and energy level at the HH band is used as a decision criterion for number of decomposition to be performed for optimum synthesis. The experimental result demonstrates the effectiveness of the algorithm in multi-resolution parametric modeling of image data which can be efficiently coded to achieve significant compression with acceptable quality. This scheme also enables to perform scalable coding of IMF parameters to achieve higher compression with perceptual quality.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors present a video coding scheme based on texture synthesis through Directional Empirical Mode Decomposition (DEMD).", "Method": "N/A", "Outcome": "The experimental result demonstrates the effectiveness of the algorithm in multi-resolution parametric modeling of image data which can be efficiently coded to achieve significant compression with acceptable quality. This scheme also enables to perform scalable coding of IMF parameters to achieve higher compression with perceptual quality.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 7s "}}
{"id": "b0ad60d7-3a3a-42c2-acbc-fba55e708ba0", "displayed_text": "Title: A powerful and general approach to context exploitation in natural language processing\n\nAbstract: In natural language, the meaning of a lexeme often varies due to the specific surrounding context. Computational approaches to natural language processing can benefit from a reliable, long-range-context-dependent representation of the meaning of each lexeme that appears in a given sentence. We have developed a general new technique that produces a context-dependent u0027meaningu0027 representation for a lexeme in a specific surrounding context. The u0027meaningu0027 of a lexeme in a specific context is represented by a list of semantically replaceable elements the members of which are other lexemes from our experimental lexicon. We have performed experiments with a lexicon composed of individual English words and also with a lexicon of individual words and selected phrases. The resulting lists can be used to compare the u0027meaningu0027 of conceptual units (individual words or frequently-occurring phrases) in different contexts and also can serve as features for machine learning approaches to classify semantic roles and relationships.", "label_annotations": {"Multi-aspect Summary": {"Context": " In natural language, the meaning of a lexeme often varies due to the specific surrounding context. Computational approaches to natural language processing can benefit from a reliable, long-range-context-dependent representation of the meaning of each lexeme that appears in a given sentence. ", "Key idea": "Authors develope a general new technique that produces a context-dependent u0027meaningu0027 representation for a lexeme in a specific surrounding context. \r\n The u0027meaningu0027 of a lexeme in a specific context is represented by a list of semantically replaceable elements the members of which are other lexemes from our experimental lexicon.", "Method": "Authors perform experiments with a lexicon composed of individual English words and also with a lexicon of individual words and selected phrases.", "Outcome": "The lists generated by the propose technique can be used to compare the u0027meaningu0027 of conceptual units (individual words or frequently-occurring phrases) in different contexts and also can serve as features for machine learning approaches to classify semantic roles and relationships.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 2s "}}
{"id": "b0ad60d7-3a3a-42c2-acbc-fba55e708ba0", "displayed_text": "Title: A powerful and general approach to context exploitation in natural language processing\n\nAbstract: In natural language, the meaning of a lexeme often varies due to the specific surrounding context. Computational approaches to natural language processing can benefit from a reliable, long-range-context-dependent representation of the meaning of each lexeme that appears in a given sentence. We have developed a general new technique that produces a context-dependent u0027meaningu0027 representation for a lexeme in a specific surrounding context. The u0027meaningu0027 of a lexeme in a specific context is represented by a list of semantically replaceable elements the members of which are other lexemes from our experimental lexicon. We have performed experiments with a lexicon composed of individual English words and also with a lexicon of individual words and selected phrases. The resulting lists can be used to compare the u0027meaningu0027 of conceptual units (individual words or frequently-occurring phrases) in different contexts and also can serve as features for machine learning approaches to classify semantic roles and relationships.", "label_annotations": {"Multi-aspect Summary": {"Context": "Computational approaches to natural language processing can benefit from a reliable, long-range-context-dependent representation of the meaning of each lexeme that appears in a given sentence. ", "Key idea": "The authors developed a general new technique that produces a context-dependent 'meaning' representation for a lexeme in a specific surrounding context. ", "Method": "The authors have performed experiments with a lexicon composed of individual English words and also with a lexicon of individual words and selected phrases to evaluate the effectiveness new technique, that produces a context-dependent 'meaning' representation for a lexeme in a specific surrounding context. ", "Outcome": "The resulting lists generated by new technique can be used to compare the 'meaning' of conceptual units (individual words or frequently-occurring phrases) in different contexts and also can serve as features for machine learning approaches to classify semantic roles and relationships.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 49s "}}
{"id": "b14f2bc1-607f-4d8b-a731-b4afdf30a633", "displayed_text": "Title: MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure\n\nAbstract:   In this paper, we propose a comprehensive benchmark to investigate models' logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence. To this end, we propose a comprehensive logical reasoning explanation form. Based on the multi-hop chain of reasoning, the explanation form includes three main components: (1) The condition of rebuttal that the reasoning node can be challenged; (2) Logical formulae that uncover the internal texture of reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The fine-grained structure conforms to the real logical reasoning scenario, better fitting the human cognitive process but, simultaneously, is more challenging for the current models. We evaluate the current best models' performance on this new explanation form. The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models. ", "label_annotations": {"Multi-aspect Summary": {"Context": "Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence.", "Key idea": "The authors propose a comprehensive benchmark to investigate models' logical reasoning capabilities in complex real-life scenarios, as well as a logical reasoning explanation form.", "Method": "The authors evaluate the current best models' performance on their proposed new explanation form.", "Outcome": "The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 12h 59m 8s "}}
{"id": "b14f2bc1-607f-4d8b-a731-b4afdf30a633", "displayed_text": "Title: MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure\n\nAbstract:   In this paper, we propose a comprehensive benchmark to investigate models' logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence. To this end, we propose a comprehensive logical reasoning explanation form. Based on the multi-hop chain of reasoning, the explanation form includes three main components: (1) The condition of rebuttal that the reasoning node can be challenged; (2) Logical formulae that uncover the internal texture of reasoning nodes; (3) Reasoning strength indicated by degrees of certainty. The fine-grained structure conforms to the real logical reasoning scenario, better fitting the human cognitive process but, simultaneously, is more challenging for the current models. We evaluate the current best models' performance on this new explanation form. The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models. ", "label_annotations": {"Multi-aspect Summary": {"Context": "Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence.", "Key idea": "The authors propose a comprehensive logical reasoning explanation form including rebuttal conditions, logical formulae, and reasoning strength to investigate models' logical reasoning capabilities in complex real-life scenarios. ", "Method": "The authors evaluate the current best models' performance on this new comprehensive logical reasoning explanation form.", "Outcome": "The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "b391a193-83e3-4f11-801f-1842647d626e", "displayed_text": "Title: Uncertain Graph Neural Networks For Facial Action Unit Detection\n\nAbstract: Capturing the dependencies among different facial action units (AU) is extremely important for the AU detection task. Many studies have employed graph-based deep learning methods to exploit the dependencies among AUs. However, the dependencies among AUs in real world data are often noisy and the uncertainty is essential to be taken into consideration. Rather than employing a deterministic mode, we propose an uncertain graph neural network (UGN) to learn the probabilistic mask that simultaneously captures both the individual dependencies among AUs and the uncertainties. Further, we propose an adaptive weighted loss function based on the epistemic uncertainties to adaptively vary the weights of the training samples during the training process to account for unbalanced data distributions among AUs. We also provide an insightful analysis on how the uncertainties are related to the performance of AU detection. Extensive experiments, conducted on two benchmark datasets, i.e., BP4D and DISFA, demonstrate our method achieves the state-of-the-art performance.", "label_annotations": {"Multi-aspect Summary": {"Context": "Capturing the dependencies among different facial action units (AU) is extremely important for the AU detection task, but the dependencies among AUs in real world data are often noisy and uncertain. Many studies have employed graph-based deep learning methods to exploit the dependencies among AUs.", "Key idea": "The authors propose an uncertain graph neural network (UGN) to learn the probabilistic mask that simultaneously captures both the individual dependencies among AUs and the uncertainties. They further propose an adaptive weighted loss function based on the epistemic uncertainties to adaptively vary the weights of the training samples during the training process to account for unbalanced data distributions among AUs.", "Method": "They analyze how the uncertainties are related to the performance of AU detection and conduct extensive experiments on two benchmark datasets, i.e., BP4D and DISFA.", "Outcome": "Extensive experiments, conducted on two benchmark datasets, i.e., BP4D and DISFA, demonstrate our method achieves the state-of-the-art performance.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 12s "}}
{"id": "b391a193-83e3-4f11-801f-1842647d626e", "displayed_text": "Title: Uncertain Graph Neural Networks For Facial Action Unit Detection\n\nAbstract: Capturing the dependencies among different facial action units (AU) is extremely important for the AU detection task. Many studies have employed graph-based deep learning methods to exploit the dependencies among AUs. However, the dependencies among AUs in real world data are often noisy and the uncertainty is essential to be taken into consideration. Rather than employing a deterministic mode, we propose an uncertain graph neural network (UGN) to learn the probabilistic mask that simultaneously captures both the individual dependencies among AUs and the uncertainties. Further, we propose an adaptive weighted loss function based on the epistemic uncertainties to adaptively vary the weights of the training samples during the training process to account for unbalanced data distributions among AUs. We also provide an insightful analysis on how the uncertainties are related to the performance of AU detection. Extensive experiments, conducted on two benchmark datasets, i.e., BP4D and DISFA, demonstrate our method achieves the state-of-the-art performance.", "label_annotations": {"Multi-aspect Summary": {"Context": "Capturing the dependencies among different facial action units (AU) is extremely important for the AU detection task.\r\nMany studies have employed graph-based deep learning methods to exploit the dependencies among AUs. However, the dependencies among AUs in real world data are often noisy and the uncertainty is essential to be taken into consideration.", "Key idea": "Authors propose an uncertain graph neural network (UGN) to learn the probabilistic mask that simultaneously captures both the individual dependencies among AUs and the uncertainties. Further, authors propose an adaptive weighted loss function based on the epistemic uncertainties to adaptively vary the weights of the training samples during the training process to account for unbalanced data distributions among AUs. ", "Method": "Authors conduct experiments on two benchmark datasets, i.e., BP4D and DISFA.", "Outcome": "Extensive experiments, conducted on two benchmark datasets, i.e., BP4D and DISFA, demonstrate the proposed method achieves the state-of-the-art performance.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 39s "}}
{"id": "b6b29c8a-7c8c-444c-b434-2ff9e166d9aa", "displayed_text": "Title: Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning\n\nAbstract:   Offline Reinforcement Learning (RL) aims to learn policies from previously collected datasets without exploring the environment. Directly applying off-policy algorithms to offline RL usually fails due to the extrapolation error caused by the out-of-distribution (OOD) actions. Previous methods tackle such problem by penalizing the Q-values of OOD actions or constraining the trained policy to be close to the behavior policy. Nevertheless, such methods typically prevent the generalization of value functions beyond the offline data and also lack precise characterization of OOD data. In this paper, we propose Pessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without explicit policy constraints. Specifically, PBRL conducts uncertainty quantification via the disagreement of bootstrapped Q-functions, and performs pessimistic updates by penalizing the value function based on the estimated uncertainty. To tackle the extrapolating error, we further propose a novel OOD sampling method. We show that such OOD sampling and pessimistic bootstrapping yields provable uncertainty quantifier in linear MDPs, thus providing the theoretical underpinning for PBRL. Extensive experiments on D4RL benchmark show that PBRL has better performance compared to the state-of-the-art algorithms. ", "label_annotations": {"Multi-aspect Summary": {"Context": "Directly applying off-policy algorithms to offline RL usually fails due to the extrapolation error caused by the out-of-distribution (OOD) actions. Previous methods tackle such problem by penalizing the Q-values of OOD actions or constraining the trained policy to be close to the behavior policy, which typically prevent the generalization of value functions beyond the offline data and also lack precise characterization of OOD data.", "Key idea": "The authors propose Pessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without explicit policy constraints, and further propose a novel OOD sampling method to tackle the extrapolating error.", "Method": "The authors conduct extensive experiments to evaluate their proposed method on D4RL benchmark.", "Outcome": "The authors show that their proposed OOD sampling and pessimistic bootstrapping yields provable uncertainty quantifier in linear MDPs. Extensive experiments on D4RL benchmark further show that the proposed PBRL has better performance compared to the state-of-the-art algorithms.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 25s "}}
{"id": "b6b29c8a-7c8c-444c-b434-2ff9e166d9aa", "displayed_text": "Title: Pessimistic Bootstrapping for Uncertainty-Driven Offline Reinforcement Learning\n\nAbstract:   Offline Reinforcement Learning (RL) aims to learn policies from previously collected datasets without exploring the environment. Directly applying off-policy algorithms to offline RL usually fails due to the extrapolation error caused by the out-of-distribution (OOD) actions. Previous methods tackle such problem by penalizing the Q-values of OOD actions or constraining the trained policy to be close to the behavior policy. Nevertheless, such methods typically prevent the generalization of value functions beyond the offline data and also lack precise characterization of OOD data. In this paper, we propose Pessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without explicit policy constraints. Specifically, PBRL conducts uncertainty quantification via the disagreement of bootstrapped Q-functions, and performs pessimistic updates by penalizing the value function based on the estimated uncertainty. To tackle the extrapolating error, we further propose a novel OOD sampling method. We show that such OOD sampling and pessimistic bootstrapping yields provable uncertainty quantifier in linear MDPs, thus providing the theoretical underpinning for PBRL. Extensive experiments on D4RL benchmark show that PBRL has better performance compared to the state-of-the-art algorithms. ", "label_annotations": {"Multi-aspect Summary": {"Context": "Previous methods tackle problem that off-policy algorithms failures due to the extrapolation error caused by the out-of-distribution (OOD) actions by penalizing the Q-values of OOD actions or constraining the trained policy to be close to the behavior policy. And these methods typically prevent the generalization of value functions beyond the offline data and also lack precise characterization of OOD data. ", "Key idea": "The authors propose Pessimistic Bootstrapping for offline RL (PBRL), a purely uncertainty-driven offline algorithm without explicit policy constraints and further propose a novel OOD sampling method to tackle the extrapolating error.", "Method": "The authors evaluate Pessimistic Bootstrapping for offline RL (PBRL) performance on D4RL benchmark.", "Outcome": "The authors prove OOD sampling and pessimistic bootstrapping yields provable uncertainty quantifier in linear MDPs, thus providing the theoretical underpinning for PBRL. \r\nExtensive experiments on D4RL benchmark show that PBRL has better performance compared to the state-of-the-art algorithms.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 40m 53s "}}
{"id": "bd84a1fd-ee85-4ac9-a6ae-e534b6013506", "displayed_text": "Title: Harmonic Decompositions of Convolutional Networks\n\nAbstract:   We consider convolutional networks from a reproducing kernel Hilbert space viewpoint. We establish harmonic decompositions of convolutional networks, that is expansions into sums of elementary functions of increasing order. The elementary functions are related to the spherical harmonics, a fundamental class of special functions on spheres. The harmonic decompositions allow us to characterize the integral operators associated with convolutional networks, and obtain as a result statistical bounds for convolutional networks. ", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors consider convolutional networks from a reproducing kernel Hilbert space viewpoint and establish harmonic decompositions of convolutional networks, that is expansions into sums of elementary functions of increasing order. ", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 49s "}}
{"id": "bd84a1fd-ee85-4ac9-a6ae-e534b6013506", "displayed_text": "Title: Harmonic Decompositions of Convolutional Networks\n\nAbstract:   We consider convolutional networks from a reproducing kernel Hilbert space viewpoint. We establish harmonic decompositions of convolutional networks, that is expansions into sums of elementary functions of increasing order. The elementary functions are related to the spherical harmonics, a fundamental class of special functions on spheres. The harmonic decompositions allow us to characterize the integral operators associated with convolutional networks, and obtain as a result statistical bounds for convolutional networks. ", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors establish harmonic decompositions of convolutional networks, which is expansions into sums of elementary functions of increasing order.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "The harmonic decompositions allow characterizing the integral operators associated with convolutional networks, and obtaining as a result statistical bounds for convolutional networks."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 23s "}}
{"id": "c268a190-6974-4190-8f48-db5dcbda8bc8", "displayed_text": "Title: Worst Case Optimal Joins on Relational and XML data\n\nAbstract: In recent data management ecosystem, one of the greatest challenges is the data variety. Data varies in multiple formats such as relational and (semi-)structured data. Traditional database handles a single type of data format and thus its ability to deal with different types of data formats is limited. To overcome such limitation, we propose a multi-model processing framework for relational and semi-structured data (i.e. XML), and design a worst-case optimal join algorithm. The salient feature of our algorithm is that it can guarantee that the intermediate results are no larger than the worst-case join results. Preliminary results show that our multi-model algorithm significantly outperforms the baseline join methods in terms of running time and intermediate result size.", "label_annotations": {"Multi-aspect Summary": {"Context": "Data varies in multiple formats such as relational and (semi-)structured data. Traditional database handles a single type of data format and thus its ability to deal with different types of data formats is limited.", "Key idea": "The author proposes a multi-model processing framework for relational and semi-structured data, and designs a worst-case optimal join algorithm. The salient feature of the algorithm is that it can guarantee that the intermediate results are no larger than the worst-case join results.", "Method": "The author assesses the proposed algorithm against the baseline join model on unning time and intermediate result size.", "Outcome": "The multi-model algorithm significantly outperforms the baseline join methods in terms of running time and intermediate result size.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 27s "}}
{"id": "c268a190-6974-4190-8f48-db5dcbda8bc8", "displayed_text": "Title: Worst Case Optimal Joins on Relational and XML data\n\nAbstract: In recent data management ecosystem, one of the greatest challenges is the data variety. Data varies in multiple formats such as relational and (semi-)structured data. Traditional database handles a single type of data format and thus its ability to deal with different types of data formats is limited. To overcome such limitation, we propose a multi-model processing framework for relational and semi-structured data (i.e. XML), and design a worst-case optimal join algorithm. The salient feature of our algorithm is that it can guarantee that the intermediate results are no larger than the worst-case join results. Preliminary results show that our multi-model algorithm significantly outperforms the baseline join methods in terms of running time and intermediate result size.", "label_annotations": {"Multi-aspect Summary": {"Context": "In recent data management ecosystem, one of the greatest challenges is the data variety. Data varies in multiple formats such as relational and (semi-)structured data. Traditional database handles a single type of data format and thus its ability to deal with different types of data formats is limited.", "Key idea": "The authors propose a multi-model processing framework for relational and semi-structured data (i.e. XML), and design a worst-case optimal join algorithm.", "Method": "The authors compare their algorithm with baseline join methods", "Outcome": "Preliminary results show that the proposed multi-model algorithm significantly outperforms the baseline join methods in terms of running time and intermediate result size.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 40s "}}
{"id": "c37e0f9f-1654-4f1f-b812-bf6f67c0c840", "displayed_text": "Title: Attack Agnostic Statistical Method for Adversarial Detection\n\nAbstract: Deep Learning based AI systems have shown great promise in various domains such as vision, audio, autonomous systems (vehicles, drones), etc. Recent research on neural networks has shown the susceptibility of deep networks to adversarial attacks - a technique of adding small perturbations to the inputs which can fool a deep network into misclassifying them. Developing defenses against such adversarial attacks is an active research area, with some approaches proposing robust models that are immune to such adversaries, while other techniques attempt to detect such adversarial inputs. In this paper, we present a novel statistical approach for adversarial detection in image classification. Our approach is based on constructing a per-class feature distribution and detecting adversaries based on comparison of features of a test image with the feature distribution of its class. For this purpose, we make use of various statistical distances such as ED (Energy Distance), MMD (Maximum Mean Discrepancy) for adversarial detection, and analyze the performance of each metric. We experimentally show that our approach achieves good adversarial detection performance on MNIST and CIFAR-10 datasets irrespective of the attack method, sample size and the degree of adversarial perturbation.", "label_annotations": {"Multi-aspect Summary": {"Context": "Recent research on neural networks has shown the susceptibility of deep networks to adversarial attacks by adding small perturbations to the inputs which can fool a deep network into misclassifying them. Current developed defenses against such adversarial attacks are using robust models that are immune to such adversaries and detecting such adversarial inputs.", "Key idea": "The author present a statistical approach for adversarial detection in image classification, which is based on constructing a per-class feature distribution and detecting adversaries based on comparison of features of a test image with the feature distribution of its class. The author uses various statistical distances such as ED and MMD for adversarial detection and analyzes the performance of each metric.", "Method": "The author uses MNIST and CIFAR-10 datasets, and various attack methods, sample sizes and degrees of adversarial perturbation to test the effectiveness of the proposal.", "Outcome": "The proposed method achieves good adversarial detection performance on MNIST and CIFAR-10 datasets irrespective of the attack method, sample size and the degree of adversarial perturbation.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 49s "}}
{"id": "c37e0f9f-1654-4f1f-b812-bf6f67c0c840", "displayed_text": "Title: Attack Agnostic Statistical Method for Adversarial Detection\n\nAbstract: Deep Learning based AI systems have shown great promise in various domains such as vision, audio, autonomous systems (vehicles, drones), etc. Recent research on neural networks has shown the susceptibility of deep networks to adversarial attacks - a technique of adding small perturbations to the inputs which can fool a deep network into misclassifying them. Developing defenses against such adversarial attacks is an active research area, with some approaches proposing robust models that are immune to such adversaries, while other techniques attempt to detect such adversarial inputs. In this paper, we present a novel statistical approach for adversarial detection in image classification. Our approach is based on constructing a per-class feature distribution and detecting adversaries based on comparison of features of a test image with the feature distribution of its class. For this purpose, we make use of various statistical distances such as ED (Energy Distance), MMD (Maximum Mean Discrepancy) for adversarial detection, and analyze the performance of each metric. We experimentally show that our approach achieves good adversarial detection performance on MNIST and CIFAR-10 datasets irrespective of the attack method, sample size and the degree of adversarial perturbation.", "label_annotations": {"Multi-aspect Summary": {"Context": "Susceptibility of deep networks to adversarial attacks - a technique of adding small perturbations to the inputs which can fool a deep network into misclassifying them.", "Key idea": "The authors present a novel statistical approach, based on constructing a per-class feature distribution and detecting adversaries based on comparison of features of a test image with the feature distribution of its class, for adversarial detection in image classification. ", "Method": "The authors make use of various statistical distances such as ED (Energy Distance), MMD (Maximum Mean Discrepancy) for adversarial detection, and analyze the performance of each metric to evaluate new statistical approach's  adversarial detection performance on MNIST and CIFAR-10 datasets.", "Outcome": "New statistical approach achieves good adversarial detection performance on MNIST and CIFAR-10 datasets irrespective of the attack method, sample size and the degree of adversarial perturbation.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "c3aa9543-0695-42b4-99af-d033e3912801", "displayed_text": "Title: Data Science for the Real Estate Industry\n\nAbstract: World's major industries, such as Financial Services, Telecom, Advertising, Healthcare, Education, etc, have attracted the attention of the KDD community for decades. Hundreds of KDD papers have been published on topics related to these industries and dozens of workshops organized---some of which have become an integral part of the conference agenda (e.g. the Health Day). Somewhat unexpectedly, the KDD conference has barely addressed the real estate industry, despite its enormous size and prominence. The reason for that apparent mismatch is two-fold: (a) until recently, the real estate industry did not appreciate the value data science methods could add (with some exceptions, such as econometrics methods for creating real-estate price indices); (b) the Data Science community has not been aware of challenging real estate problems that are perfectly suited to its methods. This tutorial provides a step towards resolving this issue. We provide an introduction to real estate for data scientists, and outline a spectrum of data science problems, many of which are being tackled by new \"prop-tech\" companies, while some are yet to be approached. We present concrete examples from three of these companies (where the authors work): Airbnb -- the most popular short-term rental marketplace, Cherre -- a real estate data integration platform, and Compass -- the largest independent real estate brokerage in the U.S.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "Hundreds of KDD papers have been published on topics related to these industries and dozens of workshops organized. However, KDD conference has barely addressed the real estate industry, despite its enormous size and prominence. ", "Key idea": "Authors discuss the reasons why KDD has never discuss real estate industry, (a) until recently, the real estate industry did not appreciate the value data science methods could add (with some exceptions, such as econometrics methods for creating real-estate price indices); (b) the Data Science community has not been aware of challenging real estate problems that are perfectly suited to its methods.\r\n\r\nThis tutorial provides a step towards resolving this issue.", "Method": "Authors provide an introduction to real estate for data scientists, and outline a spectrum of data science problems, many of which are being tackled by new \"prop-tech\" companies, while some are yet to be approached. Authors present concrete examples from three of these companies (where the authors work): Airbnb -- the most popular short-term rental marketplace, Cherre -- a real estate data integration platform, and Compass -- the largest independent real estate brokerage in the U.S.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 17s "}}
{"id": "c3aa9543-0695-42b4-99af-d033e3912801", "displayed_text": "Title: Data Science for the Real Estate Industry\n\nAbstract: World's major industries, such as Financial Services, Telecom, Advertising, Healthcare, Education, etc, have attracted the attention of the KDD community for decades. Hundreds of KDD papers have been published on topics related to these industries and dozens of workshops organized---some of which have become an integral part of the conference agenda (e.g. the Health Day). Somewhat unexpectedly, the KDD conference has barely addressed the real estate industry, despite its enormous size and prominence. The reason for that apparent mismatch is two-fold: (a) until recently, the real estate industry did not appreciate the value data science methods could add (with some exceptions, such as econometrics methods for creating real-estate price indices); (b) the Data Science community has not been aware of challenging real estate problems that are perfectly suited to its methods. This tutorial provides a step towards resolving this issue. We provide an introduction to real estate for data scientists, and outline a spectrum of data science problems, many of which are being tackled by new \"prop-tech\" companies, while some are yet to be approached. We present concrete examples from three of these companies (where the authors work): Airbnb -- the most popular short-term rental marketplace, Cherre -- a real estate data integration platform, and Compass -- the largest independent real estate brokerage in the U.S.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "KDD conference has barely addressed the real estate industry, despite its enormous size and prominence.", "Key idea": "This tutorial provides a step towards resolving the problems that the real estate industry did not appreciate the value data science methods could add and the Data Science community has not been aware of challenging real estate problems that are perfectly suited to its methods.  ", "Method": "This tutorial provides an introduction to real estate for data scientists, and outline a spectrum of data science problems. It presents concrete examples from three of these companies (where the authors work): Airbnb -- the most popular short-term rental marketplace, Cherre -- a real estate data integration platform, and Compass -- the largest independent real estate brokerage in the U.S.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 4s "}}
{"id": "c5c22c6f-e9e9-4b0a-ac57-d3baa100033b", "displayed_text": "Title: Conditional gradient methods for stochastically constrained convex minimization\n\nAbstract:   We propose two novel conditional gradient-based methods for solving structured stochastic convex optimization problems with a large number of linear constraints. Instances of this template naturally arise from SDP-relaxations of combinatorial problems, which involve a number of constraints that is polynomial in the problem dimension. The most important feature of our framework is that only a subset of the constraints is processed at each iteration, thus gaining a computational advantage over prior works that require full passes. Our algorithms rely on variance reduction and smoothing used in conjunction with conditional gradient steps, and are accompanied by rigorous convergence guarantees. Preliminary numerical experiments are provided for illustrating the practical performance of the methods. ", "label_annotations": {"Multi-aspect Summary": {"Context": " Instances of structured stochastic convex optimization problems with a large number of linear constraints naturally arise from SDP-relaxations of combinatorial problems, which involve a number of constraints that is polynomial in the problem dimension.", "Key idea": "Authors propose two novel conditional gradient-based methods for solving structured stochastic convex optimization problems with a large number of linear constraints. The most important feature of proposed framework is that only a subset of the constraints is processed at each iteration, thus gaining a computational advantage over prior works that require full passes. ", "Method": "Authors conduct rigorous convergence analysis theoretically. They also provide preliminary numerical experiments.", "Outcome": "Proposed algorithms rely on variance reduction and smoothing used in conjunction with conditional gradient steps, and are accompanied by rigorous convergence guarantees. Preliminary numerical experiments are provided for illustrating the practical performance of the methods.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 42s "}}
{"id": "c5c22c6f-e9e9-4b0a-ac57-d3baa100033b", "displayed_text": "Title: Conditional gradient methods for stochastically constrained convex minimization\n\nAbstract:   We propose two novel conditional gradient-based methods for solving structured stochastic convex optimization problems with a large number of linear constraints. Instances of this template naturally arise from SDP-relaxations of combinatorial problems, which involve a number of constraints that is polynomial in the problem dimension. The most important feature of our framework is that only a subset of the constraints is processed at each iteration, thus gaining a computational advantage over prior works that require full passes. Our algorithms rely on variance reduction and smoothing used in conjunction with conditional gradient steps, and are accompanied by rigorous convergence guarantees. Preliminary numerical experiments are provided for illustrating the practical performance of the methods. ", "label_annotations": {"Multi-aspect Summary": {"Context": "Previous methods for solving structured stochastic convex optimization problems require full passes of the constraints.", "Key idea": "The authors propose two novel conditional gradient-based methods for solving structured stochastic convex optimization problems with a large number of linear constraints, which has a computational advantage over prior works.", "Method": "The authors run preliminary numerical experiments for their proposed methods.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 19s "}}
{"id": "c676aecf-7468-4258-bb41-22bc1811bc3a", "displayed_text": "Title: DMCS : Density Modularity based Community Search\n\nAbstract: Community Search, or finding a connected subgraph (known as a community) containing the given query nodes in a social network, is a fundamental problem. Most of the existing community search models only focus on the internal cohesiveness of a community. However, a high-quality community often has high modularity, which means dense connections inside communities and sparse connections to the nodes outside the community. In this paper, we conduct a pioneer study on searching a community with high modularity. We point out that while modularity has been popularly used in community detection (without query nodes), it has not been adopted for community search, surprisingly, and its application in community search (related to query nodes) brings in new challenges. We address these challenges by designing a new graph modularity function named Density Modularity. To the best of our knowledge, this is the first work on the community search problem using graph modularity. The community search based on the density modularity, termed as DMCS, is to find a community in a social network that contains all the query nodes and has high density-modularity. We prove that the DMCS problem is NP-hard. To efficiently address DMCS, we present new algorithms that run in log-linear time to the graph size. We conduct extensive experimental studies in real-world and synthetic networks, which offer insights into the efficiency and effectiveness of our algorithms. In particular, our algorithm achieves up to 8.5 times higher accuracy in terms of NMI than baseline algorithms.", "label_annotations": {"Multi-aspect Summary": {"Context": "Most of the existing community search models only focus on the internal cohesiveness of a community, and a high-quality community often has dense connections inside communities and sparse connections to the nodes outside the community. Modularity in community search is not practiced and challenging.", "Key idea": "The author designs a first-ever graph modularity on community search and tries to efficiently address density modularity community search by using algorithms that run in log-linear time to the graph size.", "Method": "The author tests the proposed algorithms by conducting studies in real-world and synthetic networks.", "Outcome": "The author\u2019s algorithm achieves up to 8.5 times higher accuracy in terms of NMI than baseline algorithms.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 45s "}}
{"id": "c676aecf-7468-4258-bb41-22bc1811bc3a", "displayed_text": "Title: DMCS : Density Modularity based Community Search\n\nAbstract: Community Search, or finding a connected subgraph (known as a community) containing the given query nodes in a social network, is a fundamental problem. Most of the existing community search models only focus on the internal cohesiveness of a community. However, a high-quality community often has high modularity, which means dense connections inside communities and sparse connections to the nodes outside the community. In this paper, we conduct a pioneer study on searching a community with high modularity. We point out that while modularity has been popularly used in community detection (without query nodes), it has not been adopted for community search, surprisingly, and its application in community search (related to query nodes) brings in new challenges. We address these challenges by designing a new graph modularity function named Density Modularity. To the best of our knowledge, this is the first work on the community search problem using graph modularity. The community search based on the density modularity, termed as DMCS, is to find a community in a social network that contains all the query nodes and has high density-modularity. We prove that the DMCS problem is NP-hard. To efficiently address DMCS, we present new algorithms that run in log-linear time to the graph size. We conduct extensive experimental studies in real-world and synthetic networks, which offer insights into the efficiency and effectiveness of our algorithms. In particular, our algorithm achieves up to 8.5 times higher accuracy in terms of NMI than baseline algorithms.", "label_annotations": {"Multi-aspect Summary": {"Context": "Most of the existing community search models only focus on the internal cohesiveness of a community. \r\nWhile modularity has been popularly used in community detection (without query nodes), it has not been adopted for community search and its application in community search (related to query nodes) brings in new challenges. ", "Key idea": "The authors design a new graph modularity function named Density Modularity, and the community search based on the density modularity, termed as DMCS, is to find a community in a social network that contains all the query nodes and has high density-modularity.  And the authors present new algorithms that run in log-linear time to the graph size to address DMCS.", "Method": "The authors conduct extensive experimental studies using new algorithm to address the community search based on the density modularity(DMCS) problem in real-world and synthetic networks in terms of NMI.", "Outcome": " New algorithm solving the community search based on the density modularity(DMCS) problem achieves up to 8.5 times higher accuracy in terms of NMI than baseline algorithms.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "ca53b2c4-2912-4515-aae6-938c3f268a60", "displayed_text": "Title: Detecting epidemic tendency by mining search logs\n\nAbstract: We consider the problem of detecting epidemic tendency by mining search logs. We propose an algorithm based on click-through information to select epidemic related queries/terms. We adopt linear regression to model epidemic occurrences and frequencies of epidemic related terms (ERTs) in search logs. The results show our algorithm is effective in finding ERTs which obtain a high correlation value with epidemic occurrences. We also find the proposed method performs better when combining different ERTs than using single ERT.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors propose an algorithm based on click-through information to select epidemic related queries/terms, and adopt linear regression to model epidemic occurrences and frequencies of epidemic related terms (ERTs) in search log.", "Method": "N/A", "Outcome": "The results show the proposed algorithm is effective in finding ERTs which obtain a high correlation value with epidemic occurrences. The proposed method performs better when combining different ERTs than using single ERT.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 3s "}}
{"id": "ca53b2c4-2912-4515-aae6-938c3f268a60", "displayed_text": "Title: Detecting epidemic tendency by mining search logs\n\nAbstract: We consider the problem of detecting epidemic tendency by mining search logs. We propose an algorithm based on click-through information to select epidemic related queries/terms. We adopt linear regression to model epidemic occurrences and frequencies of epidemic related terms (ERTs) in search logs. The results show our algorithm is effective in finding ERTs which obtain a high correlation value with epidemic occurrences. We also find the proposed method performs better when combining different ERTs than using single ERT.", "label_annotations": {"Multi-aspect Summary": {"Context": "The problem of detecting epidemic tendency is important.", "Key idea": "Authors propose an algorithm based on click-through information to select epidemic related queries/terms. Authors adopt linear regression to model epidemic occurrences and frequencies of epidemic related terms (ERTs) in search logs.", "Method": "Authors design experiments to find epidemic related terms (ERTs).", "Outcome": "Experimental results show the proposed algorithm is effective in finding ERTs which obtain a high correlation value with epidemic occurrences. Authors also find the proposed method performs better when combining different ERTs than using single ERT.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 51s "}}
{"id": "ccae9338-7379-4af1-8fea-8945ba429c5c", "displayed_text": "Title: Triplet Loss in Siamese Network for Object Tracking\n\nAbstract: Object tracking is still a critical and challenging problem with many applications in computer vision. For this challenge, more and more researchers pay attention to applying deep learning to get powerful feature for better tracking accuracy. In this paper, a novel triplet loss is proposed to extract expressive deep feature for object tracking by adding it into Siamese network framework instead of pairwise loss for training. Without adding any inputs, our approach is able to utilize more elements for training to achieve more powerful feature via the combination of original samples. Furthermore, we propose a theoretical analysis by combining comparison of gradients and back-propagation, to prove the effectiveness of our method. In experiments, we apply the proposed triplet loss for three real-time trackers based on Siamese network. And the results on several popular tracking benchmarks show our variants operate at almost the same frame-rate with baseline trackers and achieve superior tracking performance than them, as well as the comparable accuracy with recent state-of-the-art real-time trackers.", "label_annotations": {"Multi-aspect Summary": {"Context": "Object tracking is still a critical and challenging problem with many applications in computer vision. For this challenge, more and more researchers pay attention to applying deep learning to get powerful feature for better tracking accuracy.", "Key idea": "The authors propose a novel triplet loss to extract expressive deep feature for object tracking by adding it into Siamese network framework instead of pairwise loss for training.", "Method": "The authors propose a theoretical analysis by combining comparison of gradients and back-propagation, to prove the effectiveness of their proposed method. They apply the proposed triplet loss for three real-time trackers based on Siamese network on several popular tracking benchmarks.", "Outcome": "The experimental results on several popular tracking benchmarks show that the proposed methods operate at almost the same frame-rate with baseline trackers and achieve superior tracking performance than them, as well as the comparable accuracy with recent state-of-the-art real-time trackers.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 41s "}}
{"id": "ccae9338-7379-4af1-8fea-8945ba429c5c", "displayed_text": "Title: Triplet Loss in Siamese Network for Object Tracking\n\nAbstract: Object tracking is still a critical and challenging problem with many applications in computer vision. For this challenge, more and more researchers pay attention to applying deep learning to get powerful feature for better tracking accuracy. In this paper, a novel triplet loss is proposed to extract expressive deep feature for object tracking by adding it into Siamese network framework instead of pairwise loss for training. Without adding any inputs, our approach is able to utilize more elements for training to achieve more powerful feature via the combination of original samples. Furthermore, we propose a theoretical analysis by combining comparison of gradients and back-propagation, to prove the effectiveness of our method. In experiments, we apply the proposed triplet loss for three real-time trackers based on Siamese network. And the results on several popular tracking benchmarks show our variants operate at almost the same frame-rate with baseline trackers and achieve superior tracking performance than them, as well as the comparable accuracy with recent state-of-the-art real-time trackers.", "label_annotations": {"Multi-aspect Summary": {"Context": "Object tracking is still a critical and challenging problem with many applications in computer vision. For this challenge, more and more researchers pay attention to applying deep learning to get powerful feature for better tracking accuracy", "Key idea": "Authors propose  a novel triplet loss is proposed to extract expressive deep feature for object tracking by adding it into Siamese network framework instead of pairwise loss for training. Without adding any inputs, the proposed approach is able to utilize more elements for training to achieve more powerful feature via the combination of original samples. Furthermore, authors propose a theoretical analysis by combining comparison of gradients and back-propagation, to prove the effectiveness of our method.", "Method": "Authors conduct experiments on several popular tracking benchmarks.", "Outcome": "Results on several popular tracking benchmarks show the proposed variants operate at almost the same frame-rate with baseline trackers and achieve superior tracking performance than them, as well as the comparable accuracy with recent state-of-the-art real-time trackers.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 13s "}}
{"id": "ce74316d-c5dc-47f1-b0c4-0591bc3fb4b6", "displayed_text": "Title: On the integration of structure indexes and inverted lists\n\nAbstract: Several methods have been proposed to evaluate queries over a native XML DBMS, where the queries specify both path and keyword constraints. These broadly consist of graph traversal approaches, optimized with auxiliary structures known as structure indexes; and approaches based on information-retrieval style inverted lists. We propose a strategy that combines the two forms of auxiliary indexes, and a query evaluation algorithm for branching path expressions based on this strategy. Our technique is general and applicable for a wide range of choices of structure indexes and inverted list join algorithms. Our experiments over the Niagara XML DBMS show the benefit of integrating the two forms of indexes. We also consider algorithmic issues in evaluating path expression queries when the notion of relevance ranking is incorporated. By integrating the above techniques with the Threshold Algorithm proposed by Fagin et al., we obtain instance optimal algorithms to push down top k computation.", "label_annotations": {"Multi-aspect Summary": {"Context": "Several methods have been proposed to evaluate queries over a native XML DBMS and these methods  broadly consist of graph traversal approaches, optimized with auxiliary structures known as structure indexes; and approaches based on information-retrieval style inverted lists. ", "Key idea": "The authors propose a strategy that combines the two forms of auxiliary indexes, and a query evaluation algorithm for branching path expressions based on this strategy.", "Method": "The authors perform experiments over the Niagara XML DBMS to evaluate benefits of new strategy.", "Outcome": "Experiments over the Niagara XML DBMS show the benefit of integrating the two forms of indexes. Integrating the techniques that combines the two forms of auxiliary indexes with the Threshold Algorithm proposed by Fagin et al., instance optimal algorithms to push down top k computation has been obtained.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 10s "}}
{"id": "ce74316d-c5dc-47f1-b0c4-0591bc3fb4b6", "displayed_text": "Title: On the integration of structure indexes and inverted lists\n\nAbstract: Several methods have been proposed to evaluate queries over a native XML DBMS, where the queries specify both path and keyword constraints. These broadly consist of graph traversal approaches, optimized with auxiliary structures known as structure indexes; and approaches based on information-retrieval style inverted lists. We propose a strategy that combines the two forms of auxiliary indexes, and a query evaluation algorithm for branching path expressions based on this strategy. Our technique is general and applicable for a wide range of choices of structure indexes and inverted list join algorithms. Our experiments over the Niagara XML DBMS show the benefit of integrating the two forms of indexes. We also consider algorithmic issues in evaluating path expression queries when the notion of relevance ranking is incorporated. By integrating the above techniques with the Threshold Algorithm proposed by Fagin et al., we obtain instance optimal algorithms to push down top k computation.", "label_annotations": {"Multi-aspect Summary": {"Context": "Several methods have been proposed to evaluate queries over a native XML DBMS, where the queries specify both path and keyword constraints. These broadly consist of graph traversal approaches, optimized with auxiliary structures known as structure indexes; and approaches based on information-retrieval style inverted lists.", "Key idea": "The authors propose a strategy that combines the two forms of auxiliary indexes, and a query evaluation algorithm for branching path expressions based on this strategy. They further integrate these techniques with the Threshold Algorithm proposed by Fagin et al., and obtain instance optimal algorithms to push down top k computation.", "Method": "The authors conduct experiments over the Niagara XML DBMS", "Outcome": "The experiments over the Niagara XML DBMS show the benefit of adapting their proposed stratgey.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 36s "}}
{"id": "db900c02-9a35-4a83-aa19-15b763259100", "displayed_text": "Title: Quantum Algorithms for Deep Convolutional Neural Networks\n\nAbstract: Quantum computing is a powerful computational paradigm with applications in several fields, including machine learning. In the last decade, deep learning, and in particular Convolutional Neural Networks (CNN), have become essential for applications in signal processing and image recognition. Quantum deep learning, however, remains a challenging problem, as it is difficult to implement non linearities with quantum unitaries. In this paper we propose a quantum algorithm for evaluating and training deep convolutional neural networks with potential speedups over classical CNNs for both the forward and backward passes. The quantum CNN (QCNN) reproduces completely the outputs of the classical CNN and allows for non linearities and pooling operations. The QCNN is in particular interesting for deep networks and could allow new frontiers in the image recognition domain, by allowing for many more convolution kernels, larger kernels, high dimensional inputs and high depth input channels. We also present numerical simulations for the classification of the MNIST dataset to provide practical evidence for the efficiency of the QCNN.", "label_annotations": {"Multi-aspect Summary": {"Context": "Quantum computing is a powerful computational paradigm with applications in several fields and deep learning is essential for applications in signal processing and image recognition. Quantum deep learning, however, remains a challenging problem, as it is difficult to implement non linearities with quantum unitaries.", "Key idea": "The authors propose a quantum algorithm for evaluating and training deep convolutional neural networks with potential speedups over classical CNNs for both the forward and backward passes.", "Method": "The authors evaluate their proposed method using the classification of the MNIST dataset.", "Outcome": "The authors present numerical simulations for the classification of the MNIST dataset to provide practical evidence for the efficiency of the QCNN.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 3s "}}
{"id": "db900c02-9a35-4a83-aa19-15b763259100", "displayed_text": "Title: Quantum Algorithms for Deep Convolutional Neural Networks\n\nAbstract: Quantum computing is a powerful computational paradigm with applications in several fields, including machine learning. In the last decade, deep learning, and in particular Convolutional Neural Networks (CNN), have become essential for applications in signal processing and image recognition. Quantum deep learning, however, remains a challenging problem, as it is difficult to implement non linearities with quantum unitaries. In this paper we propose a quantum algorithm for evaluating and training deep convolutional neural networks with potential speedups over classical CNNs for both the forward and backward passes. The quantum CNN (QCNN) reproduces completely the outputs of the classical CNN and allows for non linearities and pooling operations. The QCNN is in particular interesting for deep networks and could allow new frontiers in the image recognition domain, by allowing for many more convolution kernels, larger kernels, high dimensional inputs and high depth input channels. We also present numerical simulations for the classification of the MNIST dataset to provide practical evidence for the efficiency of the QCNN.", "label_annotations": {"Multi-aspect Summary": {"Context": "Quantum deep learning remains a challenging problem, as it is difficult to implement non linearities with quantum unitaries.", "Key idea": "The author proposes a quantum algorithm for evaluating and training deep convolutional neural networks with potential speedups over classical CNNs for both the forward and backward passes. The quantum CNN (QCNN) reproduces completely the outputs of the classical CNN and allows for non linearities and pooling operations and it is in particular interesting for deep networks and could allow new frontiers in the image recognition domain.", "Method": "The author presents numerical simulations for the classification of the MNIST dataset.", "Outcome": "The numerical simulations for the classification of the MNIST dataset provide practical evidence for the efficiency of the QCNN.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 25s "}}
{"id": "dca09f6f-b63a-42a1-9eb5-fbfa45bc6389", "displayed_text": "Title: Modeling Heart Rate and Activity Data for Personalized Fitness Recommendation\n\nAbstract: Activity logs collected from wearable devices (e.g. Apple Watch, Fitbit, etc.) are a promising source of data to facilitate a wide range of applications such as personalized exercise scheduling, workout recommendation, and heart rate anomaly detection. However, such data are heterogeneous, noisy, diverse in scale and resolution, and have complex interdependencies, making them challenging to model. In this paper, we develop context-aware sequential models to capture the personalized and temporal patterns of fitness data. Specifically, we propose FitRec - an LSTM-based model that captures two levels of context information: context within a specific activity, and context across a user's activity history. We are specifically interested in (a) estimating a user's heart rate profile for a candidate activity; and (b) predicting and recommending suitable activities on this basis. We evaluate our model on a novel dataset containing over 250 thousand workout records coupled with hundreds of millions of parallel sensor measurements (e.g. heart rate, GPS) and metadata. We demonstrate that the model is able to learn contextual, personalized, and activity-specific dynamics of users' heart rate profiles during exercise. We evaluate the proposed model against baselines on several personalized recommendation tasks, showing the promise of using wearable data for activity modeling and recommendation.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "Activity logs collected from wearable devices (e.g. Apple Watch, Fitbit, etc.) are a promising source of data to facilitate a wide range of applications such as personalized exercise scheduling, workout recommendation, and heart rate anomaly detection. However, such data are heterogeneous, noisy, diverse in scale and resolution, and have complex interdependencies, making them challenging to model.", "Key idea": "Authors develop context-aware sequential models to capture the personalized and temporal patterns of fitness data. Specifically, they propose FitRec - an LSTM-based model that captures two levels of context information: context within a specific activity, and context across a user's activity history.", "Method": "Authors  evaluate our model on a novel dataset containing over 250 thousand workout records coupled with hundreds of millions of parallel sensor measurements (e.g. heart rate, GPS) and metadata. ", "Outcome": "Authors  demonstrate that the model is able to learn contextual, personalized, and activity-specific dynamics of users' heart rate profiles during exercise. \r\nAuthors also evaluate the proposed model against baselines on several personalized recommendation tasks, showing the promise of using wearable data for activity modeling and recommendation.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 31s "}}
{"id": "dca09f6f-b63a-42a1-9eb5-fbfa45bc6389", "displayed_text": "Title: Modeling Heart Rate and Activity Data for Personalized Fitness Recommendation\n\nAbstract: Activity logs collected from wearable devices (e.g. Apple Watch, Fitbit, etc.) are a promising source of data to facilitate a wide range of applications such as personalized exercise scheduling, workout recommendation, and heart rate anomaly detection. However, such data are heterogeneous, noisy, diverse in scale and resolution, and have complex interdependencies, making them challenging to model. In this paper, we develop context-aware sequential models to capture the personalized and temporal patterns of fitness data. Specifically, we propose FitRec - an LSTM-based model that captures two levels of context information: context within a specific activity, and context across a user's activity history. We are specifically interested in (a) estimating a user's heart rate profile for a candidate activity; and (b) predicting and recommending suitable activities on this basis. We evaluate our model on a novel dataset containing over 250 thousand workout records coupled with hundreds of millions of parallel sensor measurements (e.g. heart rate, GPS) and metadata. We demonstrate that the model is able to learn contextual, personalized, and activity-specific dynamics of users' heart rate profiles during exercise. We evaluate the proposed model against baselines on several personalized recommendation tasks, showing the promise of using wearable data for activity modeling and recommendation.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "Since data collected from wearable devices are heterogeneous, noisy, diverse in scale and resolution, and have complex interdependencies, it's challenging to model these data.", "Key idea": "The authors propose FitRec, an LSTM-based model, which is a context-aware sequential models to capture the personalized and temporal patterns of fitness data.", "Method": "The authors evaluate FitRec model on a novel dataset containing over 250 thousand workout records coupled with hundreds of millions of parallel sensor measurements (e.g. heart rate, GPS) and metadata. And the authors compare FitRec model against baselines on several personalized recommendation tasks.", "Outcome": "FitRec model is able to learn contextual, personalized, and activity-specific dynamics of users' heart rate profiles during exercise. FitRec model have better performance on several personalized recommendation task than baselines.", "Future Impact": "FitRec model provides better promise of using wearable data for activity modeling and recommendation."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "dd282632-ee41-45da-add8-d68d89c57e2d", "displayed_text": "Title: Bagging Regional Classification Activation Maps for Weakly Supervised Object Localization.\n\nAbstract: Classification activation map (CAM), utilizing the classification structure to generate pixel-wise localization maps, is a crucial mechanism for weakly supervised object localization (WSOL). However, CAM directly uses the classifier trained on image-level features to locate objects, making it prefers to discern global discriminative factors rather than regional object cues. Thus only the discriminative locations are activated when feeding pixel-level features into this classifier. To solve this issue, this paper elaborates a plug-and-play mechanism called BagCAMs to better project a well-trained classifier for the localization task without refining or re-training the baseline structure. Our BagCAMs adopts a proposed regional localizer generation (RLG) strategy to define a set of regional localizers and then derive them from a well-trained classifier. These regional localizers can be viewed as the base learner that only discerns region-wise object factors for localization tasks, and their results can be effectively weighted by our BagCAMs to form the final localization map. Experiments indicate that adopting our proposed BagCAMs can improve the performance of baseline WSOL methods to a great extent and obtains state-of-the-art performance on three WSOL benchmarks. Code are released at https://github.com/zh460045050/BagCAMs.", "label_annotations": {"Multi-aspect Summary": {"Context": "Classification activation map (CAM) is a crucial mechanism for weakly supervised object localization (WSOL). However, CAM directly uses the classifier trained on image-level features to locate objects, making it prefers to discern global discriminative factors rather than regional object cues, so only the discriminative locations are activated when feeding pixel-level features into this classifier.", "Key idea": "The authors elaborate a plug-and-play mechanism called BagCAMs to better project a well-trained classifier for the localization task without refining or re-training the baseline structure.", "Method": "The authors evaluate BagCAMs on three WSOL benchmarks and compare it to baseline WSOL methods.", "Outcome": "Experiments indicate that adopting the proposed BagCAMs can improve the performance of baseline WSOL methods to a great extent and obtains state-of-the-art performance on three WSOL benchmarks.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 10m 36s "}}
{"id": "dd282632-ee41-45da-add8-d68d89c57e2d", "displayed_text": "Title: Bagging Regional Classification Activation Maps for Weakly Supervised Object Localization.\n\nAbstract: Classification activation map (CAM), utilizing the classification structure to generate pixel-wise localization maps, is a crucial mechanism for weakly supervised object localization (WSOL). However, CAM directly uses the classifier trained on image-level features to locate objects, making it prefers to discern global discriminative factors rather than regional object cues. Thus only the discriminative locations are activated when feeding pixel-level features into this classifier. To solve this issue, this paper elaborates a plug-and-play mechanism called BagCAMs to better project a well-trained classifier for the localization task without refining or re-training the baseline structure. Our BagCAMs adopts a proposed regional localizer generation (RLG) strategy to define a set of regional localizers and then derive them from a well-trained classifier. These regional localizers can be viewed as the base learner that only discerns region-wise object factors for localization tasks, and their results can be effectively weighted by our BagCAMs to form the final localization map. Experiments indicate that adopting our proposed BagCAMs can improve the performance of baseline WSOL methods to a great extent and obtains state-of-the-art performance on three WSOL benchmarks. Code are released at https://github.com/zh460045050/BagCAMs.", "label_annotations": {"Multi-aspect Summary": {"Context": "Classification activation map (CAM), utilizing the classification structure to generate pixel-wise localization maps, is a crucial mechanism for weakly supervised object localization (WSOL). However, CAM directly uses the classifier trained on image-level features to locate objects, making it prefers to discern global discriminative factors rather than regional object cues. Thus only the discriminative locations are activated when feeding pixel-level features into this classifier. ", "Key idea": "Authors propose a plug-and-play mechanism called BagCAMs to better project a well-trained classifier for the localization task without refining or re-training the baseline structure. BagCAMs adopts a proposed regional localizer generation (RLG) strategy to define a set of regional localizers and then derive them from a well-trained classifier. These regional localizers can be viewed as the base learner that only discerns region-wise object factors for localization tasks, and their results can be effectively weighted by our BagCAMs to form the final localization map. ", "Method": "Authors experiment on three eakly supervised object localization (WSOL) benchmarks.", "Outcome": "Authors show that adopting our proposed BagCAMs can improve the performance of baseline WSOL methods to a great extent and obtains state-of-the-art performance on three WSOL benchmarks. ", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 46s "}}
{"id": "ddf8f49c-342a-4cd0-8b3b-b588af08ed0d", "displayed_text": "Title: On the effectiveness of evaluating retrieval systems in the absence of relevance judgments\n\nAbstract: Soboroff, Nicholas and Cahan recently proposed a method for evaluating the performance of retrieval systems without relevance judgments. They demonstrated that the system evaluations produced by their methodology are correlated with actual evaluations using relevance judgments in the TREC competition. In this work, we propose an explanation for this phenomenon. We devise a simple measure for quantifying the similarity of retrieval systems by assessing the similarity of their retrieved results. Then, given a collection of retrieval systems and their retrieved results, we use this measure to assess the average similarity of a system to the other systems in the collection. We demonstrate that evaluating retrieval systems according to average similarity yields results quite similar to the methodology proposed by Soboroff et~al., and we further demonstrate that these two techniques are in fact highly correlated. Thus, the techniques are effectively evaluating and ranking retrieval systems by popularity\" as opposed to performance.", "label_annotations": {"Multi-aspect Summary": {"Context": "Soboroff, Nicholas and Cahan proposed a method for evaluating the performance of retrieval systems without relevance judgments and demonstrated that the system evaluations produced by their methodology are correlated with actual evaluations using relevance judgments in the TREC competition.", "Key idea": "The authors propose an explanation for phenomenon, the system evaluations are correlated with actual evaluations using relevance judgments in the TREC competition.", "Method": "The authors devise a simple measure for quantifying the similarity of retrieval systems by assessing the similarity of their retrieved results and they use this measure to assess the average similarity of a system to the other systems in the collection. ", "Outcome": "The authors prove that evaluating retrieval systems according to average similarity yields results  are quite similar to the methodology proposed by Soboroff et~al and these two techniques are in fact highly correlated. ", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 32s "}}
{"id": "ddf8f49c-342a-4cd0-8b3b-b588af08ed0d", "displayed_text": "Title: On the effectiveness of evaluating retrieval systems in the absence of relevance judgments\n\nAbstract: Soboroff, Nicholas and Cahan recently proposed a method for evaluating the performance of retrieval systems without relevance judgments. They demonstrated that the system evaluations produced by their methodology are correlated with actual evaluations using relevance judgments in the TREC competition. In this work, we propose an explanation for this phenomenon. We devise a simple measure for quantifying the similarity of retrieval systems by assessing the similarity of their retrieved results. Then, given a collection of retrieval systems and their retrieved results, we use this measure to assess the average similarity of a system to the other systems in the collection. We demonstrate that evaluating retrieval systems according to average similarity yields results quite similar to the methodology proposed by Soboroff et~al., and we further demonstrate that these two techniques are in fact highly correlated. Thus, the techniques are effectively evaluating and ranking retrieval systems by popularity\" as opposed to performance.", "label_annotations": {"Multi-aspect Summary": {"Context": "Soboroff, Nicholas and Cahan recently proposed a method for evaluating the performance of retrieval systems without relevance judgments. They demonstrated that the system evaluations produced by their methodology are correlated with actual evaluations using relevance judgments in the TREC competition.", "Key idea": "The authors propose an explanation for the phenomenon that the system evaluations produced by the methodology introduced by Soboroff et al. are correlated with actual evaluations using relevance judgments in the TREC competition.", "Method": "The authors devise a simple measure for quantifying the similarity of retrieval systems by assessing the similarity of their retrieved results. Then, they compare the measured similarity and compare it the the results yielded by the methodology proposed by Soboroff et al.", "Outcome": "The authors demonstrate that evaluating retrieval systems according to average similarity yields results quite similar to the methodology proposed by Soboroff et al., and they further demonstrate that these two techniques are in fact highly correlated.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 7m 17s "}}
{"id": "ded9a095-d94d-4ccd-8825-283ec4bb7093", "displayed_text": "Title: Evolution of design competence in UX practice\n\nAbstract: There has been increasing interest in the adoption of UX within corporate environments, and what competencies translate into effective UX design. This paper addresses the space between pedagogy and UX practice through the lens of competence, with the goal of understanding how students are initiated into the practice community, how their perception of competence shifts over time, and what factors influence this shift. A 12-week longitudinal data collection, including surveys and interviews, documents this shift, with participants beginning internships and full-time positions in UX. Students and early professionals were asked to assess their level of competence and factors that influenced competence. A co-construction of identity between the designer and their environment is proposed, with a variety of factors relating to tool and representational knowledge, complexity, and corporate culture influencing perceptions of competence in UX over time. Opportunities for future research, particularly in building an understanding of competency in UX based on this preliminary framing of early UX practice are addressed.", "label_annotations": {"Multi-aspect Summary": {"Context": "There has been increasing interest in the adoption of UX within corporate environments, and what competencies translate into effective UX design.", "Key idea": "The author addresses the space between pedagogy and UX practice through the lens of competence, in order to understand how students are initiated into the practice community, how their perception of competence shifts over time, and what factors influence this shift. After the data collection on the observation, a co-construction of identity between the designer and their environment is proposed.", "Method": "The author conducts a 12-week longitudinal data collection, including surveys and interviews, documents this shift, with participants beginning internships and full-time positions in UX. Students and early professionals were asked to assess their level of competence and factors that influenced competence.", "Outcome": "N/A", "Future Impact": "Opportunities for future research, particularly in building an understanding of competency in UX based on this preliminary framing of early UX practice are addressed."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 26s "}}
{"id": "ded9a095-d94d-4ccd-8825-283ec4bb7093", "displayed_text": "Title: Evolution of design competence in UX practice\n\nAbstract: There has been increasing interest in the adoption of UX within corporate environments, and what competencies translate into effective UX design. This paper addresses the space between pedagogy and UX practice through the lens of competence, with the goal of understanding how students are initiated into the practice community, how their perception of competence shifts over time, and what factors influence this shift. A 12-week longitudinal data collection, including surveys and interviews, documents this shift, with participants beginning internships and full-time positions in UX. Students and early professionals were asked to assess their level of competence and factors that influenced competence. A co-construction of identity between the designer and their environment is proposed, with a variety of factors relating to tool and representational knowledge, complexity, and corporate culture influencing perceptions of competence in UX over time. Opportunities for future research, particularly in building an understanding of competency in UX based on this preliminary framing of early UX practice are addressed.", "label_annotations": {"Multi-aspect Summary": {"Context": "There has been increasing interest in the adoption of UX within corporate environments, and what competencies translate into effective UX design.", "Key idea": "The authors address the space between pedagogy and UX practice through the lens of competence, with the goal of understanding how students are initiated into the practice community, how their perception of competence shifts over time, and what factors influence this shift.", "Method": "The authors collect data from surveys and interviews. Students and early professionals were asked to assess their level of competence and factors that influenced competence.", "Outcome": "A co-construction of identity between the designer and their environment is proposed, with a variety of factors relating to tool and representational knowledge, complexity, and corporate culture influencing perceptions of competence in UX over time.", "Future Impact": "The authors address opportunities for future research, particularly in building an understanding of competency in UX based on this preliminary framing of early UX practice."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 59s "}}
{"id": "dfba01c5-a632-4394-8607-9a32f20e526c", "displayed_text": "Title: Modeling geometric structure and illumination variation of a scene from real images\n\nAbstract: We present in this paper a system which automatically builds, from real images, a scene model containing both 3D geometric information of the scene structure and its photometric information under various illumination conditions. The geometric structure is recovered from images taken from distinct viewpoints. Structure-from-motion and correlation-based stereo techniques are used to match pixels between images of different viewpoints and to reconstruct the scene in 3D space. The photometric property is extracted from images taken under different illumination conditions (orientation, position and intensity of the light sources). This is achieved by computing a low-dimensional linear space of the spatio-illumination volume, and is represented by a set of basis images. The model that has been built can be used to create realistic renderings from different viewpoints and illumination conditions. Applications include object recognition, virtual reality and product advertisement.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The author presents a system that automatically builds, from real images, a scene model containing both 3D geometric information of the scene structure and its photometric information under various illumination conditions. The geometric structure is recovered from images taken from distinct viewpoints and processed by matching pixels between images of different viewpoints, and the photometric property is extracted from images taken under different illumination conditions and processed by computing a low-dimensional linear space of the spatio-illumination volume.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "Applications include object recognition, virtual reality and product advertisement."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 2s "}}
{"id": "dfba01c5-a632-4394-8607-9a32f20e526c", "displayed_text": "Title: Modeling geometric structure and illumination variation of a scene from real images\n\nAbstract: We present in this paper a system which automatically builds, from real images, a scene model containing both 3D geometric information of the scene structure and its photometric information under various illumination conditions. The geometric structure is recovered from images taken from distinct viewpoints. Structure-from-motion and correlation-based stereo techniques are used to match pixels between images of different viewpoints and to reconstruct the scene in 3D space. The photometric property is extracted from images taken under different illumination conditions (orientation, position and intensity of the light sources). This is achieved by computing a low-dimensional linear space of the spatio-illumination volume, and is represented by a set of basis images. The model that has been built can be used to create realistic renderings from different viewpoints and illumination conditions. Applications include object recognition, virtual reality and product advertisement.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors present a system which automatically builds, from real images, a scene model containing both 3D geometric information of the scene structure and its photometric information under various illumination conditions.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "The model that has been built can be used to create realistic renderings from different viewpoints and illumination conditions. Applications include object recognition, virtual reality and product advertisement."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "e10b883a-ded6-4b4e-9934-6daba2d2f2b2", "displayed_text": "Title: Semantic hashing using tags and topic modeling\n\nAbstract: It is an important research problem to design efficient and effective solutions for large scale similarity search. One popular strategy is to represent data examples as compact binary codes through semantic hashing, which has produced promising results with fast search speed and low storage cost. Many existing semantic hashing methods generate binary codes for documents by modeling document relationships based on similarity in a keyword feature space. Two major limitations in existing methods are: (1) Tag information is often associated with documents in many real world applications, but has not been fully exploited yet; (2) The similarity in keyword feature space does not fully reflect semantic relationships that go beyond keyword matching.   This paper proposes a novel hashing approach, Semantic Hashing using Tags and Topic Modeling (SHTTM), to incorporate both the tag information and the similarity information from probabilistic topic modeling. In particular, a unified framework is designed for ensuring hashing codes to be consistent with tag information by a formal latent factor model and preserving the document topic/semantic similarity that goes beyond keyword matching. An iterative coordinate descent procedure is proposed for learning the optimal hashing codes. An extensive set of empirical studies on four different datasets has been conducted to demonstrate the advantages of the proposed SHTTM approach against several other state-of-the-art semantic hashing techniques. Furthermore, experimental results indicate that the modeling of tag information and utilizing topic modeling are beneficial for improving the effectiveness of hashing separately, while the combination of these two techniques in the unified framework obtains even better results.", "label_annotations": {"Multi-aspect Summary": {"Context": "Designing efficient and effective solutions for large scale similarity search is an important research problem, and one popular strategy is to represent data examples as compact binary codes through semantic hashing. Many existing semantic hashing methods generate binary codes for documents by modeling document relationships based on similarity in a keyword feature space, which has two major limitations.", "Key idea": "The authors propose a novel hashing approach, Semantic Hashing using Tags and Topic Modeling (SHTTM), to incorporate both the tag information and the similarity information from probabilistic topic modeling.", "Method": "The authors evaluate the proposed SHTTM on four different datasets and compare it with several other state-of-the-art semantic hashing techniques.", "Outcome": "An extensive set of empirical studies on four different datasets has been conducted to demonstrate the advantages of the proposed SHTTM approach against several other state-of-the-art semantic hashing techniques. Furthermore, experimental results indicate that the modeling of tag information and utilizing topic modeling are beneficial for improving the effectiveness of hashing separately, while the combination of these two techniques in the unified framework obtains even better results.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 30s "}}
{"id": "e10b883a-ded6-4b4e-9934-6daba2d2f2b2", "displayed_text": "Title: Semantic hashing using tags and topic modeling\n\nAbstract: It is an important research problem to design efficient and effective solutions for large scale similarity search. One popular strategy is to represent data examples as compact binary codes through semantic hashing, which has produced promising results with fast search speed and low storage cost. Many existing semantic hashing methods generate binary codes for documents by modeling document relationships based on similarity in a keyword feature space. Two major limitations in existing methods are: (1) Tag information is often associated with documents in many real world applications, but has not been fully exploited yet; (2) The similarity in keyword feature space does not fully reflect semantic relationships that go beyond keyword matching.   This paper proposes a novel hashing approach, Semantic Hashing using Tags and Topic Modeling (SHTTM), to incorporate both the tag information and the similarity information from probabilistic topic modeling. In particular, a unified framework is designed for ensuring hashing codes to be consistent with tag information by a formal latent factor model and preserving the document topic/semantic similarity that goes beyond keyword matching. An iterative coordinate descent procedure is proposed for learning the optimal hashing codes. An extensive set of empirical studies on four different datasets has been conducted to demonstrate the advantages of the proposed SHTTM approach against several other state-of-the-art semantic hashing techniques. Furthermore, experimental results indicate that the modeling of tag information and utilizing topic modeling are beneficial for improving the effectiveness of hashing separately, while the combination of these two techniques in the unified framework obtains even better results.", "label_annotations": {"Multi-aspect Summary": {"Context": "There are two major limitations in existing semantic hashing methods: (1) Tag information is often associated with documents in many real world applications, but has not been fully exploited yet; (2) The similarity in keyword feature space does not fully reflect semantic relationships that go beyond keyword matching.", "Key idea": "The authors proposes a novel hashing approach, Semantic Hashing using Tags and Topic Modeling (SHTTM), to incorporate both the tag information and the similarity information from probabilistic topic modeling. ", "Method": "The authors conduct an extensive set of empirical studies on four different datasets to evaluate the proposed SHTTM approach against several other state-of-the-art semantic hashing techniques.", "Outcome": "Experimental results indicate that the modeling of tag information and utilizing topic modeling are beneficial for improving the effectiveness of hashing separately, while the combination of these two techniques in the unified framework obtains even better results.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 13s "}}
{"id": "e1a191db-6463-4be9-883d-dfc7fb05c5f4", "displayed_text": "Title: Dependency Parsing with Bounded Block Degree and Well-nestedness via Lagrangian Relaxation and Branch-and-Bound\n\nAbstract: We present a novel dependency parsing method which enforces two structural properties on dependency trees: bounded block degree and well-nestedness. These properties are useful to better represent the set of admissible dependency structures in treebanks and connect dependency parsing to context-sensitive grammatical formalisms. We cast this problem as an Integer Linear Program that we solve with Lagrangian Relaxation from which we derive a heuristic and an exact method based on a Branch-and-Bound search. Experimentally, we see that these methods are efficient and competitive compared to a baseline unconstrained parser, while enforcing structural properties in all cases.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors present a novel dependency parsing method which enforces two structural properties on dependency trees: bounded block degree and well-nestedness.", "Method": "N/A", "Outcome": "The authors show that the proposed methods are efficient and competitive compared to a baseline unconstrained parser, while enforcing structural properties in all cases.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 58s "}}
{"id": "e1a191db-6463-4be9-883d-dfc7fb05c5f4", "displayed_text": "Title: Dependency Parsing with Bounded Block Degree and Well-nestedness via Lagrangian Relaxation and Branch-and-Bound\n\nAbstract: We present a novel dependency parsing method which enforces two structural properties on dependency trees: bounded block degree and well-nestedness. These properties are useful to better represent the set of admissible dependency structures in treebanks and connect dependency parsing to context-sensitive grammatical formalisms. We cast this problem as an Integer Linear Program that we solve with Lagrangian Relaxation from which we derive a heuristic and an exact method based on a Branch-and-Bound search. Experimentally, we see that these methods are efficient and competitive compared to a baseline unconstrained parser, while enforcing structural properties in all cases.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors propose a dependency parsing method that enforces two structural properties, bounded block degree and well-nestedness on dependency trees. The problem is treated as an Integer Linear Program and solved with Lagrangian Relaxation.", "Method": "The authors compare the proposed dependency parsing method with a baseline unconstrained parser.", "Outcome": "The proposed methods are efficient and competitive compared to a baseline unconstrained parser, while enforcing structural properties in all cases.", "Future Impact": "The two enforced structural properties of the proposed method are useful to better represent the set of admissible dependency structures in treebanks and connect dependency parsing to context-sensitive grammatical formalisms."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 12m 10s "}}
{"id": "e2466d98-98f9-41a8-8a38-e862adc2ed47", "displayed_text": "Title: A conceptual model for IT Governance: A case study research\n\nAbstract: The purpose of this paper is to explore the importance of Information Technology (IT) Governance models for public organizations and presenting an IT Governance model that can be adopted by both practitioners and researchers. A review of the literature in IT Governance has been initiated to shape the intended theoretical background of this study. The systematic literature review formalizes a richer context for the IT Governance concept. An empirical survey, using a questionnaire based on COBIT 4.1 maturity model used to investigate IT Governance practice in multiple case studies from Kingdom of Bahrain. This method enabled the researcher to gain insights to evaluate IT Governance practices. The results of this research will enable public sector organizations to adopt an IT Governance model in a simple and dynamic manner. The model provides a basic structure of a concept; for instance, this allows organizations to gain a better perspective on IT Governance processes and provides a clear focus for decision-making attention. IT Governance model also forms as a basis for further research in IT Governance adoption models and bridges the gap between conceptual frameworks, real life and functioning governance.", "label_annotations": {"Multi-aspect Summary": {"Context": "Information Technology (IT) Governance models for public organizations is important.", "Key idea": "The author presents an IT Governance model that is constructed from review of literature and an empirical survey that uses a questionnaire based on COBIT 4.1 maturity model.", "Method": "N/A", "Outcome": "The model provides a basic structure of a concept; for instance, this allows organizations to gain a better perspective on IT Governance processes and provides a clear focus for decision-making attention.", "Future Impact": "The results of this research will enable public sector organizations to adopt an IT Governance model in a simple and dynamic manner. IT Governance model also forms as a basis for further research in IT Governance adoption models and bridges the gap between conceptual frameworks, real life and functioning governance."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 29s "}}
{"id": "e2466d98-98f9-41a8-8a38-e862adc2ed47", "displayed_text": "Title: A conceptual model for IT Governance: A case study research\n\nAbstract: The purpose of this paper is to explore the importance of Information Technology (IT) Governance models for public organizations and presenting an IT Governance model that can be adopted by both practitioners and researchers. A review of the literature in IT Governance has been initiated to shape the intended theoretical background of this study. The systematic literature review formalizes a richer context for the IT Governance concept. An empirical survey, using a questionnaire based on COBIT 4.1 maturity model used to investigate IT Governance practice in multiple case studies from Kingdom of Bahrain. This method enabled the researcher to gain insights to evaluate IT Governance practices. The results of this research will enable public sector organizations to adopt an IT Governance model in a simple and dynamic manner. The model provides a basic structure of a concept; for instance, this allows organizations to gain a better perspective on IT Governance processes and provides a clear focus for decision-making attention. IT Governance model also forms as a basis for further research in IT Governance adoption models and bridges the gap between conceptual frameworks, real life and functioning governance.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors explore the importance of Information Technology (IT) Governance models for public organizations and presenting an IT Governance model that can be adopted by both practitioners and researchers.", "Method": "The authors initiate a systematic literature review.", "Outcome": "N/A", "Future Impact": "The authors show that IT Governance model allows organizations to gain a better perspective on IT Governance processes and provides a clear focus for decision-making attention and forms as a basis for further research in IT Governance adoption models and bridges the gap between conceptual frameworks, real life and functioning governance."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 28m 33s "}}
{"id": "e58b9947-7a3a-414d-a0e8-d6cf02ed7127", "displayed_text": "Title: Exploring recommendations in internet of things\n\nAbstract: With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web-based services, physical things are becoming an integral part of the emerging ubiquitous Web. In this paper, we focus on the things recommendation problem in Internet of Things (IoT). In particular, we propose a unified probabilistic based framework by fusing information across relationships between users (i.e., usersu0027social network) and things (i.e., things correlations) to make more accurate recommendations. The proposed approach not only inherits the advantages of the matrix factorization, but also exploits the merits of social relationships and thing-thing correlations. We validate our approach based on an Internet of Things platform and the experimental results demonstrate its feasibility and effectiveness.", "label_annotations": {"Multi-aspect Summary": {"Context": "With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web-based services, physical things are becoming an integral part of the emerging ubiquitous Web.", "Key idea": "Authors propose a unified probabilistic based framework by fusing information across relationships between users (i.e., social network) and things (i.e., things correlations) to make more accurate recommendations. The proposed approach not only inherits the advantages of the matrix factorization, but also exploits the merits of social relationships and thing-thing correlations", "Method": "Authors design experiments on an Internet of Things platform.", "Outcome": "Authors validate their approach based on an Internet of Things platform and the experimental results demonstrate its feasibility and effectiveness.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 21s "}}
{"id": "e58b9947-7a3a-414d-a0e8-d6cf02ed7127", "displayed_text": "Title: Exploring recommendations in internet of things\n\nAbstract: With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web-based services, physical things are becoming an integral part of the emerging ubiquitous Web. In this paper, we focus on the things recommendation problem in Internet of Things (IoT). In particular, we propose a unified probabilistic based framework by fusing information across relationships between users (i.e., usersu0027social network) and things (i.e., things correlations) to make more accurate recommendations. The proposed approach not only inherits the advantages of the matrix factorization, but also exploits the merits of social relationships and thing-thing correlations. We validate our approach based on an Internet of Things platform and the experimental results demonstrate its feasibility and effectiveness.", "label_annotations": {"Multi-aspect Summary": {"Context": "With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web-based services, physical things are becoming an integral part of the emerging ubiquitous Web.", "Key idea": "The authors propose a unified probabilistic based framework by fusing information across relationships between users (i.e., users' social network) and things (i.e., things correlations) to make more accurate recommendations.", "Method": "The authors validate their approach based on an Internet of Things platform.", "Outcome": "The proposed approach not only inherits the advantages of the matrix factorization, but also exploits the merits of social relationships and thing-thing correlations. The experimental results demonstrate its feasibility and effectiveness.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 4s "}}
{"id": "e827ee51-aafd-4f3b-99ca-60a5e289a555", "displayed_text": "Title: Recognizing textual entailment via atomic propositions\n\nAbstract: This paper describes Macquarie Universityu0027s Centre for Language Technology contribution to the PASCAL 2005 Recognizing Textual Entailment challenge. Our main aim was to test the practicability of a purely logical approach. For this, atomic propositions were extracted from both the text and the entailment hypothesis and they were expressed in a custom logical notation. The text entails the hypothesis if every proposition of the hypothesis is entailed by some proposition in the text. To extract the propositions and encode them into a logical notation the system uses the output of Link Parser. To detect the independent entailment relations the system relies on the use of Otter and WordNet.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The author aims to test the practicability of a purely logical approach by extracting atomic propositions from both the text and the entailment hypothesis. The system uses the output of Link Parser to extract the propositions and encode them into a logical notation and uses Otter and WordNet to detect the independent entailment relations.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 16s "}}
{"id": "e827ee51-aafd-4f3b-99ca-60a5e289a555", "displayed_text": "Title: Recognizing textual entailment via atomic propositions\n\nAbstract: This paper describes Macquarie Universityu0027s Centre for Language Technology contribution to the PASCAL 2005 Recognizing Textual Entailment challenge. Our main aim was to test the practicability of a purely logical approach. For this, atomic propositions were extracted from both the text and the entailment hypothesis and they were expressed in a custom logical notation. The text entails the hypothesis if every proposition of the hypothesis is entailed by some proposition in the text. To extract the propositions and encode them into a logical notation the system uses the output of Link Parser. To detect the independent entailment relations the system relies on the use of Otter and WordNet.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors test the practicability of a purely logical approach in the PASCAL 2005 Recognizing Textual Entailment challenge.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 41s "}}
{"id": "ea414544-c89c-4039-8227-23b11e9a1239", "displayed_text": "Title: A Practically Unsupervised Learning Method to Identify Single-Snippet Answers to Definition Questions on the Web\n\nAbstract: We present a practically unsupervised learning method to produce single-snippet answers to definition questions in question answering systems that supplement Web search engines. The method exploits on-line encyclopedias and dictionaries to generate automatically an arbitrarily large number of positive and negative definition examples, which are then used to train an SVM to separate the two classes. We show experimentally that the proposed method is viable, that it outperforms the alternative of training the system on questions and news articles from TREC, and that it helps the search engine handle definition questions significantly better.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors present a practically unsupervised learning method to produce single-snippet answers to definition questions in question answering systems that supplement Web search engines. ", "Method": "The authors conduct experiments to evaluate new practically unsupervised learning method and compare it with the alternative of training the system on questions and news articles from TREC.", "Outcome": "Experiments prove that new practically unsupervised learning method is viable and it outperforms the alternative of training the system on questions and news articles from TREC, and that it helps the search engine handle definition questions significantly better.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 3s "}}
{"id": "ea414544-c89c-4039-8227-23b11e9a1239", "displayed_text": "Title: A Practically Unsupervised Learning Method to Identify Single-Snippet Answers to Definition Questions on the Web\n\nAbstract: We present a practically unsupervised learning method to produce single-snippet answers to definition questions in question answering systems that supplement Web search engines. The method exploits on-line encyclopedias and dictionaries to generate automatically an arbitrarily large number of positive and negative definition examples, which are then used to train an SVM to separate the two classes. We show experimentally that the proposed method is viable, that it outperforms the alternative of training the system on questions and news articles from TREC, and that it helps the search engine handle definition questions significantly better.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors present a practically unsupervised learning method to produce single-snippet answers to definition questions in question answering systems that supplement Web search engines.", "Method": "The authors compare the proposed methods with an alternative of training the question answering system on questions and news articles from TREC", "Outcome": "The authors show experimentally that the proposed method is viable, that it outperforms the alternative of training the system on questions and news articles from TREC, and that it helps the search engine handle definition questions significantly better.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 16s "}}
{"id": "eb15ebe7-aa58-4a98-8f9e-939967c6359f", "displayed_text": "Title: Synchronization of Group-labelled Multi-graphs.\n\nAbstract: Synchronization refers to the problem of inferring the unknown values attached to vertices of a graph where edges are labelled with the ratio of the incident vertices, and labels belong to a group. This paper addresses the synchronization problem on multi-graphs, that are graphs with more than one edge connecting the same pair of nodes. The problem naturally arises when multiple measures are available to model the relationship between two vertices. This happens when different sensors measure the same quantity, or when the original graph is partitioned into sub-graphs that are solved independently. In this case, the relationships among sub-graphs give rise to multi-edges and the problem can be traced back to a multi-graph synchronization. The baseline solution reduces multi-graphs to simple ones by averaging their multi-edges, however this approach falls short because: i) averaging is well defined only for some groups and ii) the resulting estimator is less precise and accurate, as we prove empirically. Specifically, we present MultiSynch, a synchronization algorithm for multi-graphs that is based on a principled constrained eigenvalue optimization. MultiSynch is a general solution that can cope with any linear group and we show to be profitably usable both on synthetic and real problems.", "label_annotations": {"Multi-aspect Summary": {"Context": "Many papers try to address the synchronization problem on multi-graphs, that are graphs with more than one edge connecting the same pair of nodes.  The baseline solution reduces multi-graphs to simple ones by averaging their multi-edges, however this approach falls short.", "Key idea": "The author present a synchronization algorithm, named MultiSynch, for multi-graphs that is based on a principled constrained eigenvalue optimization.", "Method": "The authors apply MultiSynch algorithm on any linear group and multi-graphs synthetic problems and real problems.", "Outcome": "The experiment proves that MultiSynch is a general solution that can cope with any linear group and this  algorithm may be profitably usable both on synthetic and real problems.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "eb15ebe7-aa58-4a98-8f9e-939967c6359f", "displayed_text": "Title: Synchronization of Group-labelled Multi-graphs.\n\nAbstract: Synchronization refers to the problem of inferring the unknown values attached to vertices of a graph where edges are labelled with the ratio of the incident vertices, and labels belong to a group. This paper addresses the synchronization problem on multi-graphs, that are graphs with more than one edge connecting the same pair of nodes. The problem naturally arises when multiple measures are available to model the relationship between two vertices. This happens when different sensors measure the same quantity, or when the original graph is partitioned into sub-graphs that are solved independently. In this case, the relationships among sub-graphs give rise to multi-edges and the problem can be traced back to a multi-graph synchronization. The baseline solution reduces multi-graphs to simple ones by averaging their multi-edges, however this approach falls short because: i) averaging is well defined only for some groups and ii) the resulting estimator is less precise and accurate, as we prove empirically. Specifically, we present MultiSynch, a synchronization algorithm for multi-graphs that is based on a principled constrained eigenvalue optimization. MultiSynch is a general solution that can cope with any linear group and we show to be profitably usable both on synthetic and real problems.", "label_annotations": {"Multi-aspect Summary": {"Context": "Synchronization refers to the problem of inferring the unknown values attached to vertices of a graph where edges are labelled with the ratio of the incident vertices, and labels belong to a group. ", "Key idea": "Authors show that baseline solution reduces multi-graphs to simple ones by averaging their multi-edges, however this approach falls short because: i) averaging is well defined only for some groups and ii) the resulting estimator is less precise and accurate.\r\nAuthors present MultiSynch, a synchronization algorithm for multi-graphs that is based on a principled constrained eigenvalue optimization.\r\n", "Method": "Authors conduct experiments on synthetic and real problems.", "Outcome": "Authors argue that MultiSynch is a general solution that can cope with any linear group and is show to be profitably usable both on synthetic and real problems.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 2s "}}
{"id": "f23bdd28-b2d6-4a42-a56c-c9774f6451b5", "displayed_text": "Title: Learning Algorithm for Tracking Hypersonic Targets in Near Space\n\nAbstract: With the development of hypersonic vehicles in near space such as X-51A, HTV-2 and so on, tracking for them is becoming a new task and hotspot. In this paper, a learning tracking algorithm is introduced for hypersonic targets, especially for the sliding jump maneuver. Firstly the algorithm uses the Sine model, which makes the tracking model more close to the particular maneuver, next two Sine models different in angular velocity are used into IMM algorithm, and it learns the target tracking error characteristics to adjust the sampling rate adaptively. The algorithm is compared with the single accurate model algorithm and general IMM algorithms with fixed sampling rate. Through simulation experiments it is proved that the algorithm in this paper can improve the tracking accuracy effectively.", "label_annotations": {"Multi-aspect Summary": {"Context": "Tracking for hypersonic vehicles in near space is becoming a new task and hotspot.", "Key idea": "The authors introduce a learning tracking algorithm for hypersonic targets, especially for the sliding jump maneuver.", "Method": "The authors compare the proposed algorithm with the single accurate model algorithm and general IMM algorithms with fixed sampling rate on simulation experiments.", "Outcome": "Through simulation experiments it is proved that the algorithm in this paper can improve the tracking accuracy effectively.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 20s "}}
{"id": "f23bdd28-b2d6-4a42-a56c-c9774f6451b5", "displayed_text": "Title: Learning Algorithm for Tracking Hypersonic Targets in Near Space\n\nAbstract: With the development of hypersonic vehicles in near space such as X-51A, HTV-2 and so on, tracking for them is becoming a new task and hotspot. In this paper, a learning tracking algorithm is introduced for hypersonic targets, especially for the sliding jump maneuver. Firstly the algorithm uses the Sine model, which makes the tracking model more close to the particular maneuver, next two Sine models different in angular velocity are used into IMM algorithm, and it learns the target tracking error characteristics to adjust the sampling rate adaptively. The algorithm is compared with the single accurate model algorithm and general IMM algorithms with fixed sampling rate. Through simulation experiments it is proved that the algorithm in this paper can improve the tracking accuracy effectively.", "label_annotations": {"Multi-aspect Summary": {"Context": "Tracking hypersonic vehicles in near space such as X-51A, HTV-2 is becoming a new task and hotspot.", "Key idea": "The authors introduce a learning tracking algorithm for hypersonic targets, especially for the sliding jump maneuver. ", "Method": "The authors conduct simulation experiments and compare new algorithm with the single accurate model algorithm and general IMM algorithms with fixed sampling rate.", "Outcome": "Simulation experiments prove that the algorithm in this paper can improve the tracking accuracy effectively.\r\n", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 13s "}}
{"id": "f2c9f8c3-f9d1-4cae-b7ae-a919ada1daaf", "displayed_text": "Title: Targeted Mismatch Adversarial Attack: Query With a Flower to Retrieve the Tower\n\nAbstract: Access to online visual search engines implies sharing of private user content -- the query images. We introduce the concept of targeted mismatch attack for deep learning based retrieval systems to generate an adversarial image to conceal the query image. The generated image looks nothing like the user intended query, but leads to identical or very similar retrieval results. Transferring attacks to fully unseen networks is challenging. We show successful attacks to partially unknown systems, by designing various loss functions for the adversarial image construction. These include loss functions, for example, for unknown global pooling operation or unknown input resolution by the retrieval system. We evaluate the attacks on standard retrieval benchmarks and compare the results retrieved with the original and adversarial image.", "label_annotations": {"Multi-aspect Summary": {"Context": "Access to online visual search engines implies sharing of private user content -- the query images.", "Key idea": "The authors introduce the concept of targeted mismatch attack for deep learning based retrieval systems to generate an adversarial image to conceal the query image.", "Method": "The authors evaluate the attacks on standard retrieval benchmarks and compare the results retrieved with the original and adversarial image.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 6m 33s "}}
{"id": "f2c9f8c3-f9d1-4cae-b7ae-a919ada1daaf", "displayed_text": "Title: Targeted Mismatch Adversarial Attack: Query With a Flower to Retrieve the Tower\n\nAbstract: Access to online visual search engines implies sharing of private user content -- the query images. We introduce the concept of targeted mismatch attack for deep learning based retrieval systems to generate an adversarial image to conceal the query image. The generated image looks nothing like the user intended query, but leads to identical or very similar retrieval results. Transferring attacks to fully unseen networks is challenging. We show successful attacks to partially unknown systems, by designing various loss functions for the adversarial image construction. These include loss functions, for example, for unknown global pooling operation or unknown input resolution by the retrieval system. We evaluate the attacks on standard retrieval benchmarks and compare the results retrieved with the original and adversarial image.", "label_annotations": {"Multi-aspect Summary": {"Context": "Transferring attacks to fully unseen networks is challenging. ", "Key idea": "The authors introduce the concept of targeted mismatch attack for deep learning based retrieval systems to generate an adversarial image to conceal the query image and try to design various loss functions for the adversarial image construction.", "Method": "The authors evaluate the attacks on standard retrieval benchmarks and compare the results retrieved with the original and adversarial image.\r\n", "Outcome": "After designing various loss functions for the adversarial image construction, attacks to partially unknown systems are successful.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 43s "}}
{"id": "f32d6bc3-d75e-4e84-8bfa-3c83578281dc", "displayed_text": "Title: Monitoring and evaluation of information systems via transaction log analysis\n\nAbstract: Transaction log analysis represents a powerful methodology which allows examination of both user commands and system responses when conducting an online information search. Machine-readable transaction log tapes from online catalogs are obtained and subsequently analyzed using stochastic pattern developments within parsed user sessions, mathematical models utilizing Markov chain analysis and the development of state transition probability matrices, which illustrate the probability of proceeding from one user or system state to another state. The objective of monitoring information systems and using transaction log analysis is to discover the extent to which systems are used and to determine the actual user patterns when conducting an information search. This in turn can aid in the evaluation of such systems and assist in the improvement of existing and future systems. Such analysis can assist in system design, while the predictive power of the methodology may allow real-time aids to be developed.As examples of the use of the methodology, patron use and system response patterns from several online public catalogs have been obtained by transaction log tapes. This paper presents an overview of the methodology, results obtained, and efforts that are being conducted within OCLCu0027s Office of Research.", "label_annotations": {"Multi-aspect Summary": {"Context": "Transaction log analysis is powerful in examining both user commands and system responses when conducting an online information search. Machine-readable transaction log tapes from online catalogs are obtained and subsequently analyzed using various methods.", "Key idea": "The author proposed monitoring information systems and using transaction log analysis to discover the extent to which systems are used and to determine the actual user patterns when conducting an information search. This in turn can aid in the evaluation of such systems and assist in the improvement of existing and future systems.", "Method": "The author uses the proposed methodology to obtain patron use and system response patterns from several online public catalogs.", "Outcome": "N/A", "Future Impact": "Such analysis can assist in system design, while the predictive power of the methodology may allow real-time aids to be developed."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 34s "}}
{"id": "f32d6bc3-d75e-4e84-8bfa-3c83578281dc", "displayed_text": "Title: Monitoring and evaluation of information systems via transaction log analysis\n\nAbstract: Transaction log analysis represents a powerful methodology which allows examination of both user commands and system responses when conducting an online information search. Machine-readable transaction log tapes from online catalogs are obtained and subsequently analyzed using stochastic pattern developments within parsed user sessions, mathematical models utilizing Markov chain analysis and the development of state transition probability matrices, which illustrate the probability of proceeding from one user or system state to another state. The objective of monitoring information systems and using transaction log analysis is to discover the extent to which systems are used and to determine the actual user patterns when conducting an information search. This in turn can aid in the evaluation of such systems and assist in the improvement of existing and future systems. Such analysis can assist in system design, while the predictive power of the methodology may allow real-time aids to be developed.As examples of the use of the methodology, patron use and system response patterns from several online public catalogs have been obtained by transaction log tapes. This paper presents an overview of the methodology, results obtained, and efforts that are being conducted within OCLCu0027s Office of Research.", "label_annotations": {"Multi-aspect Summary": {"Context": "Transaction log analysis represents a powerful methodology which allows examination of both user commands and system responses when conducting an online information search.", "Key idea": "The authors propose a way that obtain machine-readable transaction log tapes from online catalogs and subsequently analyze using stochastic pattern developments within parsed user sessions, mathematical models utilizing Markov chain analysis and the development of state transition probability matrices.", "Method": "The authors apply this way that obtain machine-readable transaction log tapes from online catalogs on several online public catalogs.", "Outcome": "The probability of proceeding from one user or system state to another state has been proved.\r\nUsing this methodology, patron use and system response patterns from several online public catalogs can be obtained by transaction log tapes.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 8m 19s "}}
{"id": "f32e53d5-c7f4-407e-a5cb-26fef230b5fd", "displayed_text": "Title: Dynamic Bayesian Logistic Matrix Factorization for Recommendation with Implicit Feedback\n\nAbstract: Matrix factorization has been widely adopted for recommendation by learning latent embeddings of users and items from observed user-item interaction data. However, previous methods usually assume the learned embeddings are static or homogeneously evolving with the same diffusion rate. This is not valid in most scenarios, where usersu0027 preferences and item attributes heterogeneously drift over time. To remedy this issue, we have proposed a novel dynamic matrix factorization model, named Dynamic Bayesian Logistic Matrix Factorization (DBLMF), which aims to learn heterogeneous user and item embeddings that are drifting with inconsistent diffusion rates. More specifically, DBLMF extends logistic matrix factorization to model the probability a user would like to interact with an item at a given timestamp, and a diffusion process to connect latent embeddings over time. In addition, an efficient Bayesian inference algorithm has also been proposed to make DBLMF scalable on large datasets. The effectiveness of the proposed method has been demonstrated by extensive experiments on real datasets, compared with the state-of-the-art methods.", "label_annotations": {"Multi-aspect Summary": {"Context": "Matrix factorization has been widely adopted for recommendation by learning latent embeddings of users and items from observed user-item interaction data. Previous methods usually wrongly assume the learned embeddings are static or homogeneously evolving with the same diffusion rate, while users' preferences and item attributes heterogeneously drift over time.\r\n", "Key idea": "The author proposes a novel dynamic matrix factorization model that learns heterogeneous user and item embeddings that are drifting with inconsistent diffusion rates by extending logistic matrix factorization to model the probability a user would like to interact with an item at a given timestamp, and a diffusion process to connect latent embeddings over time. An efficient Bayesian inference algorithm has also been proposed to make the model scalable on large datasets.", "Method": "The author compares the effectiveness of the proposed method with the state-of-the-art methods on extensive experiments on real datasets.", "Outcome": "The effectiveness of the proposed method has been demonstrated by extensive experiments on real datasets, compared with the state-of-the-art methods.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 7m 18s "}}
{"id": "f32e53d5-c7f4-407e-a5cb-26fef230b5fd", "displayed_text": "Title: Dynamic Bayesian Logistic Matrix Factorization for Recommendation with Implicit Feedback\n\nAbstract: Matrix factorization has been widely adopted for recommendation by learning latent embeddings of users and items from observed user-item interaction data. However, previous methods usually assume the learned embeddings are static or homogeneously evolving with the same diffusion rate. This is not valid in most scenarios, where usersu0027 preferences and item attributes heterogeneously drift over time. To remedy this issue, we have proposed a novel dynamic matrix factorization model, named Dynamic Bayesian Logistic Matrix Factorization (DBLMF), which aims to learn heterogeneous user and item embeddings that are drifting with inconsistent diffusion rates. More specifically, DBLMF extends logistic matrix factorization to model the probability a user would like to interact with an item at a given timestamp, and a diffusion process to connect latent embeddings over time. In addition, an efficient Bayesian inference algorithm has also been proposed to make DBLMF scalable on large datasets. The effectiveness of the proposed method has been demonstrated by extensive experiments on real datasets, compared with the state-of-the-art methods.", "label_annotations": {"Multi-aspect Summary": {"Context": "Since users preferences and item attributes heterogeneously drift over time, previous methods, which usually assume the learned embeddings are static or homogeneously evolving with the same diffusion rate, is not valid in most scenarios.", "Key idea": "The authors proposed a novel dynamic matrix factorization model, named Dynamic Bayesian Logistic Matrix Factorization (DBLMF), which aims to learn heterogeneous user and item embeddings that are drifting with inconsistent diffusion rates. And the authors proposed an efficient Bayesian inference algorithm.", "Method": "The authors conduct extensive experiments on real datasets to evaluate effectiveness of Dynamic Bayesian Logistic Matrix Factorization (DBLMF) and other state-of-the-art methods.", "Outcome": "With an efficient Bayesian inference algorithm, Dynamic Bayesian Logistic Matrix Factorization (DBLMF) method is scalable on large dataset. Compared with other state-of-the-art methods, DBLMF is more effective on real datasets.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 1s "}}
{"id": "f3cef657-1887-42a7-9e11-8fdf54f8fa90", "displayed_text": "Title: Kernel Continual Learning\n\nAbstract: This paper introduces kernel continual learning, a simple but effective variant of continual learning that leverages the non-parametric nature of kernel methods to tackle catastrophic forgetting. We deploy an episodic memory unit that stores a subset of samples for each task to learn task-specific classifiers based on kernel ridge regression. This does not require memory replay and systematically avoids task interference in the classifiers. We further introduce variational random features to learn a data-driven kernel for each task. To do so, we formulate kernel continual learning as a variational inference problem, where a random Fourier basis is incorporated as the latent variable. The variational posterior distribution over the random Fourier basis is inferred from the coreset of each task. In this way, we are able to generate more informative kernels specific to each task, and, more importantly, the coreset size can be reduced to achieve more compact memory, resulting in more efficient continual learning based on episodic memory. Extensive evaluation on four benchmarks demonstrates the effectiveness and promise of kernels for continual learning.", "label_annotations": {"Multi-aspect Summary": {"Context": "Catastrophic forgetting may be a significant challenge in continual learning.", "Key idea": "The authors introduce a simple but effective variant of continual learning named kernel continual learning that leverages the non-parametric nature of kernel methods to tackle catastrophic forgetting. This method not requires memory replay and systematically avoids task interference in the classifiers.", "Method": "The authors apply kernel continual learning on different tasks and evaluate results on four benchmarks.", "Outcome": "Using kernel continual learning, more informative kernels specific to each task can be generated, and the coreset size can be reduced to achieve more compact memory, resulting in more efficient continual learning based on episodic memory. Kernel continual learning demonstrates the effectiveness and promise of kernels for continual learning on four benchmarks.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 2s "}}
{"id": "f3cef657-1887-42a7-9e11-8fdf54f8fa90", "displayed_text": "Title: Kernel Continual Learning\n\nAbstract: This paper introduces kernel continual learning, a simple but effective variant of continual learning that leverages the non-parametric nature of kernel methods to tackle catastrophic forgetting. We deploy an episodic memory unit that stores a subset of samples for each task to learn task-specific classifiers based on kernel ridge regression. This does not require memory replay and systematically avoids task interference in the classifiers. We further introduce variational random features to learn a data-driven kernel for each task. To do so, we formulate kernel continual learning as a variational inference problem, where a random Fourier basis is incorporated as the latent variable. The variational posterior distribution over the random Fourier basis is inferred from the coreset of each task. In this way, we are able to generate more informative kernels specific to each task, and, more importantly, the coreset size can be reduced to achieve more compact memory, resulting in more efficient continual learning based on episodic memory. Extensive evaluation on four benchmarks demonstrates the effectiveness and promise of kernels for continual learning.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors introduce kernel continual learning, a simple but effective variant of continual learning that leverages the non-parametric nature of kernel methods to tackle catastrophic forgetting. They further introduce variational random features to learn a data-driven kernel for each task.", "Method": "The authors evaluated their proposed methods on four benchmarks.", "Outcome": "Extensive evaluation on four benchmarks demonstrates the effectiveness and promise of kernels for continual learning.", "Future Impact": "The proposed method allows generating more informative kernels specific to each task, and, more importantly, the coreset size can be reduced to achieve more compact memory, resulting in more efficient continual learning based on episodic memory."}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 53s "}}
{"id": "f41bfd75-a6b0-44d7-a842-b117797ebd7e", "displayed_text": "Title: Studying Feature Generation from Various Data Representations for Answer Extraction\n\nAbstract: In this paper, we study how to generate features from various data representations, such as surface texts and parse trees, for answer extraction. Besides the features generated from the surface texts, we mainly discuss the feature generation in the parse trees. We propose and compare three methods, including feature vector, string kernel and tree kernel, to represent the syntactic features in Support Vector Machines. The experiment on the TREC question answering task shows that the features generated from the more structured data representations significantly improve the performance based on the features generated from the surface texts. Furthermore, the contribution of the individual feature will be discussed in detail.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "Authors propose and compare three methods, including feature vector, string kernel and tree kernel, to represent the syntactic features in Support Vector Machines.", "Method": "Authors conduct experiments on the TREC question answering task.", "Outcome": "The experiment on the TREC question answering task shows that the features generated from the more structured data representations significantly improve the performance based on the features generated from the surface texts. Authors also discuss the contribution of the individual feature.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 26s "}}
{"id": "f41bfd75-a6b0-44d7-a842-b117797ebd7e", "displayed_text": "Title: Studying Feature Generation from Various Data Representations for Answer Extraction\n\nAbstract: In this paper, we study how to generate features from various data representations, such as surface texts and parse trees, for answer extraction. Besides the features generated from the surface texts, we mainly discuss the feature generation in the parse trees. We propose and compare three methods, including feature vector, string kernel and tree kernel, to represent the syntactic features in Support Vector Machines. The experiment on the TREC question answering task shows that the features generated from the more structured data representations significantly improve the performance based on the features generated from the surface texts. Furthermore, the contribution of the individual feature will be discussed in detail.", "label_annotations": {"Multi-aspect Summary": {"Context": "N/A", "Key idea": "The authors study how to generate features from various data representations, such as surface texts and parse trees, for answer extraction.\r\nThe authors propose and compare three methods, including feature vector, string kernel and tree kernel, to represent the syntactic features in Support Vector Machines. ", "Method": "The authors compares the performance of three feature representation methods (feature vector, string kernel, tree kernel) for syntactic features derived from parse trees, and evaluates their effectiveness in a Support Vector Machine model on the TREC question answering task.\r\n\r\n", "Outcome": "The experiment result of the TREC question answering task demonstrates that features generated from the more structured data representations significantly improve the performance based on the features generated from the surface texts.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 3s "}}
{"id": "f4f1bbaf-c1a2-44d5-8305-27235fa69d62", "displayed_text": "Title: Modeling and Mining Feature-Rich Networks\n\nAbstract: In the field of web mining and web science, as well as data science and data mining there has been a lot of interest in the analysis of (social) networks. With the growing complexity of heterogeneous data, feature-rich networks have emerged as a powerful modeling approach: They capture data and knowledge at different scales from multiple heterogeneous data sources, and allow the mining and analysis from different perspectives. The challenge is to devise novel algorithms and tools for the analysis of such networks.\n\nThis tutorial provides a unified perspective on feature-rich networks, focusing on different modeling approaches, in particular multiplex and attributed networks. It outlines important principles, methods, tools and future research directions in this emerging field.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "In the field of web mining and web science, as well as data science and data mining there has been a lot of interest in the analysis of (social) networks. With the growing complexity of heterogeneous data, feature-rich networks have emerged as a powerful modeling approach, but the challenge is to devise novel algorithms and tools for the analysis of such networks.", "Key idea": "The authors provide a unified perspective on feature-rich networks, focusing on different modeling approaches, in particular multiplex and attributed networks. They also outline important principles, methods, tools and future research directions in this emerging field.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 59s "}}
{"id": "f4f1bbaf-c1a2-44d5-8305-27235fa69d62", "displayed_text": "Title: Modeling and Mining Feature-Rich Networks\n\nAbstract: In the field of web mining and web science, as well as data science and data mining there has been a lot of interest in the analysis of (social) networks. With the growing complexity of heterogeneous data, feature-rich networks have emerged as a powerful modeling approach: They capture data and knowledge at different scales from multiple heterogeneous data sources, and allow the mining and analysis from different perspectives. The challenge is to devise novel algorithms and tools for the analysis of such networks.\n\nThis tutorial provides a unified perspective on feature-rich networks, focusing on different modeling approaches, in particular multiplex and attributed networks. It outlines important principles, methods, tools and future research directions in this emerging field.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "It's challenging  to devise novel algorithms and tools for the analysis of  feature-rich networks.", "Key idea": "This tutorial provides a unified perspective on feature-rich networks, focusing on different modeling approaches, in particular multiplex and attributed networks.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 16s "}}
{"id": "f64fdfde-7e93-411b-865a-1e29d71c95b2", "displayed_text": "Title: Scalable training of hierarchical topic models\n\nAbstract: Large-scale topic models serve as basic tools for feature extraction and dimensionality reduction in many practical applications. As a natural extension of flat topic models, hierarchical topic models (HTMs) are able to learn topics of different levels of abstraction, which lead to deeper understanding and better generalization than their flat counterparts. However, existing scalable systems for flat topic models cannot handle HTMs, due to their complicated data structures such as trees and concurrent dynamically growing matrices, as well as their susceptibility to local optima.\r\n\r\nIn this paper, we study the hierarchical latent Dirichlet allocation (hLDA) model which is a powerful nonparametric Bayesian HTM. We propose an efficient partially collapsed Gibbs sampling algorithm for hLDA, as well as an initialization strategy to deal with local optima introduced by tree-structured models. We also identify new system challenges in building scalable systems for HTMs, and propose efficient data layout for vectorizing HTM as well as distributed data structures including dynamic matrices and trees. Empirical studies show that our system is 87 times more efficient than the previous open-source implementation for hLDA, and can scale to thousands of CPU cores. We demonstrate our scalability on a 131-million-document corpus with 28 billion tokens, which is 4--5 orders of magnitude larger than previously used corpus. Our distributed implementation can extract 1,722 topics from the corpus with 50 machines in just 7 hours.", "label_annotations": {"Multi-aspect Summary": {"Context": "Large-scale topic models serve as basic tools for feature extraction and dimensionality reduction and hierarchical topic models (HTMs) are able to learn topics of different levels of abstraction. However, existing scalable systems for flat topic models cannot handle HTMs, due to their complicated data structures.", "Key idea": "The authors propose an efficient partially collapsed Gibbs sampling algorithm for hLDA, as well as an initialization strategy to deal with local optima introduced by tree-structured models. They further identify new system challenges in building scalable systems for HTMs, and propose efficient data layout.", "Method": "The authors evaluate the proposed method on a 131-million-document corpus with 28 billion tokens, which is 4--5 orders of magnitude larger than previously used corpus.", "Outcome": "Empirical studies show that the proposed system is 87 times more efficient than the previous open-source implementation for hLDA, and can scale to thousands of CPU cores. The distributed implementation is also scalable on a 131-million-document corpus with 28 billion tokens, extracting 1,722 topics from the corpus with 50 machines in just 7 hours.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 3m 0s "}}
{"id": "f64fdfde-7e93-411b-865a-1e29d71c95b2", "displayed_text": "Title: Scalable training of hierarchical topic models\n\nAbstract: Large-scale topic models serve as basic tools for feature extraction and dimensionality reduction in many practical applications. As a natural extension of flat topic models, hierarchical topic models (HTMs) are able to learn topics of different levels of abstraction, which lead to deeper understanding and better generalization than their flat counterparts. However, existing scalable systems for flat topic models cannot handle HTMs, due to their complicated data structures such as trees and concurrent dynamically growing matrices, as well as their susceptibility to local optima.\r\n\r\nIn this paper, we study the hierarchical latent Dirichlet allocation (hLDA) model which is a powerful nonparametric Bayesian HTM. We propose an efficient partially collapsed Gibbs sampling algorithm for hLDA, as well as an initialization strategy to deal with local optima introduced by tree-structured models. We also identify new system challenges in building scalable systems for HTMs, and propose efficient data layout for vectorizing HTM as well as distributed data structures including dynamic matrices and trees. Empirical studies show that our system is 87 times more efficient than the previous open-source implementation for hLDA, and can scale to thousands of CPU cores. We demonstrate our scalability on a 131-million-document corpus with 28 billion tokens, which is 4--5 orders of magnitude larger than previously used corpus. Our distributed implementation can extract 1,722 topics from the corpus with 50 machines in just 7 hours.", "label_annotations": {"Multi-aspect Summary": {"Context": "Large-scale topic models serve as basic tools for feature extraction and dimensionality reduction in many practical applications. As a natural extension of flat topic models, hierarchical topic models (HTMs) are able to learn topics of different levels of abstraction, which lead to deeper understanding and better generalization than their flat counterparts. However, existing scalable systems for flat topic models cannot handle HTMs, due to their complicated data structures such as trees and concurrent dynamically growing matrices, as well as their susceptibility to local optima.", "Key idea": "Authors propose an efficient partially collapsed Gibbs sampling algorithm for hLDA, as well as an initialization strategy to deal with local optima introduced by tree-structured models. Authors also identify new system challenges in building scalable systems for HTMs, and propose efficient data layout for vectorizing HTM as well as distributed data structures including dynamic matrices and trees.", "Method": "Authors setup experiments to compare the proposed method with  previous open-source implementation for hierarchical latent Dirichlet allocation (hLDA).", "Outcome": "Empirical studies show that proposed system is 87 times more efficient than the previous open-source implementation for hLDA, and can scale to thousands of CPU cores. Authors demonstrate the scalability on a 131-million-document corpus with 28 billion tokens, which is 4--5 orders of magnitude larger than previously used corpus.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 48s "}}
{"id": "f82f2e26-5437-4397-b781-50668ce5851b", "displayed_text": "Title: Discriminative Alignment Training without Annotated Data for Machine Translation\n\nAbstract: In present Statistical Machine Translation (SMT) systems, alignment is trained in a previous stage as the translation model. Consequently, alignment model parameters are not tuned in function of the translation task, but only indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. In this approach, alignments are optimized for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed.", "label_annotations": {"Multi-aspect Summary": {"Context": "In present Statistical Machine Translation (SMT) systems, alignment is trained in a previous stage as the translation model. Consequently, alignment model parameters are not tuned in function of the translation task, but only indirectly.", "Key idea": "The authors propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion.", "Method": "The authors evaluate the proposed framework in terms of automatic translation evaluation metrics", "Outcome": "The evaluation of the proposed framework in terms of automatic translation evaluation metrics shows an improvement of translation quality is observed.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 1m 59s "}}
{"id": "f82f2e26-5437-4397-b781-50668ce5851b", "displayed_text": "Title: Discriminative Alignment Training without Annotated Data for Machine Translation\n\nAbstract: In present Statistical Machine Translation (SMT) systems, alignment is trained in a previous stage as the translation model. Consequently, alignment model parameters are not tuned in function of the translation task, but only indirectly. In this paper, we propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. In this approach, alignments are optimized for the translation task. In addition, no link labels at the word level are needed. This framework is evaluated in terms of automatic translation evaluation metrics, and an improvement of translation quality is observed.", "label_annotations": {"Multi-aspect Summary": {"Context": "Since in present Statistical Machine Translation (SMT) systems, alignment is trained in a previous stage as the translation model, so, alignment model parameters are not tuned in function of the translation task, but only indirectly.", "Key idea": "The authors propose a novel framework for discriminative training of alignment models with automated translation metrics as maximization criterion. ", "Method": "The authors evaluate novel framework in terms of automatic translation evaluation metrics.", "Outcome": "This method optimize alignments in translation task and no link labels at the word level are needed.\r\nAn improvement of translation quality is observed when evaluating novel framework in terms of automatic translation evaluation metrics.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 35s "}}
{"id": "fb538ce2-abf4-4bd8-b35c-1bfe3ab9f48e", "displayed_text": "Title: JU_NLP at SemEval-2016 Task 11: Identifying Complex Words in a Sentence.\n\nAbstract: The complex word identification task refers to the process of identifying difficult words in a sentence from the perspective of readers belonging to a specific target audience. This task has immense importance in the field of lexical simplification. Lexical simplification helps in improving the readability of texts consisting of challenging words. As a participant of the SemEval-2016: Task 11 shared task, we developed two systems using various lexical and semantic features to identify complex words, one using Naive Bayes and another based on Random Forest Classifiers. The Naive Bayes classifier based system achieves the maximum G-score of 76.7% after incorporating rule based post-processing techniques.", "label_annotations": {"Multi-aspect Summary": {"Context": "The complex word identification task refers to the process of identifying difficult words in a sentence from the perspective of readers belonging to a specific target audience. Lexical simplification helps in improving the readability of texts consisting of challenging words.", "Key idea": "The author develops two systems using various lexical and semantic features to identify complex words, one using Naive Bayes and another based on Random Forest Classifiers.", "Method": "The author tests the perfromance Naive Bayes classifier based system by incorporating rule based post-processing techniques and using G-score as an indicator.", "Outcome": "The Naive Bayes classifier based system achieves the maximum G-score of 76.7% after incorporating rule based post-processing techniques.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 26s "}}
{"id": "fb538ce2-abf4-4bd8-b35c-1bfe3ab9f48e", "displayed_text": "Title: JU_NLP at SemEval-2016 Task 11: Identifying Complex Words in a Sentence.\n\nAbstract: The complex word identification task refers to the process of identifying difficult words in a sentence from the perspective of readers belonging to a specific target audience. This task has immense importance in the field of lexical simplification. Lexical simplification helps in improving the readability of texts consisting of challenging words. As a participant of the SemEval-2016: Task 11 shared task, we developed two systems using various lexical and semantic features to identify complex words, one using Naive Bayes and another based on Random Forest Classifiers. The Naive Bayes classifier based system achieves the maximum G-score of 76.7% after incorporating rule based post-processing techniques.", "label_annotations": {"Multi-aspect Summary": {"Context": "The complex word identification task refers to the process of identifying difficult words in a sentence from the perspective of readers belonging to a specific target audience. Lexical simplification helps in improving the readability of texts consisting of challenging words.", "Key idea": "The authors develope two systems using various lexical and semantic features to identify complex words, one using Naive Bayes and another based on Random Forest Classifiers.", "Method": "N/A", "Outcome": "The Naive Bayes classifier based system achieves the maximum G-score of 76.7% after incorporating rule based post-processing techniques.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 2m 2s "}}
{"id": "feb75e1f-7838-48ca-9a78-cc31b717e5bf", "displayed_text": "Title: Lightning Talk - Think Outside the Dataset: Finding Fraudulent Reviews using Cross-Dataset Analysis\n\nAbstract: Many crowd-sourced review platforms, such as Yelp, TripAdvisor, and Foursquare, have sprung up to provide a shared space for people to write reviews and rate local businesses. With the substantial impact of businesses\u2019 online ratings on their selling\u00a0[2], many businesses add themselves to multiple websites to more easily be discovered. Some might also engage in reputation management, which could range from rewarding their customers for a favorable review, or a complex review campaign, where armies of accounts post reviews to influence a business\u2019 average review score.\n\nMost of previous work use supervised machine learning, and only focus on textual and stylometry features\u00a0[1, 3, 4, 7]. Their obtained ground truth data is not large and comprehensive\u00a0[4, 5, 6, 7, 8, 10]. These works also assume a limited threat model, e.g., an adversary\u2019s activity is assumed to be found near sudden shifts in the data\u00a0[8], or focused on positive campaigns.\n\nWe propose OneReview , a system for finding fraudulent content on a crowd-sourced review site, leveraging correlations with other independent review sites, and the use of textual and contextual features. We assume that an attacker may not be able to exert the same influence over a business\u2019 reputation on several websites, due to increased cost. OneReview focuses on isolating anomalous changes in a business\u2019 reputation across multiple review sites, to locate malicious activity without relying on specific patterns. Our intuition is that a business\u2019s reputation should not be very different in multiple review sites; e.g., if a restaurant changes its chef or manager, then the impact of these changes should appear on reviews across all the websites. OneReview utilizes Change Point Analysis method on the reviews of every business independently on every website, and then uses our proposed Change Point Analyzer to evaluate change-points, detect those that do not match across the websites, and identify them as suspicious. Then, it uses supervised machine learning, utilizing a combination of textual and metadata features to locate fraudulent reviews among the suspicious reviews.\n\nWe evaluated our approach, using data from two reviewing websites, Yelp and TripAdvisor, to find fraudulent activity on Yelp. We obtained Yelp reviews, through the Yelp Data Challenge\u00a0[9], and used our Change Point Analyzer to correlate this with data crawled from TripAdvisor. Since realistic and varied ground truth data is not currently available, we used a combination of our change point analysis and crowd-labeling to create a set of 5,655 labeled reviews. We used k-cross validation (k=5) on our ground truth and obtained 97% (+/- 0.01) accuracy, 91% (+/- 0.03) precision and 90% (+/- 0.06) recall. The model was used on the suspicious reviews, which classified 61,983 reviews, about 8% of all reviews, as fraudulent.\n\nWe further detected fraudulent campaigns that are actively initiated by or targeted toward specific businesses. We identified 3,980 businesses with fraudulent reviews, as well as, 14,910 suspected spam, where at least 40% of their reviews are classified as fraudulent. We also used community detection algorithms to locate several large astroturfing campaigns. These results show the effectiveness of OneReview in detecting fraudulent campaigns.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "Many business engaged in reputation management by using armies of accounts post reviews to influence a business\u2019 average review score. Unfortunately, most of previous work use supervised machine learning, and only focus on textual and stylometry features, and ground truth data they gained is not large and comprehensive .", "Key idea": "The authors propose a system for finding fraudulent content on a crowd-sourced review site named OneReview, leveraging correlations with other independent review sites, and the use of textual and contextual features. OneReview focuses on isolating anomalous changes in a business\u2019 reputation across multiple review sites, to locate malicious activity without relying on specific patterns. And it uses supervised machine learning, utilizing a combination of textual and metadata features to locate fraudulent reviews among the suspicious reviews. ", "Method": "The authors evaluate OneReview approach with data from two reviewing websites, Yelp and TripAdvisor, to find fraudulent activity on Yelp. The authors obtain Yelp reviews through the Yelp Data Challenge and use our Change Point Analyzer to correlate this with data crawled from TripAdvisor. And finally use a combination of our change point analysis and crowd-labeling to create a set of 5,655 labeled reviews as evaluation dataset.", "Outcome": "The authors used k-cross validation(k=5) on  ground truth and obtained 97% accuracy, 91% precision and 90% recall. Besides, authors use this model to identify 3,980 businesses with fraudulent reviews and 14,910 suspected spam, where at least 40% of their reviews are classified as fraudulent.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 4s "}}
{"id": "feb75e1f-7838-48ca-9a78-cc31b717e5bf", "displayed_text": "Title: Lightning Talk - Think Outside the Dataset: Finding Fraudulent Reviews using Cross-Dataset Analysis\n\nAbstract: Many crowd-sourced review platforms, such as Yelp, TripAdvisor, and Foursquare, have sprung up to provide a shared space for people to write reviews and rate local businesses. With the substantial impact of businesses\u2019 online ratings on their selling\u00a0[2], many businesses add themselves to multiple websites to more easily be discovered. Some might also engage in reputation management, which could range from rewarding their customers for a favorable review, or a complex review campaign, where armies of accounts post reviews to influence a business\u2019 average review score.\n\nMost of previous work use supervised machine learning, and only focus on textual and stylometry features\u00a0[1, 3, 4, 7]. Their obtained ground truth data is not large and comprehensive\u00a0[4, 5, 6, 7, 8, 10]. These works also assume a limited threat model, e.g., an adversary\u2019s activity is assumed to be found near sudden shifts in the data\u00a0[8], or focused on positive campaigns.\n\nWe propose OneReview , a system for finding fraudulent content on a crowd-sourced review site, leveraging correlations with other independent review sites, and the use of textual and contextual features. We assume that an attacker may not be able to exert the same influence over a business\u2019 reputation on several websites, due to increased cost. OneReview focuses on isolating anomalous changes in a business\u2019 reputation across multiple review sites, to locate malicious activity without relying on specific patterns. Our intuition is that a business\u2019s reputation should not be very different in multiple review sites; e.g., if a restaurant changes its chef or manager, then the impact of these changes should appear on reviews across all the websites. OneReview utilizes Change Point Analysis method on the reviews of every business independently on every website, and then uses our proposed Change Point Analyzer to evaluate change-points, detect those that do not match across the websites, and identify them as suspicious. Then, it uses supervised machine learning, utilizing a combination of textual and metadata features to locate fraudulent reviews among the suspicious reviews.\n\nWe evaluated our approach, using data from two reviewing websites, Yelp and TripAdvisor, to find fraudulent activity on Yelp. We obtained Yelp reviews, through the Yelp Data Challenge\u00a0[9], and used our Change Point Analyzer to correlate this with data crawled from TripAdvisor. Since realistic and varied ground truth data is not currently available, we used a combination of our change point analysis and crowd-labeling to create a set of 5,655 labeled reviews. We used k-cross validation (k=5) on our ground truth and obtained 97% (+/- 0.01) accuracy, 91% (+/- 0.03) precision and 90% (+/- 0.06) recall. The model was used on the suspicious reviews, which classified 61,983 reviews, about 8% of all reviews, as fraudulent.\n\nWe further detected fraudulent campaigns that are actively initiated by or targeted toward specific businesses. We identified 3,980 businesses with fraudulent reviews, as well as, 14,910 suspected spam, where at least 40% of their reviews are classified as fraudulent. We also used community detection algorithms to locate several large astroturfing campaigns. These results show the effectiveness of OneReview in detecting fraudulent campaigns.\n\n", "label_annotations": {"Multi-aspect Summary": {"Context": "Many crowd-sourced review platforms, such as Yelp, TripAdvisor, and Foursquare, have sprung up to provide a shared space for people to write reviews and rate local businesses.  Some might also engage in reputation management, which could range from rewarding their customers for a favorable review, or a complex review campaign, where armies of accounts post reviews to influence a business\u2019 average review score.", "Key idea": "Authors propose OneReview , a system for finding fraudulent content on a crowd-sourced review site, leveraging correlations with other independent review sites, and the use of textual and contextual features. ", "Method": "OneReview utilizes Change Point Analysis method on the reviews of every business independently on every website, and then uses our proposed Change Point Analyzer to evaluate change-points, detect those that do not match across the websites, and identify them as suspicious.  Then, it uses supervised machine learning, utilizing a combination of textual and metadata features to locate fraudulent reviews among the suspicious reviews.", "Outcome": "Authors used k-cross validation (k=5) on our ground truth and obtained 97% (+/- 0.01) accuracy, 91% (+/- 0.03) precision and 90% (+/- 0.06) recall.\r\nOneReview identified 3,980 businesses with fraudulent reviews, as well as, 14,910 suspected spam, where at least 40% of their reviews are classified as fraudulent.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 4m 34s "}}
{"id": "fed7302a-43a7-412e-8ace-d07905e38c3c", "displayed_text": "Title: One-class collaborative filtering with random graphs\n\nAbstract: The bane of one-class collaborative filtering is interpreting and modelling the latent signal from the missing class. In this paper we present a novel Bayesian generative model for implicit collaborative filtering. It forms a core component of the Xbox Live architecture, and unlike previous approaches, delineates the odds of a user disliking an item from simply being unaware of it. The latent signal is treated as an unobserved random graph connecting users with items they might have encountered. We demonstrate how large-scale distributed learning can be achieved through a combination of stochastic gradient descent and mean field variational inference over random graph samples. A fine-grained comparison is done against a state of the art baseline on real world data.", "label_annotations": {"Multi-aspect Summary": {"Context": "The bane of one-class collaborative filtering is interpreting and modelling the latent signal from the missing class. ", "Key idea": "Authors present a novel Bayesian generative model for implicit collaborative filtering. It forms a core component of the Xbox Live architecture, and unlike previous approaches, delineates the odds of a user disliking an item from simply being unaware of it. \r\nAuthors demonstrate how large-scale distributed learning can be achieved through a combination of stochastic gradient descent and mean field variational inference over random graph samples.", "Method": "Authors design fine-grained comparison of the proposed method against a state of the art baseline on real world data..", "Outcome": "The proposed method is better than state of the art baseline on real world data in fine-grained comparison.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 5m 41s "}}
{"id": "fed7302a-43a7-412e-8ace-d07905e38c3c", "displayed_text": "Title: One-class collaborative filtering with random graphs\n\nAbstract: The bane of one-class collaborative filtering is interpreting and modelling the latent signal from the missing class. In this paper we present a novel Bayesian generative model for implicit collaborative filtering. It forms a core component of the Xbox Live architecture, and unlike previous approaches, delineates the odds of a user disliking an item from simply being unaware of it. The latent signal is treated as an unobserved random graph connecting users with items they might have encountered. We demonstrate how large-scale distributed learning can be achieved through a combination of stochastic gradient descent and mean field variational inference over random graph samples. A fine-grained comparison is done against a state of the art baseline on real world data.", "label_annotations": {"Multi-aspect Summary": {"Context": "The bane of one-class collaborative filtering is interpreting and modelling the latent signal from the missing class.", "Key idea": "The authors present a novel Bayesian generative model for implicit collaborative filtering. ", "Method": "The authors demonstrate a fine-grained comparison is done against a state of the art baseline on real world data and novel Bayesian generative model.", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 2s "}}
{"id": "ffd14676-a525-479f-a74e-2c5d3a85c510", "displayed_text": "Title: Shape representation in parallel systems\n\nAbstract: There has been a recent revival of interest in parallel systems in which computation is performed by excitatory and inhibitory interactions within a network of relatively simple, neuronlike units [1 2 3 4]. At the early stages of visual processing, individual units can represent hypotheses about how small local fragments of the visual input should be interpreted, and interactions between units can encode knowledge about the constraints between local interpretations. Higher up in the visual system, the representational issues are more complex. This paper considers the difficulties involved in representing shapes in parallel systems, and suggests ways of overcoming them. In doing so, it provides a mechanism for shape perception and visual attention which allows a novel interpretation of the Gestalt slogan that the whole is more than the sum of its parts.", "label_annotations": {"Multi-aspect Summary": {"Context": "In parallel systems, visual processing by having individual network units represent hypotheses about how small local fragments of the visual input should be interpreted and encode knowledge about the constraints between local interpretations. But the representational issues are more complex higher up in the visual system.", "Key idea": "The author provides a mechanism for shape perception and visual attention which allows a novel interpretation of the Gestalt slogan that the whole is more than the sum of its parts.", "Method": "N/A", "Outcome": "N/A", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 0m 26s "}}
{"id": "ffd14676-a525-479f-a74e-2c5d3a85c510", "displayed_text": "Title: Shape representation in parallel systems\n\nAbstract: There has been a recent revival of interest in parallel systems in which computation is performed by excitatory and inhibitory interactions within a network of relatively simple, neuronlike units [1 2 3 4]. At the early stages of visual processing, individual units can represent hypotheses about how small local fragments of the visual input should be interpreted, and interactions between units can encode knowledge about the constraints between local interpretations. Higher up in the visual system, the representational issues are more complex. This paper considers the difficulties involved in representing shapes in parallel systems, and suggests ways of overcoming them. In doing so, it provides a mechanism for shape perception and visual attention which allows a novel interpretation of the Gestalt slogan that the whole is more than the sum of its parts.", "label_annotations": {"Multi-aspect Summary": {"Context": "There has been a recent revival of interest in parallel systems in which computation is performed by excitatory and inhibitory interactions within a network of relatively simple, neuronlike units. However, higher up than small local fragments, the representational issues are complex in the visual system.", "Key idea": "The authors suggest ways of overcoming the difficulties involved in representing shapes in parallel systems. They provide a mechanism for shape perception and visual attention.", "Method": "N/A", "Outcome": "The proposed mechanism allows a novel interpretation of the Gestalt slogan that the whole is more than the sum of its parts.", "Future Impact": "N/A"}}, "span_annotations": {}, "behavioral_data": {"time_string": "Time spent: 0d 0h 7m 10s "}}